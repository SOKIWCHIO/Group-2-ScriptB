Source: Reddit/bioinformatics
URL: https://reddit.com/r/bioinformatics/comments/1ftp3xu/advice_for_pipeline_tool/
Title:  Advice for pipeline tool? 

Content:
I don't use any kind of data pipeline software in my lab, and I'd like to start. I'm looking for advice on a simple tool which will suit my needs, or what I should read.

I found this but it is overwhelming - [https://github.com/pditommaso/awesome-pipeline](https://github.com/pditommaso/awesome-pipeline)

The main problem I am trying to solve is that, while doing a machine learning experiment, I try my best to carefully record the parameters that I used, but I often miss one or two parameters, meaning that the results may not be reproducible. I could solve the problem by putting the whole analysis in one comprehensive script, but this seems wasteful if I want to change the end portion of the script and reuse intermediary data generated by the beginning of the script. I often edit scripts to pull out common functionality, or edit a script slightly to change one parameter, which means that the scripts themselves no longer serve as a reliable history of the computation.

Currently much data is stored as csv files. The metadata describing the file results is stored in comments to the csv file or as part of the filename. Very silly, I know.

I am looking for a tool that will allow me to express which of my data depends on what scripts and what other data. Ideally the identity of programs and data objects would be tracked through a cryptographic hash, so that if a script or data dependency changes, it will invalidate the data output, letting me see at a glance what needs to be recomputed. Ideally there is a systematic way to associate metadata to each file expressing its upstream dependencies so one can recall where it came from.

I would appreciate if the tool was compatible with software written in multiple different languages.

I work with datasets which are on the order of a few gigabytes. I rarely use any kind of computing cluster, I use a desktop for most data processing. I would appreciate if the tool is lightweight, I think full containerization of every step in the pipeline would be overkill.

I do my computing on WSL, so ideally the tool can be run from the command line in Ubuntu, and bonus points if there is a nice graphical interface compatible with WSL (or hosted via a local webserver, as Jupyter Notebooks are).

I am currently looking into some tools where the user defines a pipeline in a programming language with good static typing or in an embedded domain-specific language, such as Bioshake, Porcupine and Bistro. Let me know if you have used any of these tools and can comment on them.

Comments:
- Snakemake or nextflow are the most common bioinformatics specific ones.

Other than that there are the more generic tools like Airflow, Prefect, Dagster etc.

Most of these have some sort of caching built in or can be configured to use caching/hashing.

Overall though, unless you are doing very large scale or have regulatory requirements I think what you are talking about is overkill.
- Jesus Christ I didn’t realize there were that many workflow languages.  What happened to not reinventing the wheel guys, cmon.
- I think you're over complicating. Have an "innermost" script that runs the model one time and takes in all the parameters. Version- control this script. 


Then write an "outer" script which has the parameters fed to the inner.
- DVC and dvc experiments are built for the exact use case you’re describing 

https://dvc.org/

- Have a look at the DVC experiments section 
- Do their tutorial

It will take some time to get the hang of it, but you end up with very robust reproducibility (that’s what I’ve experienced so far…)
- For ML work, MLflow (https://mlflow.org/docs/latest/tracking.html ) is popular for the tracking of runs. I know this doesn't solve all your problems of the pipeline running though.

Your problems are why Azure ML (which I use), AWS Sagemaker etc. have been created.
- Nextflow and document/save parameters. And save the Nextflow reports.
- I would recommend resisting the temptation to overcomplicate things by choosing a framework with too many built-in idiosyncrasies. Instead, consider giving GNU make and git a try. Here's a sample Makefile for a simple pipeline:

```
# Variables
PYTHON := python3
SCRIPTS_DIR := scripts
DATA_DIR := data
OUTPUT_DIR := output

# Phony targets
.PHONY: all

# Default target
all: $(OUTPUT_DIR)/final_report.pdf

# Data processing step
$(OUTPUT_DIR)/processed_data.csv: $(DATA_DIR)/raw_data.csv $(SCRIPTS_DIR)/process_data.py
	$(PYTHON) $(SCRIPTS_DIR)/process_data.py $< $@

# Analysis step
$(OUTPUT_DIR)/analysis_results.json: $(OUTPUT_DIR)/processed_data.csv $(SCRIPTS_DIR)/analyze_results.py
	$(PYTHON) $(SCRIPTS_DIR)/analyze_results.py $< $@

# Report generation step
$(OUTPUT_DIR)/final_report.pdf: $(OUTPUT_DIR)/analysis_results.json $(SCRIPTS_DIR)/generate_report.py
	$(PYTHON) $(SCRIPTS_DIR)/generate_report.py $< $@

# Clean up
clean:
	rm -rf $(OUTPUT_DIR)/*
```

To summarize briefly, the `final_report.pdf` is the default target. We set the dependencies for each intermediate step; for instance, `processed_data.csv` relies on `raw_data.csv` and `process_data.py`. When any dependency changes, `make` executes `process_data.py` using `raw_data.csv` as input and produces `processed_data.csv` as output.

Unfortunately, changes are tracked via modification timestamps rather than using a cryptographic signature. Unless you really need the latter, avoid it, especially with large datasets that can unnecessarily slow down your pipeline.

To keep track of parameters, store those details in a JSON or YAML config file and read from it within your scripts. Whenever `make` detects that your config file is newer than its target, it will rerun the entire pipeline. 
Use git to snapshot your project and take advantage of branches for experiments.

Reusable parts of your project can be organized in a `utils` folder, a separate file, or a module, depending on the conventions of the language you're using.
- What's wrong with bash?

I normally stick with good ol' bash scripts until they become unwieldy, then maybe port to nextflow for portability/scalability if it's something I will use often.

You talk about hashing files to check for updates - this is what make does. You can use Snakemake if you like python.
- Document your projet with a markdown. Build a script that fixes parameters you don't change and takes the others as an input. Explain in the markdown the code you execute, why, version it.
- I do appreciate the answers saying it's overkill, I'll try to avoid killing a mosquito with a sledgehammer here.
- https://www.commonwl.org/ but you lose on some control structures (loops, recursion, if then for a long time was not a thing). If everything you have gets processed in the exact same set of steps it is fine.

For bioinformatics people usually want support for HPC executors and cloud computing / storage systems, and just pass metadata plus files around running some simple bash commands. They also usually have built-in support for tasks like splitting a fasta file into sequences etc… Generally you get some run reports, but for more ‘pipeline management / database of runs’ aspects you have no support or paid product like Seqera Platform aka Nextflow Tower.

I would stay away from Nextflow / Snakemake for ML, for genomics they are a lot better…
- I can understand wanting to write one in your favorite language so that you can write your build routines in your favorite language, but I agree there are too many. That github I linked should display highlights like number of contributors and forks, last maintained/updated, etc to narrow it down.
- Another vote for nextflow. There are already workflows available from other groups, which can be used for orientation.
- Would a hash really add noticeably to the overall computation time? That's unintuitive to me. (Not that timestamps are a bad alternative, I think this would be fine.)


I agree that I want to stay away from domain specific idiosyncracies as I doubt the experiments/machine learning techniques  I'm running are common enough to be one of those idiosyncracies.


I can understand that Make gives a lightweight solution to this problem. I write a Makefile every once in a while but I've never gotten the hang of the syntax. Too many
special operators defined by $,&,#, *, etc. 


A couple people have recommended git. I agree that checking in the code used for an experiment is helpful for aiding reproducibility of the experiment but on the other hand I wouldn't want to use `git log` itself as an experiment journal.



Let me know if you can recommend any libraries for generate_report.py that would minimize the work of writing that.


Given that I have some parameters for stage 1 and some parameters for stage 2 I would like to figure out a solution where the outputs of stage 1 map to different files under different input parameters so that I can change the stage 1 parameters without overwriting the previous results. I could append the parameters to the filename automatically I guess, this seems like a hacky solution but it's lightweight and minimal.
- Yes.

Pipeline tools are fine, sometimes amazing.

For any day to day work, “smaller stuff” always keep track of commands in a script file.

If you run a command with arguments and options, put that command in a script file, then run the script.

Super helpful to revisit what you’ve done, also helpful for rerunning later.
- This is closest to what I'm already doing, and I've done it for a while. It works reasonably well. There are a number of tradeoffs to this approach.

- If I change one parameter near the end of the pipeline, I don't want to recompute everything before it. I want to cache intermediary results generated by different parameters. But as soon as I separate the process into multiple separate scripts, it becomes harder to guarantee that the connection between them is correctly documented, as I have to document the provenance of the inputs to the second script.

- Version control is definitely a good idea, but personally I think the strengths of git etc. are more towards maintaining a library, integrating different people's work, rolling back bugs and so on than serving as a logging system for experiments. Like, the documentation of each experiment could include the git commit id of the code that was used to run it, sure, that's a good idea. But I think it's an incomplete solution.
- That helps me understand a bit better the purpose of these various pipeline tools. I don't need to coordinate any cloud computing systems. I primarily want to systematically cache intermediate data while tagging it with metadata which clearly and comprehensively explains its provenance, and also be able to tell clearly what datasets need to be re-run in light of changes to data or scripts.
- > Would a hash really add noticeably to the overall computation time? That's unintuitive to me.

You'd definitely notice it. Even datasets of just a few gigabytes can delay the build time by a few seconds, which gets really annoying when you're trying to iterate quickly.

> I write a Makefile every once in a while but I've never gotten the hang of the syntax. Too many special operators defined by $,&,#, *, etc. 

Absolutely. Just to clarify, `$<` refers to the first requirement and `$@` to the target. You can skip using these shortcuts if you prefer, but it might make things a bit more verbose:

```
processed_data.csv: raw_data.csv process_data.py
	python3 process_data.py raw_data.csv  processed_data.csv
```

The GNU make documentation is quite good, and if you run into any issues, LLMs can now create a decent Makefile or explain details very competently.

> ... but on the other hand I wouldn't want to use git log itself as an experiment journal.

I get what you're saying. It really comes down to how detailed you need to be in your report. The simplest approach might be to parse the parameters and any extra details you care about and include them in a section of your final report. This way, you'll have a clear record of every part of your pipeline and the associated git commit to reproduce it.

> Let me know if you can recommend any libraries for generate_report.py that would minimize the work of writing that.

For minimal reports, the easiest method is to use f-strings for interpolation to create a markdown template, and then convert it to a PDF using pandoc.

```
...
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
disp.plot(cmap=plt.cm.Blues)
plt.savefig('plot.png')

iris_md = pd.DataFrame(iris.data).head().to_markdown()

template = f"""
# Iris Dataset Report

## 1. Example Data Rows

{iris_md}

## 2. Summary

The accuracy is : {accuracy}\n
The confusion matrix is:\n
![Confusion Matrix](plot.png)

"""

# Create a markdown file
with open('report.md', 'w') as md_file:
    md_file.write(template)

# Use pandoc to convert markdown to PDF
subprocess.run(['pandoc', 'report.md', '-o', 'report.pdf'])
```

For a more flexible approach, you might want to consider using the Jinja templating system. Another possibility is to pass variables directly to a markdown template via Pandoc, however, if you need to display plots and tables, this route might turn into a headache. I'd also recommend looking into the "literate programming" approach where your code essentially becomes your report. Tools like Pweave and Quarto (or RMarkdown in R) could be really helpful for this.
- Then at low scale for ML use Tensorboard/Wandb, then go into bigger platforms like MLflow. 

Nextflow/Snakemake are more about running a CSV of sequencing or imaging samples through published pipelines that do QC and processing for methods like RNAseq. At this point you get a nice QC report, then if you are brave you add a script or notebook to merge all your data together and output figures for a paper. They are more about automation and reproducibility instead of exploring parameter space, adding new samples and re-running analysis is easy. It beats running 100s of time the same sequence of bash commands. After a while I am either adding data and running every steps on those new data only, or tinkering late stage steps, and I don’t really care about old runs.

Trying to compare many runs together is tricky, you have to manage yourself how the output folders are named, how the input parameters are stored, and how you will you do comparisons across runs. By default you would “squash” the previous output…. You also don’t have concepts like model or artifact registries, model evaluation etc…
