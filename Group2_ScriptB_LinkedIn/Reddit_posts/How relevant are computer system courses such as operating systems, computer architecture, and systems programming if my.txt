Source: Reddit/bioinformatics
URL: https://reddit.com/r/bioinformatics/comments/13judx9/how_relevant_are_computer_system_courses_such_as/
Title: How relevant are computer system courses such as operating systems, computer architecture, and systems programming if my goal is to become a bioinformatics engineer or bioinformatics software enginner?

Content:
I see a lot of comments on bioinformatics jobs containing a lot of linux and command line uses. However, I don't see a lot of people needing stuff like memory managment, resource allocation,etc etc. Are courses that dive deep into how a computer system work such as operating system or computer architecutre relevant or useful if I want to get into the engineering side of bioinformatics and the cs side? 

I am interested in machine learning side of bioinformatics and so a lot of my courses have been tailored to preparation and theory for machine learning such as linear algebra, multivariate calculus,probability theory, etc but I feel like I don't have the knowledge of computer systems like cs people do. Was curious if that held back anyone or the knowledge is not really relevant?

Comments:
- Computer systems courses are very relevant. Bioinformatics deals with large datasets. These courses help you understand the relationships between the CPU, GPU, memory, and generally how a computer works.

With these courses you'll be able to better understand how to get your programs to run more efficiently, and better understand what computer resources your projects would need.
- In general the more you know about the "next layer down" from whatever level of abstraction you're working on the more effective you'll be and the easier it'll be for you to reason about problems. In the context of computational work this means that the design decisions for the tool you're working with, be it a programming language or a command line tool or whatever, are determined by how the OS and hardware function. Without understanding how those work the design decisions feel arbitrary and can only be mastered though brute memorization, but the more you understand those deeper layers the more things will feel natural and comprehensible. There is of course diminishing returns on this as most abstractions are not enormously leaky and many design decisions are in fact unconstrained by system properties so it'd probably not be worth taking loads of classes on OSs, compilers, computer hardware, etc. at the expense of classes on genetics or biochemistry which are equally important subjects for bioinformatics. In machine learning based tools in particular most of the serious errors I've seen have been the result of bad assumptions about the biology or the measurement technology rather than issues with code.
- Cloud and cluster (HPC) management, workflow applications (nextflow)
- OS/systems stuff is far more important for the software engineering side than ML
- I doubt you'd ever find yourself held back by this stuff although if you knew it I'd imagine you would find a way to be thinking about it during the workday. The real low-level systems stuff isn't super-relevant a lot in my experience although the major exception is knowing how the cache works and writing code that minimizes cache misses. Bioinformatics unfortunately lives in this paradigm where huge amounts of RAM are a de facto big data solution, and moving shit from RAM into the register and back is the bottleneck in almost every case so cache-efficiency can be the difference between hours and days. For bioinformatics, in practice, that means finding a way to do whatever you're doing in numpy because that code is so incredibly well optimized for such things. (Mostly this is because it uses blas and MKL on the backend and I know there are other libraries in R, etc... that use the same things. So it's not just numpy, but numpy is my personal swiss army knife.) So I wouldn't feel that you really need to have a course in it, but it never hurts to do your own side reading to broaden your understanding, although that's pretty much true of anything.
- I'm a "regular" software engineer, and 80% of the stuff I learned in OS and arch classes has been irrelevant to my work (web development, distributed systems, and performance engineering).

I think if you want to develop an instinct for how computers work just for the sake of writing better code, all you really need to do is learn enough C to understand pointers and threads, then learn the general design of the compiler/interpreter you're using. this will require understanding some OS and arch topics, but not as much as you learn in a semester long course. If you wanted to go above and beyond, you could learn about parallel/concurrent programming in C (which is actually a fun topic imho).

It definitely helpful to know how to *administer* an operating system, but stuff like page replacement or scheduling algorithms aren't that useful. You might actually have a much higher ROI learning about database internals and how to troubleshoot slow queries. I've personally had to do this way more often than I've had to think about how the OS is working, I imagine this would be doubly true in bioinformatics work.
- The folks who write the workhorse software we all use are for sure very well versed in the inner workings of the machine. Often these people are physicists or computer scientists and are always genius level programmers with a vast understanding of low level languages like C or C++ (or even Fortran).

That being said, there is plenty of room for clever combinations of existing tools without having to deal with the low level stuff, as well as implementation of clever algorithms in stead.

I have a reasonable understanding of computer architecture, also from early dabbling in C/C++, but apart from knowing what RAM, CPU and GPUs are, I rarely had much benefit from knowing more. One fun example which I use to teach basic computer architecture, however, is how an R-loop can be sped up substantially by allocating a data-structure a priori rather than appending values to it as it grows.
- I want to learn about that as well. Any suggestion for a book or course for that ?
- I’d say, except for some very niche roles, almost no relevance. It’s never come up in my dozens of years in the field.
- I am an old-school computer scientist, and am new to bioinformatics.  I just changed my code for throughput purposes and went from processing 1,000 proteins per second to 84,000 per second.   That was all computer science (parallelism, data compression, data sharding, etc. etc.)

The more you know, the more you can apply to problem domains.
- I would put a qualifier on this and say that it depends what you're trying to do. 

I'm a biochemist-turned-bioinformatician, and although I know how to write good Python programs including ones that use GPU acceleration and compiled packages, I couldn't tell you much about the exact architecture under the hood! I know basically how the computer parts are connected, but beyond that, the hardware is a black box of magical mystery to me. 

If I knew the under-the-hood things better, maybe I could write faster programs, this is true. But for the kinds of work that I do - protein sequence analysis and proteomics - I've always been able to process things reasonably quickly just on my own high-end laptop with an RTX graphics card, so performance increases really aren't necessary for me. I'd say this is true for many others too, just because of how affordable compute resources have become.
- IMO this is an excellent post. One example of the first point off the top of my head is if you're creating a numpy array, do you use the "C" layout or the "F" layout. Depending on what you're doing one of those could be substantially faster than the other, and it's useful to know what it means and why. But most of the time the default is "C" and numpy will figure out how to optimize itself on that basis. And on the last point about most errors being made due to bad assumptions, holy hell I could not agree more. Just today I told someone that my rule of thumb is that all databases for bacterial genetics are roughly 10% garbage. I know that's painting with a broad brush, but it's amazing how often I've come to that conclusion over the years (here's a [twitter thread](https://twitter.com/traingene/status/1469716014195789824?lang=en) from my old boss about one of those times).
- Really insightful thank you !
- I talked to a guy at google once upon a time about ML work and he made a hard point that, at google anyway, folks thought about themselves as software engineers first and ML engineers second. For them that was of necessity. I'm sure that's not universally true but in a lot of cases ML and software engineering are pretty closely coupled.
- thank you a lot. Yea I know the basic of what cacheing is cuz I took all the theory courses however I haven't taken any computer systems courses. Thanks for your insight and it's reassuring that I won't be held back by not knowing all this stuff
- Yea that’s why my question was tailored towards if I want to get into the engineering side of bioinformatics and I think that’s why the commentter said it was important. 

It’s true that bioinformatics is very broad. We need people from both backgrounds where some people main focus is solving biological problems and not necessarily on the technical side. We also need people on writing faster algorithms and knowing the inside out of computer systems.
- Most people in the field do bioinformatics analysis fine with little background on computer systems. However, relevant knowledge may make a big difference when you need it in future. If you are interested in the topic and confident that you can get a decent grade, go for it.
