Source: Reddit/MachineLearning
URL: https://reddit.com/r/MachineLearning/comments/14slf2p/d_list_of_prior_works_on_llm_hallucination/
Title: [D] List of prior works on LLM hallucination, organized by evaluation, benchmark, enhancement, and survey

Content:
Hallucinations present a key challenge for LLMs.

Our team compiled a list of prior works on hallucination.

May this benefit others also exploring how to eliminate hallucinations.

Please suggest missing papers; we'll update the post.

To account for future papers, we'll maintain an ongoing list from our website.

Please DM for the URL since sharing our URL is prohibited.

We organized the papers with a simple framework. Happy to use a standard taxonomy if one exists.

Questions:

1. Would people like a similar list for LLM reasoning?
2. Should we create a separate category for datasets?

Note: summaries were generated by feeding abstracts into GPT4.

DEBES

Domain: hallucination

Evaluation: papers that measure and score how LLMs hallucinate

Benchmark: papers that evaluate two or more models against one or more hallucination evaluations

Enhancement: papers that mitigate or eliminate hallucinations

Survey: papers that summarize hallucination literature

=====

**Evaluations**

1. Retrieving Supporting Evidence for LLMs Generated Answers (University of Waterloo): [https://arxiv.org/pdf/2306.13781.pdf](https://arxiv.org/pdf/2305.14627v1.pdf). The study investigates a method to automatically verify responses generated by large language models (LLMs) using a corpus. The experiment involves presenting a question to the LLM, receiving a generated answer, and then querying the corpus with the combination of the question and generated answer. The LLM is then asked to verify if the generated answer is supported by the retrieved answer. This experiment uses the MS MARCO (V1) test collection, with three retrieval methods. Results indicate that LLMs can verify their answers given appropriate supporting material, but with 70-80% accuracy, the method is not completely reliable in detecting hallucinations. Significant improvements are reported compared to other methods on three different datasets.
2. Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation (ETH Zurich): [https://arxiv.org/pdf/2305.15852.pdf](https://arxiv.org/pdf/2305.15852.pdf). This study focuses on self-contradictions in large language models (large LMs), including their evaluation, detection, and mitigation. The researchers created a framework to elicit self-contradictions and found they're common across different LMs and topic types. The study shows ChatGPT and GPT-4 perform well at identifying self-contradictions, while Vicuna-13B struggles. An iterative algorithm was developed to help LMs eliminate self-contradictions while retaining fluency and informativeness. The approach applies to black-box LMs and needs no external grounded knowledge.
3. Detecting and Mitigating Hallucinations in Multilingual Summarisation (University of Edinburgh, University of Cambridge): [https://arxiv.org/pdf/2305.13632v1.pdf](https://arxiv.org/pdf/2305.13632v1.pdf). This research addresses the issue of hallucinations (unfaithful summaries) in neural models used for abstractive summarisation, particularly in cross-lingual settings. A new metric, mFACT, is developed to assess the faithfulness of non-English summaries, using translation-based transfer from existing English faithfulness metrics. A method is also proposed to minimize hallucinations in cross-lingual transfer, where the loss of each training example is weighted by its faithfulness score. Through extensive experiments, mFACT proved the most suitable for detecting hallucinations. The suggested loss weighting method significantly improved performance and faithfulness, surpassing strong baselines such as MAD-X. The authors have shared their code and dataset online.
4. RefGPT: Reference → Truthful & Customized Dialogues Generation by GPTs and for GPTs (Shanghai Jiao Tong University, Hong Kong Polytechnic University, Beijing University of Posts and Telecommunications): [https://arxiv.org/pdf/2305.14994.pdf](https://arxiv.org/pdf/2305.14994.pdf). The abstract discusses a method called RefGPT, proposed to generate accurate and personalized dialogues, solving issues with current Large Language Models (LLMs) like ChatGPT, which tend to generate incorrect information (hallucination). RefGPT generates dialogue by using given references, not just the model's own knowledge, and it provides detailed control for better customization. The researchers also introduce two datasets created using GPT-4: RefGPT-Fact (100k factual multi-turn dialogues) and RefGPT-Code (76k multi-turn dialogues for coding scenarios). The resources are available on GitHub.
5. ALIGNSCORE: Evaluating Factual Consistency with A Unified Alignment Function (UC San Diego): [https://arxiv.org/pdf/2305.16739.pdf](https://arxiv.org/pdf/2305.16739.pdf). This abstract discusses a new approach to automatically evaluate factual consistency in text generation using a unified training framework called ALIGNSCORE. The model incorporates a diverse array of data sources from seven different tasks, resulting in 4.7 million training examples. Extensive testing on large-scale benchmarks, including 22 previously unseen datasets, shows that ALIGNSCORE significantly outperforms existing metrics. Despite its size of 355M parameters, it matches or even surpasses the performance of larger metrics based on ChatGPT and GPT-4.
6. HaRiM+: Evaluating Summary Quality with Hallucination Risk (NCSOFT NLP Center): [https://arxiv.org/pdf/2211.12118v2.pdf](https://arxiv.org/pdf/2211.12118v2.pdf). This study reinterprets the decoder overconfidence-regularizing objective from a previous work as a hallucination risk measurement for estimating the quality of generated summaries. The researchers introduce HaRiM+, a reference-free metric that calculates hallucination risk based on token likelihoods using only an existing summarization model. HaRiM+ doesn't need additional model training or ad-hoc modules, and aligns well with human judgment on three summary-quality annotation sets: FRANK, QAGS, and SummEval. This work could aid in improving automated summary evaluation and generation.

**Benchmarks**

1. TruthfulQA: Measuring How Models Mimic Human Falsehoods (Open AI, University of Oxford): [https://arxiv.org/pdf/2109.07958.pdf](https://arxiv.org/pdf/2109.07958.pdf). The abstract introduces a benchmark for measuring the truthfulness of language models in generating answers. It consists of 817 questions across various categories. The questions are designed to challenge models with false beliefs or misconceptions. GPT-3, GPT-Neo/J, GPT-2, and a T5-based model were tested. The best model was truthful in 58% of the questions, while humans achieved 94% accuracy. Models often produced false answers that imitated popular misconceptions and could potentially mislead humans. Interestingly, larger models were generally less truthful, in contrast to other NLP tasks. Scaling up models alone is deemed less effective in improving truthfulness, suggesting the importance of fine-tuning with alternative training objectives.
2. Holistic Evaluation of Language Models (CRFM, HAI- Stanford University): [https://arxiv.org/pdf/2211.09110.pdf](https://arxiv.org/pdf/2211.09110.pdf). The study introduces the Holistic Evaluation of Language Models (HELM), aimed at improving transparency in understanding language models' capabilities, risks, and limitations. The approach involves taxonomizing various scenarios and metrics relevant to language models and evaluating a subset of these, considering what's missing or underrepresented. It measures seven metrics (accuracy, calibration, robustness, fairness, bias, toxicity, efficiency) across 16 core scenarios, ensuring that all aspects are considered. In addition, HELM conducts targeted evaluations on specific aspects, like knowledge, reasoning, and disinformation. A comprehensive evaluation of 30 significant language models on 42 scenarios, some of which have not been used in mainstream evaluation, was carried out, with results indicating 25 key findings regarding the interaction of various scenarios, metrics, and models. HELM aims to serve as a continuously updated benchmark tool for the community.
3. HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models (Renmin University of China, Université de Montréal): [https://arxiv.org/pdf/2305.11747v2.pdf](https://arxiv.org/pdf/2305.11747v2.pdf). The study introduces the Hallucination Evaluation for Large Language Models (HaluEval), a benchmark tool for examining the tendency of large language models like ChatGPT to generate hallucinated content—information not rooted in the source or unverifiable. This was done through a two-step ChatGPT-based framework, generating and annotating a large collection of samples. The results indicate that ChatGPT can create unverifiable information in response to 11.4% of user queries, suggesting difficulty in recognizing hallucinated content. However, enhancing hallucination recognition is possible with external knowledge or additional reasoning steps.
4. A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation (Peaking uni, Microsoft, Tencent, Xiaowei, Meta): [https://arxiv.org/pdf/2104.08704v2.pdf](https://arxiv.org/pdf/2104.08704v2.pdf). This paper presents a new approach to addressing the issue of hallucination (generating incorrect or non-existent content) in large pre-trained models like GPT3. Rather than using sentence or document level detection, it proposes a token-level, reference-free hallucination detection task and introduces a new dataset, HADES (HAllucination DEtection dataSet), for this purpose. The dataset is created by modifying text segments from English Wikipedia and verifying them with crowdsourced annotations. To combat label imbalance, an iterative model-in-loop strategy is employed. Multiple baseline models are created following thorough data analyses.
5. Enabling Large Language Models to Generate Text with Citations (Princeton University): [https://arxiv.org/pdf/2305.14627v1.pdf](https://arxiv.org/pdf/2305.14627v1.pdf). This study introduces ALCE, the first benchmark for evaluating automatic citation generation in large language models (LLMs). Noting that LLMs often "hallucinate" or fabricate information, the researchers aim to improve their factual accuracy and verifiability by having them generate text with citations. ALCE amasses a variety of questions and retrieval corpora, calling for the creation of comprehensive systems to find supporting evidence and generate answers with references. The researchers create automatic metrics for fluency, correctness, and citation quality, all of which correlate strongly with human assessments. Tests reveal that current systems, including state-of-the-art LLMs, could improve, as evidenced by the finding that 49% of responses from the best model on the ELI5 dataset lacked full citation support. The research concludes by suggesting areas for further investigation, such as developing better information retrievers, advancing long-context LLMs, and enhancing the synthesis of information from multiple sources.
6. Diving Deep into Modes of Fact Hallucinations in Dialogue Systems (University at Buffalo): [https://arxiv.org/pdf/2301.04449v1.pdf](https://arxiv.org/pdf/2301.04449v1.pdf). This research addresses the issue of fact hallucination in Knowledge Graph (KG) grounded chatbots, a problem where entities not referenced in knowledge sources or conversation history are inaccurately introduced into responses. Prior solutions have tweaked training procedures or used multi-step refining methods, but there's been little focus on developing an entity-level hallucination detection system. This paper investigates different types of hallucination in KG-grounded chatbots via human feedback analysis, introduces a series of perturbation strategies to create a synthetic dataset named FADE (FActual Dialogue Hallucination DEtection Dataset), and evaluates multiple baseline models for hallucination detection against human-verified data and established benchmarks.
7. FAITHDIAL: A Faithful Benchmark for Information-Seeking Dialogue (Alberta Machine Intelligence Institute): [https://arxiv.org/pdf/2204.10757.pdf](https://arxiv.org/pdf/2204.10757.pdf). FAITHDIAL, a new benchmark for hallucination-free dialogues, was created to improve the faithfulness of information-seeking dialogue systems. This benchmark edits unsupported utterances (hallucinations) in the Wizard of Wikipedia (WoW) benchmark. It was found to be more reliable than WoW while sustaining engaging dialogues. FAITHDIAL effectively serves as a training signal for a hallucination critic, boosting performance by 12.8 F1 score on the BEGIN benchmark, and promotes high-quality dialogue generation. It has demonstrated utility in zero-shot transfer on datasets like CMU-Dog and TopicalChat. Moreover, human evaluations found FAITHDIAL-trained models produce more interpretable, cooperative, and engaging responses.
8. Evaluating the Factual Consistency of Large Language Models Through Summarization (UNC Chapel Hill): [https://arxiv.org/pdf/2211.08412.pdf](https://arxiv.org/pdf/2211.08412.pdf). The authors introduce the Factual Inconsistency Benchmark (FIB), a new tool designed to assess the factual consistency of large language models (LLMs) in summarization tasks. The benchmark gauges the accuracy of models by comparing scores they assign to factually consistent and inconsistent summaries. Evaluation of 23 LLMs, including models like BLOOM and OPT, reveals that LLMs generally prefer factually consistent summaries, although they tend to favor factually inconsistent ones if they appear verbatim in the source document. The FIB benchmark, code, and data are publicly available.

**Enhancements**

1. On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation (Stanford University): [https://arxiv.org/pdf/2005.03642.pdf](https://arxiv.org/pdf/2005.03642.pdf). This paper explores the role of exposure bias in neural machine translation (NMT) and its connection to the issue of "hallucinations" under domain shift. The authors establish that exposure bias contributes to these hallucinations. They further demonstrate, through trials on three datasets, that using Minimum Risk Training, an algorithm that minimizes exposure bias, can lessen hallucinations. They also examine why exposure bias worsens during domain shifts and its connection to the beam search problem - performance degradation with increasing beam size. The findings justify methods to reduce exposure bias, which, despite not enhancing in-domain test set performance, improve model robustness during domain shifts.
2. Certified Reasoning with Language Models (Stanford University): [https://arxiv.org/pdf/2306.04031.pdf](https://arxiv.org/pdf/2306.04031.pdf). The abstract discusses the development of 'guides' for language models to enhance their reasoning abilities. These guides, such as LOGICGUIDE, use state and incremental constraints to steer the models towards valid statements. They help models formalize assumptions, ensuring sound reasoning. LOGICGUIDE significantly boosts the performance of language models like GPT-3, GPT-3.5 Turbo, and LLaMA in reasoning tasks, with accuracy gains of up to 35%. It also minimizes content effects, or the interference of prior and current assumptions. Moreover, LOGICGUIDE allows LLaMA to self-improve by learning from its verified self-generated reasoning, preventing learning from hallucinations.
3. Holistic Evaluation of Language Models (Stanford University): [https://arxiv.org/pdf/2306.03872.pdf](https://arxiv.org/pdf/2211.09110.pdf). The paper introduces the Holistic Evaluation of Language Models (HELM), aimed at improving the transparency of language models. HELM characterizes a broad array of use cases and metrics of interest for language models, also identifying underrepresented areas. It utilizes a multi-metric approach, measuring seven metrics across 16 core scenarios 87.5% of the time to reveal trade-offs across models and metrics. It also includes seven targeted evaluations for a more in-depth analysis of specific aspects. HELM evaluates 30 prominent language models on 42 scenarios, significantly improving benchmark coverage from an average of 17.9% to 96.0%. The study results in 25 top-level findings on the interaction of scenarios, metrics, and models. All raw prompts and completions are made public, and a toolkit is provided to facilitate future updates and additions to HELM.
4. CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing (Microsoft): [https://arxiv.org/pdf/2305.11738.pdf](https://arxiv.org/pdf/2305.11738.pdf). The abstract discusses the development of a framework named CRITIC, designed to mitigate issues in large language models (LLMs) such as generating flawed content or hallucinating facts. CRITIC, inspired by human interaction with tools for refinement, enables LLMs to validate and improve their own outputs. It uses relevant tools to assess and revise initial text based on received feedback. Trials involving free-form question answering, mathematical program synthesis, and toxicity reduction suggest CRITIC enhances LLMs' performance and underscores the significance of external feedback in LLMs' continuous self-improvement.
5. PURR: Efficiently Editing Language Model Hallucinations by Denoising Language Model Corruptions (UC Irvine, Google): [https://arxiv.org/pdf/2305.14908v1.pdf](https://arxiv.org/pdf/2305.14908v1.pdf). Large language models can generate false claims or "hallucinations", a problem being addressed by recent research through prompt-based editing. However, the use of large language models for editing has significant cost and speed issues. This study presents a solution by training compact editors to denoise text corrupted by large language models in an unsupervised way, creating faux hallucinations for training purposes. Their model, Petite Unsupervised Research and Revision (PURR), improves attribution and offers significantly faster execution times over existing methods.
6. Hallucinated but Factual! Inspecting the Factuality of Hallucinations in Abstractive Summarization (McGill University): [https://arxiv.org/pdf/2109.09784v2.pdf](https://arxiv.org/pdf/2109.09784v2.pdf). State-of-the-art abstractive summarization systems often produce hallucinations, generating content not directly inferred from the source. Surprisingly, many of these hallucinations are factual and can provide valuable background information in summaries. This paper introduces a novel detection method that distinguishes factual from non-factual hallucinations of entities using prior and posterior probabilities from masked language models. The approach outperforms baselines and aligns well with human judgments. When used as a reward signal in reinforcement learning, the detector significantly enhances summary factuality while preserving abstractiveness.
7. Controlled Hallucinations: Learning to Generate Faithfully from Noisy Data (Google): [https://arxiv.org/pdf/2010.05873v1.pdf](https://arxiv.org/pdf/2010.05873v1.pdf). Neural text generation performs well with abundant training data, but this is not always available. Heuristic rules used to collect parallel data introduce noise, causing models to generate unsupported text. We propose a technique to control and acknowledge these hallucinations without modifying the model architecture. We test its effectiveness on the noisy WikiBio corpus, evaluating both automatically and with human input.
8. Adversarial Feature Hallucination Networks for Few-Shot Learning (Northeastern University): [https://arxiv.org/pdf/2003.13193v2.pdf](https://arxiv.org/pdf/2003.13193.pdf). This paper presents a new approach for few-shot learning (FSL), a method used when only a small amount of labeled data is available. The proposed Adversarial Feature Hallucination Networks (AFHN) uses conditional Wasserstein Generative Adversarial networks (cWGAN) to create diverse and discriminative features based on limited samples. The AFHN model integrates two novel regularizers, a classification regularizer and an anti-collapse regularizer, to enhance the discriminability and diversity of these features. Comparative results from three common benchmarks indicate that AFHN outperforms other data augmentation-based FSL strategies and current leading methods.
9. Improving Language Models via Plug-and-Play Retrieval Feedback (Allen Institute for Artificial Intelligence): [https://arxiv.org/pdf/2305.14002.pdf](https://arxiv.org/pdf/2305.14002.pdf). This paper introduces REFEED, a pipeline that enhances large language models (LLMs) by incorporating automatic retrieval feedback. LLMs often generate incorrect or hallucinated information, limiting their practical applicability. Human feedback improves factuality but is resource-intensive and impractical during inference. REFEED generates initial outputs, retrieves relevant information from large document collections, and incorporates it for output refinement. Experiments show that REFEED improves performance by +6.0% (zero-shot) and +2.5% (few-shot) compared to baselines without retrieval feedback.
10. Controlling Hallucinations at Word Level in Data-to-Text Generation (Clement Rebuffel, Marco Roberti, Laure Soulier, Geoffrey Scoutheeten, Rossella Cancelliere, Patrick Gallinari): [https://arxiv.org/pdf/2102.02810.pdf](https://arxiv.org/pdf/2102.02810v2.pdf). Data-to-Text Generation (DTG) involves converting structured data into natural language descriptions, with modern methods involving neural-based generators. However, these methods often include misleading statements or "hallucinations." This paper addresses this issue with a novel Multi-Branch Decoder that treats hallucinations at the word level. The model leverages word level labels derived from co-occurrence analysis and dependency parsing to learn from each training instance. Evaluations on the WikiBio benchmark show the model's accuracy and effectiveness, reducing hallucinations while maintaining fluency and coherence, even in noisy settings.
11. SELFCHECKGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models (University of Cambridge): [https://arxiv.org/pdf/2303.08896v2.pdf](https://arxiv.org/pdf/2303.08896v2.pdf). The abstract presents a study on "SelfCheckGPT," a sampling-based method to fact-check large language models (LLMs) like GPT-3 without needing an external database. It exploits the tendency of LLMs to produce similar, consistent facts for a concept, while hallucinated facts result in divergent, inconsistent samples. The method's efficiency was tested on GPT-3 generated passages about individuals from the WikiBio dataset. Results indicated that SelfCheckGPT could effectively identify factual and non-factual sentences and assess passage factuality. Its performance in hallucination detection matched or exceeded grey-box methods.
12. Mutual Information Alleviates Hallucinations in Abstractive Summarization (ETH Zurich): [https://arxiv.org/pdf/2210.13210v2.pdf](https://arxiv.org/pdf/2210.13210v2.pdf). This paper investigates the issue of "hallucination" in abstractive summarization models, where they generate content unsupported by the original text. The research identifies high model uncertainty as a key factor causing such hallucinations, with models preferring high-frequency phrases from the training set when unsure about the next output. To combat this, the paper proposes a decoding strategy that focuses on the mutual information between source and target tokens rather than just the target token's probability during periods of model uncertainty. Experiments on the XSUM dataset demonstrate a decrease in hallucination occurrences while maintaining strong ROUGE and BERTS scores.
13. RHO (ρ): Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding (Hong Kong University of Science and Technology): [https://arxiv.org/pdf/2212.01588.pdf](https://arxiv.org/pdf/2212.01588.pdf). The paper presents RHO, a method to improve dialogue systems by reducing "hallucinated" responses unsupported by the input source. The technique involves integrating information from a knowledge graph (KG) into the dialogue context. This is achieved by (1) locally grounding knowledge, which combines textual embeddings with KG embeddings, and (2) globally grounding knowledge, which gives RHO multi-hop reasoning abilities via attention mechanisms. The method also includes a response re-ranking technique based on KG sub-graph walks for improved reasoning. Experimental results show RHO significantly outperforms existing methods in reducing hallucination and overall performance.
14. MoFE: Mixture of Factual Experts for Controlling Hallucinations in Abstractive Summarization (Anonymous): [https://openreview.net/pdf?id=JegLdW0zORF](https://openreview.net/pdf?id=JegLdW0zORF). Neural abstractive summarization models often produce factually incorrect content, known as hallucination. To address this, the Mixture of Factual Experts (MoFE) model is proposed, which unites several summarization experts targeting different factual errors. The MoFE model combines these experts using weights and logits ensembling techniques. This strategy offers a modular solution to control factual inaccuracies while upholding performance on standard ROUGE metrics.
15. Reducing Hallucinations in Neural Machine Translation with Feature Attribution (Imperial College London): [https://arxiv.org/pdf/2211.09878.pdf](https://arxiv.org/pdf/2211.09878.pdf). This abstract discusses the issue of hallucinations in Neural Machine Translation (NMT) models that arise due to low-quality training data. The authors present a case study, first utilizing feature attribution methods to understand the behavior of an NMT model producing hallucinations. Subsequently, these methods are leveraged to propose a new loss function aimed at reducing hallucinations. This proposed solution importantly does not necessitate retraining the model from the beginning.
16. Optimal Transport for Unsupervised Hallucination Detection in Neural Machine Translation (Multiple EU schools): [https://arxiv.org/pdf/2212.09631.pdf](https://arxiv.org/pdf/2212.09631.pdf). This paper tackles the issue of hallucination detection in Neural Machine Translation (NMT), where models can generate incorrect translations detached from the source content. The proposed solution is a fully unsupervised, plug-in detector that uses an optimal transport formulation to identify distinct cross-attention patterns characteristic of hallucinations. The detector is compatible with any attention-based NMT model. Experiments demonstrated that this detector outperforms prior model-based detectors and rivals those using external models trained on large sample sets.
17. Trapping LLM “Hallucinations” Using Tagged Context Prompts (UMD Baltimore): [https://arxiv.org/pdf/2306.06085.pdf](https://arxiv.org/pdf/2306.06085.pdf). This paper addresses the issue of hallucinations in large language models like ChatGPT, which generate false or fabricated information. The authors propose a novel method using context and embedded tags to identify and flag instances of model-generated data outside its domain knowledge. By adding context to question prompts, they significantly reduce overall hallucination frequency in generative language models. Additionally, placing tags within contexts effectively eliminates hallucinations in model responses with 98.88% effectiveness.
18. Contrastive Learning Reduces Hallucination in Conversations (Shandong University, University of Amsterdam): [https://arxiv.org/pdf/2212.10400.pdf](https://arxiv.org/pdf/2212.10400.pdf). The abstract discusses MixCL, a contrastive learning scheme designed to address "hallucination" in pre-trained language models (LMs), where these models generate irrelevant or factually incorrect responses. The proposed mixed contrastive objective optimizes the knowledge elicitation process of LMs to minimize hallucination. The effectiveness of MixCL is evaluated through experiments on Wizard-of-Wikipedia, a dialogue benchmark. Results show that MixCL reduces hallucination and improves relevancy and factuality in LM-based dialogue agents, matching performance levels of knowledge-based models, but with greater efficiency and scalability.

**Surveys**

1. Survey of Hallucination in Natural Language Generation (Center for Artificial Intelligence Research (CAiRE), Hong Kong University of Science and Technology): [https://arxiv.org/pdf/2202.03629.pdf](https://arxiv.org/pdf/2202.03629.pdf). This survey examines the progress and challenges in addressing hallucinated texts in Natural Language Generation (NLG). It discusses advancements in NLG using deep learning models like Transformer-based language models, leading to improved performance in tasks such as abstractive summarization and dialogue generation. However, the survey highlights the issue of unintended text hallucinations and the negative impact on system performance. It provides an overview of metrics, mitigation methods, and future directions for tackling hallucination in NLG. The survey also covers task-specific research progress in abstractive summarization, dialogue generation, generative question answering, data-to-text generation, machine translation, and visual-language generation. The aim of the survey is to facilitate collaboration among researchers to overcome the challenge of hallucinated texts in NLG.
2. On the Origin of Hallucinations in Conversational Models: Is it the Datasets or the Models? (IBM research, University of Alberta, Mcgill University): [https://arxiv.org/pdf/2204.07931v1.pdf](https://arxiv.org/pdf/2204.07931v1.pdf). This study explores the causes of factually incorrect statements, known as hallucination, in knowledge-grounded conversational models. The researchers conducted a human study on popular benchmarks and state-of-the-art models, revealing that over 60% of the responses were hallucinated. These findings highlight concerns about the quality of datasets and models currently used, with annotations provided for further research.
3. Probing Causes of Hallucinations in Neural Machine Translations (WeChat AI, Tencent, China): [https://arxiv.org/pdf/2206.12529v1.pdf](https://arxiv.org/pdf/2206.12529v1.pdf). The abstract discusses the issue of hallucination in Neural Machine Translation (NMT). Hallucination refers to the generation of fluent but irrelevant translations. The study aims to understand the causes of hallucination through probing methods and improve future architecture designs. The experiments reveal that hallucination is often associated with deficiencies in the encoder, particularly with embeddings, and vulnerable cross-attentions. Interestingly, cross-attention helps to mitigate some errors caused by the encoder.

Comments:
- I really wish there was a strong open source, instruction following model that did NOT hallucinate.  Thank you for this collection of papers.
