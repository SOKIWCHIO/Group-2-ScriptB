Source: Reddit/MachineLearning
URL: https://reddit.com/r/MachineLearning/comments/w31fpp/d_most_important_unsolved_problems_in_ai_research/
Title: [D] Most important unsolved problems in AI research

Content:
[Updated: items marked with * were added/updated based on the responses so far].

Suggesting this topic for discussion, as I am trying to identify the current most important unsolved problems in AI research. Below are a few proposed items that are top of mind for me, would appreciate any input (what to add or what to remove from the list) and relevant sources.

---

Compositionality*. Ability to perform symbolic operations, generalize, including learning from a relatively small set of samples, and get the most out of every sample (sample efficiency and few-shot learning), etc. Also includes the ability to learn by receiving explicit instructions. (e.g. https://arxiv.org/abs/2205.01128)

Multimodality*. Ability to process and relate information from multiple modalities, like text, audio, visual, etc.

Ability to match knowledge to context. For e.g. the text generated by the LLM is a great match for a sci-fi novel, but not as advice to a patient regarding their medical condition.

Uncertainty awareness*. Ability to characterize uncertainty relative to the similarity of the current observations to the training data, explain it to an observer, and adjust behavior if necessary. (https://arxiv.org/pdf/1809.07882.pdf)

Catastrophic forgetting. It is a known limitation to continual learning, however, it seems like the large-scale models show an indication of robustness. (http://www.cognitionresearch.org/papers/overview/sparchai.pdf)

Enabling robust continuous learning in deployment. The current paradigm separates training and inference, while in biology intelligent creatures are capable of continuous learning. 

Figuring out an approach for the messy middle.
- Low-level operations with a focus on a very narrow scope and maximum efficiency seem reasonably straightforward and enjoy growing application in the industry. Noise removing, pattern recognition, recommenders, etc. Specialized ANNs seem to have success there.
- High-level abstract reasoning is being explored by large language and multi-modal models. Like our explicit reasoning (solving a math problem, or learning to operate a new coffee machine) it is extremely powerful, but also slow and resource-intensive. (E.g. https://arxiv.org/abs/2207.05608)
- But there is that middle, as in driving, where we still do fairly complex operations with very high reliability, precision, and responsiveness, all with low cognitive load (figuratively “on autopilot”). 

Explainability* - enabling human experts to understand the underlying factors of why an AI decision has been made.
https://link.springer.com/chapter/10.1007/978-3-031-04083-2_2

Alignment* - ensuring that AI is properly aligned with human values. https://link.springer.com/article/10.1007/s11023-020-09539-2

Energy efficiency. The human brain is believed to consume tens of W of power (https://www.pnas.org/doi/10.1073/pnas.172399499) while less capable LLMs like GPT-3 require several kW (estimated as the power consumption of DGX A100 based on https://www.reddit.com/r/singularity/comments/inp025/if_you_want_to_run_your_own_full_gpt3_instance/). Two orders of magnitude more.

Comments:
- Sample-efficient RL
- Explainable AI is a big one for me. There are some decisions an AI will never be trusted to make if it can't tell you why it reached that conclusion.

I'd also imagine that as ML becomes more widely adopted in day to day life, adversarial attacks will become more common and will become something that it's vital to identify and prevent.
- Hinton on a recent podcast said the open question that interests him most is finding an efficient learning algorithm for spiking neural networks.
- is there still interest in attempting to "model" interactions using existing knowledge in neuroscience to create new AI models?
- A clear definition of *model capacity* and *task difficulty* would be nice to better plan in advance whether (and to what extent) a model can solve a task.

All models are wrong, some are useful. 

I guess, I see myself out and digest the no free lunch theorem.
- Few-shot learning.
- Uncertainty aware models
- Creating a dense learning signal without direct supervision, I would think with a predictive model
- There are hundreds of unsolved problems in AI research outside the ones you mentioned and the most important one to us redditors may not be the most important one to you. For example, I'm very interested in multi modal learning but others may be more interested in topics you mentioned like continuous learning. I'd recommend reading more papers from each of the topics you listed and go from there.
- How to create safe AI agents ("AI alignment") should probably be on the list, too. My own best guess is that most research on this is seeking magic bullets where there are none. I think we'll just have to teach an agent like we teach kids, with lots of corrections of bad behavior. It may even turn out that we have to separately train each agent (rather than making copies), which would dramatically change the economics from what is often envisioned (no cheap labor).
- Many of the founders of deep learning believe the most important unsolved problem in AI research is figuring out how biological brains perform credit assignment, i.e. how they estimate gradients or their equivalent in biological neural networks.

It's really the only missing link from figuring out how e.g. insect brains work at a fundamental level. From there, we can just follow a series of evolutionary leaps from simple invertebrate brains until we get to birds and mammals.
- In my view, composing different concepts without requiring additional training is the next big thing.
- Explicit short-term working memory for LLMs. So they don't forget things they told you 100 words ago and contradict themselves every other sentence.
- Executive control is still largely missing from AI, as many current AI systems are like small subsections of a brain without the centralized control structure.
- Alignment
- AI to clean up data before it going into the ML analysis.
- What ever happened to the goal of good old fashioned AI (GOFAI)?
- > Energy efficiency. The human brain is believed to consume tens of W of power

I foresee multi-task learning solving this later by giving higher level programs more important information to work with. This is related to what I think is one of the most important problems - applying multi-task learning to computer vision. I wrote a post a while ago about how [event cameras](https://www.reddit.com/r/MachineLearning/comments/ntii8i/d_i_think_all_vision_researchers_should_be_using/) play into this. Neuromorphic sensors, like event cameras, offer the basis for low-powered and higher sample rate inputs for models.

The big picture for multi-task computer vision would be to use two event cameras as input and extract nearly every output possible. With per-pixel variable rate sampling a futuristic event camera has the potential to algorithmically focus on regions of importance at over 10K Hz without motion blur or exposure issues. A model can essentially create a saliency map utilizing all its tasks to judge what is important moment to moment and increase or decrease the sample rate in those areas. The output from the model would be SLAM (and raw keypoints), depth, semantic segmentation, object identification, velocity map, depth, lights (location, size, and properties), materials, shadow removal, mirror identification, pose, face tracking, hand tracking, temporal super resolution, etc. Higher level algorithms would then construct a 3D scene and could throttle areas of the scene for samples and feed this into the network as further input as it refines. This behavior would be similar to walking into a room for the first time and your eyes glance around identifying the structure and objects. As viewed from a 3D saliency map there would be spikes as the data is collected and then a drop everywhere as only dynamic objects require further samples. Imagine a TV is on in a living room for example. If you've been there before very little energy is expended ideally to reposition oneself, but the content on the TV has your focus. In the very big picture as the vision system collects data and applies this multi-task learning it builds a memory of every known 3D object that further reduces energy consumption by rapidly identifying known entities and ignoring them to focus on new data. At the application level these known objects can be placed in the 3D scene to fill in missing data creating a predicted fuzzy view of the world. Like identifying part of a coffee cup and filling in the rest without walking around it. As more data is input, temporal algorithms resolve the ground truth over time.

I digress, but part of this is creating models also that rapidly lookup information based on fuzzy information using very little energy. Seeing low resolution objects with a few samples and quickly classifying it based on potentially spatial factors. Things like it was on a desk and is small and cylindrical, so it's probably a cup or something like it.

A big reason why I think this is so important to solve is because of the impact I see it having in mixed reality where headsets need to be relatively low powered. They can offload some things to edge compute, but for responsiveness and privacy they'll need to perform a lot of understanding by themselves in real-time. Imagine walking in front of a mirror and seeing yourself with different clothes requiring the vision system to perform many simultaneous tasks at once to get a result. A lot of this also extends to robotics and automation. A robotic arm might not need say pose tracking, but if it was standard in all models (to help with understanding of the world) then a safety system could automatically trigger a shutdown. I foresee these kind of general purpose highly optimized chips later being created. People will ask "well why not simplify it so it can't output shadows or lights" and the answer will be "it lowers the quality of every other task". (This will invariably lead to situations like [Toy Soldiers](https://www.youtube.com/watch?v=KSu4Z9V8YEg), but I assume that'll be expected by then when vision chips are that general purpose).
- We perceive the world through signals.  As time passes, information changes just because it was perceived say a microsecond ago.  Some information has to be represented in terms of time.  Spiking ANNs are well suited for that.  I think this is one of the most important problems in AI.

Here are some of my scribblings on that topic: https://github.com/rand3289/PerceptionTime
- So what is between me and a virtual assistant that is always on and conversational, remembers everything I want it to and is able to help me to write a book and stuff?
