Source: Reddit/MachineLearning
URL: https://reddit.com/r/MachineLearning/comments/1ncrkpp/d_negative_r²_on_unseen_dataset_despite_good/
Title: [D] Negative R² on unseen dataset despite good train/test performance

Content:
I am working on a regression problem where I predict Pavement Condition Index (PCI) values from multi-sensor time-series data collected in the same region and under the same conditions. I have multiple sets of data from the same collection process, where I use some sets for training and testing and keep the remaining ones for evaluating generalization. Within the training and testing sets, the model performs well, but when I test on the held-out dataset from the same collection, the R² value often becomes negative , even though the mean absolute error and root mean square error remain reasonable. I have experimented with several feature engineering strategies, including section-based, time-based, and distance-based windowing, and I have tried using raw PCI data as well. I also tested different window lengths and overlap percentages, but the results remain inconsistent. I use the same data for a classification task, the models perform very well and generalize properly, yet for PCI regression, the generalization fails despite using the same features and data source. In some cases, removing features like latitude, longitude, or timestamps caused performance to drop significantly, which raises concerns that the model might be unintentionally relying on location and time information instead of learning meaningful patterns from sensor signals. I have also experimented with different models, including traditional machine learning and deep learning approaches, but the issue persists. I suspect the problem may be related to the variance of the target PCI values across datasets, potential data leakage caused by overlapping windows, or possibly a methodological flaw in how the evaluation is performed. I want to understand whether it is common in research to report only the R² values on the train/test splits from the same dataset, or whether researchers typically validate on entirely separate held-out sets as well. Given that classification on the same data works fine but regression fails to generalize, I am trying to figure out if this is expected behavior in PCI regression tasks or if I need to reconsider my entire evaluation strategy.

Comments:
- What do the distributions look like between the two different domains? Is it possible that the training set domain as a very different distribution of values? Is your training set imbalanced? Does it cover the same range of values? Plot it all out!

>whether it is common in research to report only the R² values on the train/test splits from the same dataset, or whether researchers typically validate on entirely separate held-out sets as well.

Unless you're doing work on domain adaptation, this is the common approach. There's no guarantee your model will generalize under domain shift. But you SHOULD show it.
- I suggest removing the latitude, longitude, and timestamps, as that is probably allowing the network to memorize data. Use 10 fold cross validation with early stopping in a LSTM with raw sensor data including all observations from the previous 12 seconds, optimize hyperparameters using evolutionary search or optuna for the best set of hyperparameters across all 10 folds (one set total, not one set per fold), and create an ensemble for testing on the unseen dataset.
- > In some cases, removing features like latitude, longitude, or timestamps caused performance to drop significantly, which raises concerns that the model might be unintentionally relying on location and time information instead of learning meaningful patterns from sensor signals.

I think this is an area worth more exploration. Some kind of data leakage was my first thought when I read your title.

What sensors are you using and what do they output? Time series of a few variables, at various locations? Or something else?
- You might check the data distribution also do [KS-Test](https://www.geeksforgeeks.org/machine-learning/kolmogorov-smirnov-test-ks-test/) for your training as well as testing data there might be some thing to see.
- You're absolutely right about examining sensor input distributions. I focused on PCI target distributions but haven't thoroughly analyzed the sensor feature space. My data consists of 6 core sensors (accelUserX/Y/Z, gyroX/Y/Z) sampled at 33Hz plus speed data. Each dataset represents different collection sessions, and I suspect there might be a sensor domain shift between sessions due to environmental conditions or sensor calibration drift. I create features using 12-second sliding windows, calculating statistical measures like mean, std, RMS, and frequency domain features. The model utilizes these engineered features, rather than raw coordinates. When I remove latitude/longitude features, performance drops significantly, which suggests that the model might be learning dataset-specific patterns rather than robust sensor-road relationships. But I want to make the model domain adaptive. All data is collected from Phoenix, Arizona; therefore, there should be little to no difference in the road pattern. Also, the same vehicle, same sensor collection settings had been used.
- I checked the distributions, and they are almost similar. For example, in one dataset chunk, the mean PCI is approximately 56, while in another held-out set, it’s approximately 62; thus, the overall shift isn’t huge. To analyze this, I created different chunks of training data and tested on the held-out sets. I did notice that the target variance differs slightly across datasets, but not drastically. The training set is somewhat imbalanced , most of the PCI values fall between 45 and 75, but it still covers a reasonable range. I also tried Gaussian smoothing and a few other preprocessing techniques, but they didn’t improve generalization. Given that the distribution shift doesn’t seem severe, what else would you recommend checking to understand why R² on the held-out data remains negative?
- My sensors output time-series data at 33Hz  (accelerometer X/Y/Z, gyroscope X/Y/Z, speed) with GPS updates at \~1Hz. I collect data on different road segments over multiple sessions, creating 12-second sliding windows with \~400 samples each to calculate statistical features like mean, std, RMS, and frequency domain features. The model utilizes these engineered features, rather than raw coordinates.
- I’m using accelerometer (x, y, z) and gyroscope (x, y, z) signals, collected as time series, and the goal is to relate those sensor-derived features to PCI values.
- I can't really offer complete advice, but definitely first try regularization strategies. Maybe pass the model and the inputs to ChatGPT and ask for some regularization strategies to try, and see if they improve generalization at all. If that doesn't work, start looking further.   
  
A paper I really realy like is this Deep Imbalanced Regression paper: [https://dir.csail.mit.edu/](https://dir.csail.mit.edu/)  
This is for imbalanced output distributions but it should give you an idea of thigns to look for.
- I actually don't know if you've demonstrated the distribution shift isn't severe. You also need to look at the sensor inputs. Are there areas that are covered in the training set and not the out-of-distribution test set?
- Interesting. Can you quantify the variance and imbalance?
- The dataset is imbalanced. 35% of the PCI values fall between 50 and 65. For the ranges of 0-20 and 90-100, there is little to no data available. I tried using LDS, over-sampling, and inverse weighting. None of those worked out. Little to no improvement.
