Source: Reddit/MachineLearning
URL: https://reddit.com/r/MachineLearning/comments/rai9r9/d_the_reviews_and_my_rebuttals_for_my_rejected/
Title: [D] The reviews and my rebuttals for my rejected ICML and NeurIPS submissions

Content:
To increase transparency about the review process, and in the hopes that it might help others shape their papers/rebuttals, I'm sharing the reviews and my rebuttals for [my ICML submission of `baller2vec`](https://github.com/airalcorn2/baller2vec/issues/3), [my NeurIPS submission of `baller2vec`](https://github.com/airalcorn2/baller2vec/issues/4), and [my NeurIPS submission of `baller2vec++`](https://github.com/airalcorn2/baller2vecplusplus/issues/1), each of which was rejected.

Notably, I received the same, highly negative reviewer on all three submissions (Reviewer #2, Reviewer 7Vsi, and Reviewer GRxC, respectively), which was a bummer (particularly because it's clear that the reviewer's negativity caused another reviewer to lower their score on both the ICML and NeurIPS submissions of `baller2vec`). The reviewer claimed to be "an expert in both sport and machine learning" yet repeatedly mischaracterized and misunderstood aspects of the models, going as far as to state, "The Methods (theory) section is explained in a needlessly complicated and uninformative manner" in their ICML review. In contrast, the other three ICML reviewers said our "writing is very clear" (Reviewer #5), they "quite enjoyed reading this paper" (Reviewer #1), and our writing was "Very clear" (Reviewer #4). Even ignoring their ratings, I think this raises important questions about what it means to be "peer reviewed". Given that there are probably many hundreds (thousands?) of researchers in the field of multi-agent spatiotemporal modeling, why was this same individual given three opportunities to judge my work on behalf of the community? Why are their anonymous opinions considered representative? 

Additionally, I found the dataset criticisms from some of the reviewers incredibly frustrating (notably, the above reviewer did not mention anything about additional datasets in their ICML review, but did in their NeurIPS reviews). At least two other multi-agent trajectory modeling papers that were accepted to NeurIPS this year only evaluated their methods on a single, large real-world dataset: (1) "[GRIN: Generative Relation and Intention Network for Multi-agent Trajectory Prediction](https://openreview.net/forum?id=ephWA7KaWmD)" evaluated their method on a small simulated dataset (50K training sequences) and a preprocessed NBA dataset (100K training sequences), and (2) "[Collaborative Uncertainty in Multi-Agent Trajectory Forecasting](https://openreview.net/forum?id=sO4tOk2lg9I)" evaluated their method on the nuScenes dataset (1,000 scenes) and Argoverse (206K training sequences), so it doesn't seem like this expectation is evenly applied by reviewers.

Comments:
- I understand feeling frustrated about negative reviews. I've been there myself - we all have.

But can you really call foul about a biased reviewer asking for more datasets when 3/4 reviewers agreed that you needed to evaluate on more datasets? 3/4 reviewers also felt that you needed to include more baselines and/or that your current baseline comparisons were flawed.

Okay, maybe the reviewers disagreed about how clear your prose was, but that's pretty minor compared to the dataset and baseline criticisms. And when 3/4 reviewers have the same criticisms, it's a pretty open and shut case for the meta reviewer.

On the bright side, it seems that you have pretty concrete points to improve on if you want to resubmit.
- I find data set critiques to not be applied fairly too. It is indeed quite sad
- How are you so sure it's the same reviewer. The chances of a reviewer getting the same paper (and the extension paper) and across multiple conferences is very low. The PC chair doesn't have access to prior interaction between the reviewers and the paper when they distribute the assignments, especially not across different conferences. I only skimmed the three reviews, but to me, they feel like different people. Like you say, they have different requests.

As for inconsistency between reviews for papers, that comes down to the realities of submitting to a conference. Luck in getting an easy reviewer is a huge factor in getting accepted to a conference. 

This is the biggest difference between submitting to a conference versus a journal. Conference reviewers are given 3-6 papers and a short review term. There are also only a few interactions between the authors and the reviewers. Because of this, conference reviewers read your paper searching for a reason to reject your paper. If they find the one reason that they care about, they reject. One rejection is enough to get rejected by the PC chair. In your case, the reviewer's thing that they cared about was datasets. For those other papers,they were lucky enough to get reviewers that don't care about datasets. To get accepted to a top conference, you need to make sure there are no glaring flaws ( lack of novelty is a huge one) and have a lot of luck.

Journals are the opposite. Journal reviewers aim to actually improve your computer. This is because revisions of a paper will keep on being assigned to the reviewer. To get accepted to a journal, you just need to keep on revising until the reviewers can't complain anymore and have to accept your paper.
- Thanks for taking the time to share this.  I know it's not easy to share negative feedback, but I think there are many people that will learn a lot from this.
- I am hoping the reviews can be open and democratized. That's why we started [https://doublind.com/](https://doublind.com/), an IMDB for research papers.
- Different reviewers put different weights on different parts of the paper, that's why all papers are not held to the same standard.  Although in your case, you are not put down was one reviewer, even if 7Vsi was not here, your paper would still likely be rejected. Almost all reviewers agree that you should need more datasets/baseline. You might think it's unfair because some papers get "lucky" and get accepted without it, but at least all your reviews are relatively consistent and you know what to work on to improve it to make it good enough for A\* conferences. Sometimes the idea is so cool/novel, that reviewers accept weaker baseline comparisons and datasets, but in your case, you agree in your rebuttal that "b2v is not a major technical innovation," so it should at least be thoroughly evaluated to be convincing.

Also, in my experience, paper being "clearly written" is not a real strength, it's something reviewers add when they don't really know what strength to point out and need to say something positive, so I wouldn't count on it.

I think your rebuttals are also a bit weak, the goal is to show willingness to improve the paper and to give ammunitions for reviewers who like your work to defend it, yet, you agree with the reviewers that b2v is not a "major technical innovation" and you are extremely unwilling to address the two main concerns (database and baseline). Especially, the argument that previous work only uses one dataset so our paper can use one dataset is a terrible argument that doesn't make you look good.
- Do you get invited to review in ICML if you have accepted papers in Neurips? When does this invite usually come?
- >But can you really call foul about a biased reviewer asking for more datasets when 3/4 reviewers agreed that you needed to evaluate on more datasets?

The biased reviewer was a separate point from the datasets point, and I do think I can and should point out when papers aren't held to the same standards.

>3/4 reviewers also felt that you needed to include more baselines and/or that your current baseline comparisons were flawed.

I didn't really intend for this post to be another rebuttal, but, for posterity, I *did* compare to a single agent Transformer as requested by one of the reviewers. Another reviewer stated, "It would have been prudent to compare the performance of their model against the most applicable SOTA methods", but the GRNN baseline *is* the most applicable SOTA method as a permutation invariant model (with respect to the agents) that had been applied to a basketball dataset (the reviewer did not make any specific suggestions). Lastly, the highly negative reviewer asserted that our "baseline [had] not [been] properly employed and tested" and suggested "Comparing to Yeh et al directly would be a better approach". As we pointed out to that same reviewer in our ICML rebuttal, the code for Yeh et al. (2019) is not publicly available. Further, as that reviewer should be aware given their familiarity with Yeh et al. (2019), the model’s hyperparameters were not included in their paper, which made direct comparison to their approach impossible.
- That's because reviewers are human and not machines.
- the chance of getting the same reviewer, especially if they are vehement, negative, and opinionated, are actually pretty high because of the bidding process. if I rejected a paper as a reviewer where I detected something seriously wrong (wrong math, plagiarism, etc.) I \*always\* check during the next conference's bidding process if the paper is re-submitted and then I bid on it. Given that for most papers, there will be very few "willing and eager" bids, I'm very likely to get it. Likewise other reviewers might not necessarily find plagiarism, but may just want to make their life easy and review papers that they have reviewed before and already have an opinion formed, or they think that the one missing dataset really should bar that paper from all conferences ever. Whatever it is, it is not unlikely to get the same reviewers, and in fact almost everyone I know has at least one story about reviewers clearly "stalking" their papers over multiple conferences, sometimes even starting their reviews with "I already reviewed this paper at X...".
- > How are you so sure it's the same reviewer.

[The reviewer literally admitted as much on their NeurIPS review of `baller2vec`.](https://openreview.net/forum?id=OsBM0sWwn_-&noteId=kXlKyPoMqy) I'm assuming it was the same reviewer for `baller2vec++` based on many of the same criticisms popping up (e.g., not liking binning and not thinking NLL was a good measure of model performance, which none of the other reviewers on any of the submissions mentioned), the general negativity *and* confidence, and, frankly, their general incompetence. As just one example, here's one of the reviewer's criticisms of `baller2vec++`:

>The statement in 22-24 that these models treat the trajectories as independent is untrue. Something like Social-LSTM naturally contains information about past states. Felsen 2018 explicitly conditioned on past trajectories to predict future motion. The statement that the macro-intents in Zhan et al. are independent is not quite true either.

and my reply:

>Lines 22-24 read:

>>However, nearly all multi-agent spatiotemporal models (e.g., [1-6]) implicitly (through their loss functions) assume the trajectories of the agents at each time step are statistically independent given the agents' previous locations (Figure 1).

>We believe we were quite precise here, and we included Figure 1 to make our motivation even more explicit. Additionally, all of the predicted trajectory outputs in Felsen et al. (2018) are conditionally statistically independent of one another because they are only derived from the input trajectories. Lastly, the full architecture of Zhan et al. (2019) consists of two recurrent neural networks: (1) a trajectory prediction network and (2) a macro-intents prediction network (as found in the authors’ implementation here, which we linked to in a footnote). The shared macro-intent for the agents at each time step in the trajectory prediction network is the concatenation of the predicted individual macro-intents, which are conditionally statistically independent as revealed through their implementation.
- >To get accepted to a top conference, you need to make sure there are no glaring flaws ( lack of novelty is a huge one) and have a lot of luck.

In my opinion, the "novelty" criticism is also unevenly applied. If the architecture isn't novel enough (and I'll reemphasize the point I made in my rebuttals that the fact no one had published a similar architecture anywhere suggests it wasn't a priori obvious), why aren't the qualitative results, which I find pretty cool, novel enough? What makes simply using a multivariate Gaussian loss, which was the case for "Collaborative Uncertainty in Multi-Agent Trajectory Forecasting", particularly novel?
- I don't have any experience with motion prediction or sports, so I'm not qualified to comment on the technical specifics.

But let's talk about the human component. So far, you have similar comments across multiple submissions and across multiple reviewers within each submission. That leaves nearly a dozen reviewers that disagree with your assessment. Even if you are 100% right and they are all 100% wrong, your odds of convincing that many people that they are wrong is pretty much zero.

Getting through peer review is about placating the reviewers. Even when they ask for dumb things. You do have some currency to push back, but it's limited. Pushing back on the same major comment from three different reviewers a is a long shot. And I don't think what the reviewers are asking for here is totaly unreasonable (i.e. even if it's unecessary/superfluous, it doesn't seem that it's bad science that would subtract from the quality of the paper)

So you have to ask yourself, is this a hill worth dying on? I'm not passing judgment - if you feel that these changes would debase your paper then I can understand your viewpoint. But the cold, hard reality is that you have two choices: compromise and publish or don't compromise and don't publish.
- Humans are biological machines.
- No two reviewers take the same approach to writing reviews. This is what I meant by not being machines. Each researcher is going to have what they think it's important and want to focus on. Expecting all reviewers to agree is a good way to get upset.
- > [...] almost everyone I know has at least one story about reviewers clearly "stalking" their papers over multiple conferences"

Yannic hit the nail on the head. In my experience, reviewers want the easy way for which less groundwork is needed. Some of the times it is a good thing - it helps the evaluation quality and improves upon the existing feedback. Other times, it has been used to settle opinions and vendettas. In 2018-19, for a IEEE sponsored conference (on which I was a SAC), we had to shoot warnings directed to two authors & their affiliate students who were crazily trashing each other's papers during reviews. (They of course bid on each other's papers, as previous arxiv preprints made it easy to guess authorship). In my case, a CVPR submit and subsequent resubmit to another conference and coincidentally also the journal version, were reviewed by the same person. And trashed without much justification. I had to write to the IEEE Transaction editor to point out the issues in the review & he/she was replaced.
- Isn't this what the area chairs are for? Wouldn't this be something you attempt to notify them of if you recognize the paper as something that had huge issues? I don't understand how your method doesn't bias the double-blind review process in an extremely negative way.
- But, for these papers the reviews are very mild. The rejection reason are very normal, low novelty, lack of datasets, lack of comparisons, minor clarification issues, etc. It's nothing scathing and nothing worth stalking. In addition, the reviews are detailed, have different complaints, and have different writing styles.
- >sometimes even starting their reviews with "I already reviewed this paper at X...".

[That's exactly what happened here.](https://openreview.net/forum?id=OsBM0sWwn_-&noteId=kXlKyPoMqy)
- I any case, getting rejections is a reality of publishing at top conferences. NeurIPS has an acceptance rate of 20%. That means there were about 8000 other papers that were rejected. That's 8000 passion projects and 8000 papers with comparable work put in to yours, all rejected. 

Not every accepted paper is good and not every rejected paper is bad. But having super common flaws like not enough comparative methods, few datasets, insufficient discussion, lack of algorithm novelty, etc., hurts your chances to please all of the reviewers and put your paper in the top 20%. 

You have a chance to fix your paper.  I have over 50 publications and writing papers becomes formulaic. Just make sure it doesn't have the common flaws and reviewers will have a hard time rejecting your paper. 

You also have to consider if ICML or NeurIPS is a good fit for your paper. Those conferences are more algorithm and theory based. Not as much as AISTATS but more than CVPR. They aren't as friendly to new applications of methods. 

Another thing, personally, I think your rebuttal is too aggressive. You want to the reviewers to look at your paper in a better light so they increase their score. Arguing with the reviewers isn't going to change their mind.
