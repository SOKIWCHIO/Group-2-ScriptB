Source: Reddit/MachineLearning
URL: https://reddit.com/r/MachineLearning/comments/1mfqaqz/r_kimi_k2_open_agentic_intelligence_technical/
Title: [R] Kimi K2: Open Agentic Intelligence (Technical Report)

Content:
The Moonshot AI team behind the recent [Kimi K2](https://x.com/Kimi_Moonshot/status/1943687594560332025) model, one of the leading open-weights LLM, just released the technical report: https://arxiv.org/abs/2507.20534

---

**Kimi K2: Open Agentic Intelligence**

*We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32 billion activated parameters and 1 trillion total parameters. We propose the MuonClip optimizer, which improves upon Muon with a novel QK-clip technique to address training instability while enjoying the advanced token efficiency of Muon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero loss spike. During post-training, K2 undergoes a multi-stage post-training process, highlighted by a large-scale agentic data synthesis pipeline and a joint reinforcement learning (RL) stage, where the model improves its capabilities through interactions with real and synthetic environments.
Kimi K2 achieves state-of-the-art performance among open-source non-thinking models, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on Tau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench Verified, and 47.3 on SWE-Bench Multilingual -- surpassing most open and closed-sourced baselines in non-thinking settings. It also exhibits strong capabilities in coding, mathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6, 49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on OJBench, all without extended thinking. These results position Kimi K2 as one of the most capable open-source large language models to date, particularly in software engineering and agentic tasks. We release our base and post-trained model checkpoints to facilitate future research and applications of agentic intelligence.*

---

Recently, there has been discussions about Muon and MuonClip, which the Moonshot AI team has developed for training Kimi. See recent discussions here on r/MachineLearning : https://old.reddit.com/r/MachineLearning/comments/1m2y23l/p_understanding_muon_a_revolutionary_neural/

Comments:
- Open‚Äësource LLMs are moving *fast* üòÆ. K2 pulling these numbers without a long chain‚Äëof‚Äëthought is kinda wild. MuonClip feels like a smart glow‚Äëup for Muon ‚Äî stable training over **15T+ tokens** with zero loss spikes is no joke.

Then you add that massive RL + agentic data synthesis stage, and it makes sense why it‚Äôs crushing SWE‚ÄëBench and LiveCodeBench. This is exactly the kind of ‚Äúagentic‚Äù capability people have been hoping for in open models.

If it performs this well outside benchmarks, we might actually see devs and researchers relying way less on closed models.
- I love k2. Like a physics sparring partner that gets the weird math.
