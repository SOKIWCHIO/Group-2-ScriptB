[
  {
    "filename": "2539406e10744f2aee4aa82abb3e5c23.txt",
    "text": "08 app abdullah k abdullah k 5 aware trajectory prediction via rule regularized heteroscedastic deep classification recent advancement deep based trajectory prediction shown significant potential capturing intricate interaction however challenge persist distribution generalization primarily due imbalanced datasets insufficient diversity hinder robustness calibration tackle issue novel framework shift spectral heteroscedastic informed forecasting trajectory introduced framework innovatively integrates well calibrated modeling informative prior derived automated rule extraction shift reformulates trajectory prediction classification task utilizes heteroscedastic spectral normalized gaussian process effectively separate epistemic aleatoric uncertainty informative prior learned training label automatically generated natural language driving rule stop rule drivability constraint leveraging retrieval augmented generation framework powered large language extensive evaluation nuscenes dataset reveal shift surpasses state art method achieving significant improvement calibration displacement metric notably demonstrates exceptional performance complex scenario like intersection uncertainty inherently elevated algorithm artificialintelligence dl d datascience deeplearning ml machinelearning tech technology trajectoryprediction uncertaintymodeling aware trajectory prediction via rule regularized heteroscedastic deep classification arxiv org 2 1 134 3000 latest development deep application chal",
    "cluster": 0,
    "keywords": "neural, network, abdullah, prediction, framework, quantification, bayesian, performance, algorithm, deep"
  },
  {
    "filename": "2b61c1892476d4e2789790d03718d246.txt",
    "text": "08 app abdullah k abdullah k 7 exploring architectural inferential inductive bias exchangeable sequence modeling recent advancement autoregressive highlighted efficacy modeling exchangeable sequence particularly addressing observation conditioned latent factor approach offer robust framework quantifying arising missing rather relying solely latent variable critical role posterior inference decision making process active bandit problem prompted investigation effective inductive bias exchangeable sequence modeling significant limitation conventional single step generation method inability differentiate epistemic aleatoric contrast multi step autoregressive generation approach advocated within bayesian statistic demonstrating enhanced quantification translates improved performance downstream decision making task furthermore analysis recent transformer architecture reveals notable gap may fail guarantee exchangeability incurring substantial computational cost controlled synthetic experiment illustrate custom architecture significantly underperform standard causal mask emphasizing urgent need innovative architectural solution algorithm artificialintelligence autoregressivemodels bayesianstatistics dl d datascience deeplearning ml machinelearning tech technology transformer uncertaintyquantification architectural inferential inductive bias exchangeable sequence modeling arxiv org 1 134 3000 trend architecture understanding transformer artificial intelligence identifying source bias quantization transfo",
    "cluster": 0,
    "keywords": "neural, network, abdullah, prediction, framework, quantification, bayesian, performance, algorithm, deep"
  },
  {
    "filename": "31c1ad757f8ed0640b4bcc1c1038116d.txt",
    "text": "08 app abdullah k abdullah k 5 conformalized generative bayesian imaging quantification framework computational imaging quantification pivotal establishing trust based computational imaging methodology recent innovation generative modeling bayesian neural network paved way advanced aware image reconstruction technique study introduces scalable framework adeptly quantifies aleatoric inherent epistemic related uncertainty integrating generative based posterior sampling method bayesian neural network framework enhances quantification latent variable deep ensembling moreover incorporation conformal prediction methodology ensures rigorous calibration enabling reliable assessment evaluation across magnetic resonance imaging computed tomography image inpainting demonstrate framework effectively capture true characteristic type uncertainty finding also reveal applying conformal prediction yield marginal coverage guarantee aligned frequentist principle advancement represents significant step towards reliable computational imaging solution algorithm artificialintelligence bayesianmodeling computationalimaging dl d datascience deeplearning generativemodels ml machinelearning tech technology uncertaintyquantification conformalized generative bayesian imaging quantification framework computational imaging arxiv org 1 133 3000 generative update trend latest trend 3d reconstruction emerging innovation imaging assessing reliability generative generative impact risk assessment enterprise ready generative solution",
    "cluster": 0,
    "keywords": "neural, network, abdullah, prediction, framework, quantification, bayesian, performance, algorithm, deep"
  },
  {
    "filename": "374e1f50d736014ee1a9176db357c264.txt",
    "text": "08 app abdullah k abdullah k 5 exciting advancement multimodal horizon recent study titled sure enhancing multimodal pretraining missing modality estimation introduces sure scalable reconstruction estimation groundbreaking framework designed tackle challenge posed incomplete datasets real world application key highlight integration diverse source multimodal shown remarkable success yet often assumes availability modality rarely case addressing missing modality sure enhances pretrained multimodal reconstructing missing modality providing reliable estimate innovative loss function introduction pearson correlation based loss statistical error propagation technique allows precise quantification uncertainty missing prediction robust performance across task extensive experiment demonstrate sure achieves state art performance sentiment analysis genre classification action recognition framework improves interpretability also enhances overall performance even faced incomplete airesearch algorithm artificialintelligence dl d datascience deeplearning ml machinelearning multimodallearning tech technology uncertaintyestimation sure enhancing multimodal pretraining missing modality estimation arxiv org 1 1 133 3000 improving multimodal performance emerging trend multimodal understanding multimodal processing advantage multimodal solution benefit multimodal approach multimodal reasoning technique career productivity finance soft skill emotional intelligence p",
    "cluster": 0,
    "keywords": "neural, network, abdullah, prediction, framework, quantification, bayesian, performance, algorithm, deep"
  },
  {
    "filename": "522188a68cdde8abc5320b4fd91791c0.txt",
    "text": "08 app abdullah k abdullah k 6 disentangling uncertainty system dynamic recent advancement estimation led development compressed representation cdrm effectively distinguishes aleatoric inherent randomness epistemic scarcity within learned regressive system dynamic innovative framework leverage neural network encode distribution facilitating direct sampling output distribution without limited gaussian prior key highlight cdrm include novel inference procedure utilizes langevin dynamic sampling enhanced prediction capability theoretical advantage demonstrates improved memory computational efficiency compared traditional bin based compression method empirical success achieves area receiver operating characteristic curve aurocs 0 8876 aleatoric 0 9981 epistemic uncertainty showcasing superior performance complex datasets multimodal output distribution research significantly contributes risk aware control reinforcement efficient exploration robust policy transfer dynamic system algorithm artificialintelligence dl d datascience deeplearning ml machinelearning neuralnetworks reinforcementlearning tech technology uncertaintyestimation disentangling uncertainty compressed representation arxiv org 1 133 3000 future trend reinforcement understanding dynamic memory system tip growing understand neural network llm diverse datasets equitable challenge benefit deep career productivity finance soft skill emotional intelligence project management education",
    "cluster": 0,
    "keywords": "neural, network, abdullah, prediction, framework, quantification, bayesian, performance, algorithm, deep"
  },
  {
    "filename": "66dc8894ff61f239e5bdb61a20cc53df.txt",
    "text": "08 app abdullah k abdullah k 6 quantification uncertainty probabilistic deep neural network recent advancement neural network architecture demonstrated exceptional accuracy however often exhibit reliance training resulting challenge related interpretability overfitting particularly smaller datasets probabilistic neural network utilizing variational inference offer solution incorporating estimation weight distribution yet conventional variational inference typically employ single density approximation may yield suboptimal posterior estimate compromise performance introducing boosted bayesian neural network bbnn innovative approach enhances weight distribution approximation implementing boosting variational inference bvi iteratively constructing mixture density bvi significantly expands approximating family lead expressive posterior distribution enhances generalization estimation bbnn approach increase computational complexity result approximately five percent higher accuracy compared traditional neural network providing superior quantification advancement particularly crucial high stake application medical diagnostics implication false negative severe finding underscore potential leveraging mixture based variational family improved posterior approximation within probabilistic deep framework algorithm artificialintelligence dl d datascience deeplearning ml machinelearning neuralnetworks probabilisticmodels tech technology variationalinference quantification uncertainty probabilistic deep neural network implementing boosting variational inference arxiv org 1 133",
    "cluster": 0,
    "keywords": "neural, network, abdullah, prediction, framework, quantification, bayesian, performance, algorithm, deep"
  },
  {
    "filename": "70951d2a73eca893e39a06273e8cb200.txt",
    "text": "08 app abdullah k abdullah k 7 exploring quantification neural network recent advancement field neural network highlighted challenge associated quantification method dropout bayesian neural network laplace approximation traditional approach often face issue underfitting high computational demand particularly applied large scale datasets groundbreaking study titled post hoc quantification pre trained neural network via activation level gaussian process researcher propose novel framework shift focus weight space activation level achieved introduction gaussian process activation function gapa effectively capture neuron level uncertainty preserving original mean prediction study present two innovative method 1 gapa free method utilizes empirical kernel training hyperparameter optimization ensuring high efficiency training 2 gapa variational approach employ gradient descent hyperparameter kernel providing enhanced flexibility empirical result indicate gapa variational consistently outperforms laplace approximation across various datasets based multiple quantification metric research open new avenue practical application neural network uncertain environment enhances reliability decision making process algorithm artificialintelligence dl d datascience deeplearning gaussianprocesses ml machinelearning neuralnetworks tech technology uncertaintyquantification post hoc quantification pre trained neural network via activation level gaussian process arxiv org 1 134 3000 neural net",
    "cluster": 0,
    "keywords": "neural, network, abdullah, prediction, framework, quantification, bayesian, performance, algorithm, deep"
  },
  {
    "filename": "8b6722570db49874b80872fff7fa978f.txt",
    "text": "08 app abdullah k abdullah k 6 exploring bayesian neural network architecture evaluation neural network evolve complexity challenge achieving high predictive performance alongside robust quantification intensifies paper delf intricacy bayesian neural network highlighting limitation commonly used posterior approximation implication accuracy key insight include computational burden associated markov chain monte carlo mcmc method hinder comprehensive exploration multimodal posterior due high cost variational inference present efficient alternative often lack asymptotic guarantee sampling based method provide leading potential concentration around single mode architectural choice significantly influence performance thus study evaluates computational cost accuracy quantification across various scenario innovative technique averaging ensembling examined potential enhance posterior exploration notably variational inference demonstrated superior quantification compared mcmc method furthermore ensemble variational approximation achieved comparable accuracy substantially reduced cost research underscore importance architectural consideration bayesian neural network improved predictive capability algorithm artificialintelligence bayesianneuralnetworks dl d datascience deeplearning mcmc ml machinelearning tech technology uncertaintyquantification variationalinference architecture evaluation bayesian neural network arxiv org 1 133 3000 ensemble improves prediction evaluation technique evaluate performance understand neural network",
    "cluster": 0,
    "keywords": "neural, network, abdullah, prediction, framework, quantification, bayesian, performance, algorithm, deep"
  },
  {
    "filename": "979c9ab71a5abba5c2782c1960c051d8.txt",
    "text": "08 app abdullah k abdullah k 6 unveiling power stellar dating realm astronomy astrophysics rigorous management uncertainty paramount ensuring credible outcome integration artificial intelligence present transformative opportunity enhance process study introduces hierarchical bayesian architecture employ neural network probabilistic relationship specifically aimed forecasting stellar attribute mass radius age addressing observational uncertainty measurement epistemic uncertainty inherent predictive architecture generates distribution encapsulate potential value range prediction focus dating main sequence star method known chemical clock utilizing hierarchical architecture optimize information extraction datasets accounting correlation stellar parameter versatility bayesian neural network allows effective capture complex relationship result indicate integrating machine within bayesian framework error propagation managed effectively yielding prediction broader margin facilitating conservative estimate stellar dating remarkably age prediction achieved mean absolute error less 1 billion year star within test dataset innovative approach underscore importance quantification astronomical research highlight advanced modeling technique enhance understanding stellar evolution algorithm artificialintelligence astronomy bayesianneuralnetworks dl d datascience deeplearning ml machinelearning stellardating tech technology uncertaintyquantification unveiling power journey bayesian neural networ",
    "cluster": 0,
    "keywords": "neural, network, abdullah, prediction, framework, quantification, bayesian, performance, algorithm, deep"
  },
  {
    "filename": "9cbe490ff3852a47d3804aafc9c7c39c.txt",
    "text": "08 app abdullah k abdullah k 6 exploring advanced quantification neural network traditional neural network regression often provide point estimate neglecting critical aspect predictive probabilistic neural network pnns emerged solution generating output distribution facilitate construction prediction interval however prevalent assumption gaussian output distribution lead excessively wide interval particularly faced outlier non normal address limitation novel approach known distributed neural network tdistnns proposed tdistnns produce distributed output characterized location scale degree freedom parameter flexibility allows modeling heavy tailed predictive distribution enhances robustness non gaussian tailored loss function distribution developed alongside efficient gradient computation ensure seamless integration existing deep framework empirical evaluation reveal tdistnns achieve superior balance coverage interval width compared traditional gaussian based pnns consistently yielding narrower prediction interval maintaining appropriate coverage level advancement represents significant contribution estimation neural network regression task particularly beneficial scenario involving complex output distribution algorithm artificialintelligence dl d datascience deeplearning ml machinelearning neuralnetworks probabilisticmodels tech technology uncertaintyquantification probabilistic neural network pnns distributed output adaptive prediction interval beyond gaussian assumption arxiv org 1 134 3000 understand neural netw",
    "cluster": 0,
    "keywords": "neural, network, abdullah, prediction, framework, quantification, bayesian, performance, algorithm, deep"
  },
  {
    "filename": "bbe5595ca9450c3d443c28c885bcd67f.txt",
    "text": "08 app abdullah k abdullah k 7 introducing adaprl adaptive pairwise regression estimation universal regression task realm deep regression significant challenge arises traditional point wise approach often overlook intricate interrelationship among point oversight lead suboptimal performance increased overfitting due aleatoric present training combat issue present adaprl groundbreaking adaptive pairwise framework innovative capture relative difference point also integrates deep probabilistic effectively quantify prediction key highlight adaprl enhanced prediction accuracy improved ranking ability increased generalization capability robustness noisy resilience reduced availability enhanced interpretability result extensive experiment across various real world regression datasets including recommendation system age prediction time series forecasting demonstrate adaprl compatibility diverse backbone network ability achieve state art performance without incurring additional inference cost moreover adaprl seamlessly integrated existing regression framework performance enhancement join u exploring future regression task algorithm artificialintelligence dl d datascience deeplearning ml machinelearning regression tech technology uncertaintyestimation adaprl adaptive pairwise regression estimation universal regression task arxiv org 1 134 3000 address overfitting machine deep breakthrough trend improve predictive accuracy la",
    "cluster": 0,
    "keywords": "neural, network, abdullah, prediction, framework, quantification, bayesian, performance, algorithm, deep"
  },
  {
    "filename": "c60aaab5c1bf9b853d485c131678c419.txt",
    "text": "08 app abdullah k abdullah k 6 evaluation time policy switching offline reinforcement realm offline reinforcement rl present unique challenge particularly leveraging fixed datasets optimal task resolution traditional policy algorithm effective online setting often face difficulty offline scenario due tendency overestimate value distribution action recent advancement novel policy switching technique introduced method dynamically integrates strength pure policy rl agent enhancing behavior behavioral cloning bc agent ensuring adherence existing innovation lie utilizing epistemic quantified rl alongside aleatoric derived dataset characteristic empirical result demonstrate policy switching technique surpasses individual algorithm also competes robustly state art method across various benchmark furthermore approach facilitates seamless transition offline online fine tuning enabling rapid adaptation without extensive hyper parameter modification potentially matching exceeding existing performance standard key takeaway dynamic policy switching enhances performance offline rl utilization epistemic aleatoric uncertainty effective transition offline datasets online environment airesearch algorithm artificialintelligence dl d datascience deeplearning ml machinelearning policyswitching reinforcementlearning tech technology evaluation time policy switching offline reinforcement arxiv org 1 133 3000 future trend reinforcement emerging innovation paradigm apply reinforcement llm devel",
    "cluster": 0,
    "keywords": "neural, network, abdullah, prediction, framework, quantification, bayesian, performance, algorithm, deep"
  },
  {
    "filename": "ed49a2207e16c9df7caf3c255a0849af.txt",
    "text": "08 app abdullah k abdullah k 5 exploring epistemic recommendation system task recommending item user cornerstone science yet traditional face significant challenge particularly handling explicit feedback sparse recent advancement highlight limitation conventional approach notably susceptibility overfitting inability incorporate epistemic prediction response novel method known bayesian deep ensemble collaborative filtering bdecf proposed innovative framework leverage bayesian neural network integrate weight parameter enhancing generalization prediction quality key feature bdecf include 1 interpretable non linear matching utilizing attention mechanism improved user item embedding interaction 2 ensemble based supermodel approach foster robustness reliability prediction extensive empirical evaluation across diverse real world datasets underscore effectiveness bdecf critical component research signifies substantial step forward developing reliable recommendation system better serve user need addressing inherent uncertainty algorithm artificialintelligence bayesianlearning dl d datascience deeplearning ml machinelearning recommendationsystems tech technology epistemic aware recommendation system via bayesian deep ensemble arxiv org 1 134 3000 future trend recommendation algorithm ensemble improves prediction include optimization understanding diagnosis prediction reliability improve predictive accuracy improves hurricane prediction career productivity finance soft skill",
    "cluster": 0,
    "keywords": "neural, network, abdullah, prediction, framework, quantification, bayesian, performance, algorithm, deep"
  },
  {
    "filename": "83c548257a7670f6685e804aaaf99846.txt",
    "text": "08 app nathan anecone nathan anecone co founder cto semantic reach 8 study machine detail discover kind mess nobody know anything work countless heuristic trick niche use case come twice never show machine much result driven pragmatic field quite yet science shame scientifically principled theoretical understanding machine could lead directed informed practice improved outcome overall long ago came across work e jaynes 1922 1998 physicist also made several significant contribution probability statistic good deal jaynes work dating far back 1950s bear striking resemblance modern machine indeed much machine emerged paper probabilistic statistical modeling previous decade jaynes participated developing one jaynes best known contribution principle maximum entropy essentially state least biased probability distribution one highest relative known turn fairly common sense axiom rule anything inconsistent present knowledge appears almost ubiquitously throughout machine play implicit explicit role technique regularization bias mitigation stochastic gradient descent weight initialization bayesian reasoning deep generalization reinforcement diffusion modeling rotational invariance convolutional neural network besides closest thing law gravity machine recent year various cutting edge maximum entropy algorithm published maximum entropy regularizat",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "94191bc94f274f9fe442a317ac969bb3.txt",
    "text": "08 app tom whitehead tom whitehead head machine intellegens 2 gaussian process often taken gold standard quantification ever wonder training went gp estimate directly depend training output appears gp mean prediction different noise level different place mean gaussian process suffer predicting similar everywhere unless explicitly tell local noise level compare simple random forest picture naturally higher noisier without told local noise level real material chemical project understanding key often know noise level advance need machinelearning tool handle real experimental come 317 47 victor guiller design experiment doe expert l al empowering r formulation lab science smart experimentation black belt lean six sigma 2 tom whitehead agree gaussian process available quantification many provide metric well many method framework enable agnostic calibrated estimation quantification mixed feeling direct comparison nature computed standard deviation quite different random forest relies bootstrap sample get assessment comparison standard deviation interval displayed involves obtained confidence interval gaussian process one obtained bootstrap interval random forest last one known lot tighter traditional confidence interval could lead",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "Advice on labeling this type of image for machine learning_.txt",
    "text": "source reddit computervision title advice labeling type image machine content hey r computervision thank people gave advice post made back worked good way find roi suggestion got next step make machine simply put decided make ml binarise image otsu found unreliable threshold type image different lightning condition since noise cause threshold mess misplacing pixel place label white pixel band stripe essentially ever band bet false ground truth large amount image suggestion making process less painful appreciated thank p consulted uni supervisor approach seem suggest sit zoom label want idea would like hear different approach guy suggest comment uneven lighting try moving average detect edge label space edge depending logic could remove noise median filter keep edge alive use kernel trigger horizontal line go hi u spampham downloaded picture posted tried something halcon installation using variable threshold var threshold pre post processing could solve issue please find code segment dark stripe segment bright part use light parameter maybe must change parameter tried bright part dev close window dev update read input image read image tolabel sample png convert color gray rgb1 gray tolabel image open window display result get image size image width height dev open window fit size 0 0 width height 1 1 windowhandle set display font windowhandle 16 mono true false dev display image stop segement background mask threshold image regionbackground 50 255 closing circle regionbackground regionbackground 3 5 dev display regionbackground stop threshold image using variable threshold var threshold image region 15 15 0 5 dark intersection region regionbackground regionintersection dev display image dev display regionintersection stop select region particular height size connection regionintersection connectedregions closing circle connectedregions connectedregions 3 5 select shape connectedregions selectedregions area 15 4000 display result dev display image dev display selectedregions disp continue message windowhandle black true stop disp end program message windowhandle black true",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "cf374bf2c7d7b0f98282d397111cb7b6.txt",
    "text": "08 app ilium ekhlakov ilium ekhlakov senior scientist growth retention advanced analytics predictive ml causal inference google cloud platform gcp year experience considering new challenge 4 train ensemble often use bootstrapping bagging people without even realizing method help improve generalization creating multiple different sample training main difference bagging sample without replacement bootstrapping sample replacement goal treat single training dataset one possible realization true population example company client lead period training might generate dozen even hundred sample build robust handle randomness cross validation however normally use 3 5 split reduce effect one particular validation split much bootstrapping bagging come final hold test often use one hold set yet logic applies hold set one possible future dataset often assume hold closely resemble see inference time reality similarity guaranteed important remember performance metric random variable fixed constant new hold split bootstrap sample lead different metric value rather relying single point estimate choose multiple hold split allows example could nested cross validation outer loop evaluation inner loop hyperparameter tuning obtain honest estimate performance bootstrappin",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "Epistemic uncertainty really could reflect the state of the model_[D].txt",
    "text": "source reddit machinelearning title epistemic really could reflect state content try use mc dropout deep ensemble get epistemic epoch assuming regression task quantify directly output std recorded epoch finally plotted curve result value rapid rise slow decline trend seems intuitive curve continue fluctuate stable curve make wonder episodic unstable training process increasing epoch case output std used basis comment fluctuation could due rate adjustment",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "Examining uncertain predictions on the training set.txt",
    "text": "source reddit datascience title examining uncertain prediction training set content let say train deep dataset certain labelled correctly full let say 80 correct label text binary classification task fine tuning pre trained training run prediction training dataset compute probability class would like check example able confidently distinguish 2 class maybe spot mistake dataset way investigating instance unsure make mistake identify potential issue dataset instance may warrant investigation determine labeling error ambiguous sample factor contributing let dive deeper problem maybe would work traditional machine problem come transformer output transformer logits fed sigmoid case else softmax multiclass get probability class mean sigmoid forcing value 2 extreme 1 0 sigmoid thinking correct work case run prediction logits instead running sigmoid function run softmax 2 class instead essentially want spot mistake training dataset saying make sense comment softmax essentially multi class version sigmoid function binary label sigmoid function sufficient get probability positive class calculate q 1 p probability negative class multi class case output softmax function normalizes value sum 1 e g 0 2 0 3 0 5 logit function map probability 0 1 real sigmoid inverse map real probability 0 1 sigmoid push value towards extreme many circumstance almost certainly end value middle think need take fundamental look reason believe 80 correct 20 incorrect perform cleaning parse label confident considered unsupervised method capture anomaly read paper back remember took 0 3 incorrect label completely break ml ah see thanks explanation yeah think entirely correct around 80 part gathered hand good part rather automated using pipeline chatgpt clustering embeddings etc cleaning hand go bos order might losing much time little performance increase also know isolation forest work tabular nothing capture anomaly nlp task though sadly maybe sort semantic clustering still get messy real solution quickly",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "Gaussian Processes for Mixed Continuous and Categorical Inputs.txt",
    "text": "source reddit datascience title gaussian process mixed continuous categorical input content dealing mixed categorical continuous input key thus gaussian process would great anyone aware python library could use mixed input gps potentially define kernel implement scratch thank comment gpytorch",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "How Does a Model Detect Objects in Images of Different Sizes_.txt",
    "text": "source reddit computervision title detect object image different size content new machine question working image recognition common challenge dealing image varying size suppose trained detects dog provide dataset containing small image dog large image bigger dog recognize correctly despite difference size comment size image really matter much size object object detector usually made 3 part backbone big cnn fpn gather output backbone different scale finally classification head tell pixel fpn output dog cat something else useful extra important info fpn gather info different scale roughly speaking fpn pixel coarse scale correspond large object original image finest scale correspond smaller object final image go paperwithcode search spp spatial pyramid pooling scale pixel wise whatever want use nearest neighbor constant hi recently published article topic maybe find useful scale invariance almost architecture mechanism learn feature invariant scale lighting position etc many way achieve mechanism architecture augmentation help",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "Looking for a Blog post that small image resolutions are enough for CV_DL.txt",
    "text": "source reddit computervision title looking blog post small image resolution enough cv dl content cross posted r machinelearning looking blog post someone pretty well known student era researcher cv dl 224x224 336x512 resolution enough computer vision neat interactive visualization could try different resolution augmentation etc argument quite convincing human solve task fairly reasonably looking image neural network sure tia bugging since looking share junior comment probably yes thanks bunch nice missed one thanks share think quite good information ultimately terribly surprising human correctly recognize specific face shockingly low number total bit information across image anything expect dedicated effort could definitely get ml system work quite well much smaller image necessarily efficiently though",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "Query - Confidence Interval.txt",
    "text": "source reddit datascience title query confidence interval content hey guy hope everyone well quick query would really helpful get guidance query working classification problem dependent variable 0 1 client asking final prediction file would following column 1 name 2 prediction 3 confidence score confidence score column client want generate output 0 100 1 0 score represents high confidence 0 output 2 100 score represents high confidence 1 output 3 50 score represents output question way get output like confidence score something similar used random forest classifier train test working 95 accuracy please let know need provide information thanks advance comment random forest multiple tree outputting multiple prediction final output predicted class could simply say confidence score amount tree predicted 1 class total amount tree completely theoretical sound way computing confidence various reason get interested ask try explain help come pessimistic approximation least really set using rf try logistic regression output probability highest probability prediction confidence simply probability use logistic regression output rf get well calibrated probability think scikit learn built class sklearn calibration calibratedclassifiercv worth read hi thanks reply lot outlier using random forest less impacted logistic regression working well open using well predict proba x well random forest showing confidence probabiliy hi u sniffykix thanks reply mean used logistic regression rf like prediction generated via rf need done via logistic regression thanks case described confidence probability multiply hundred due nature rf trained probability representable tried logistic regression even dataset outlier perform outlier univariate multivariate maybe deal outlier univariate maybe cap outlier see performs must read link essentially random forest prediction pred proba accurate bias towards 0 1 0 9 roughly speaking calibratedclassifercv resolve sorry answer actual question mean train logistic regression using rf output input target variable target calibratedclassifiercv thanks sorry got question would great help rf trainee mean please help understand difference probability generated logistic regression output v rf pred proba understand correctly case probability help determine class prediction belongs probability generated lr typo rf random forest trained ok probability random orest generates go back core work much oversimplyfing random forest produce set tree using procedure called bagging definitely google selecting set feature split tree us tree produced predict target variable tree bad design split subset random feature imagine tree split least predictive feature hence probability estimate might pessimistic lower actually proability estimate probably many tree predicted class total amount tree check logistic regression straight predicts probability design statistically relevant probability skip detail highly encorage fgiure logistic regression really work way intepret follows probabilties around 0 5 bad uncertain positive class negative class want close 0 1 thanks u the75th make clear er thanks taking time explain answer really appreciate oh yeah final easiest way explain rf give proportion agree really probability strictest sense logistic regression give well defined real probability fine using output random forest think helpful understand giving work high level",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "Question about using predictions to generate new training target data.txt",
    "text": "source reddit machinelearning title question using prediction generate new training target content hi analysing text simple yes category training set around 3k produce reasonable f1scores accuracy f1 70 appears produce probability 0 1 thinking manually tagging use another round training probability prediction around 0 45 0 55 intuition tell area allocating category close 0 1 unwise potentially causing bias aware noticeable impact improving performance comment example active well studied area idea identifying new item annotate called sampling one number heuristic danger bias experience bit black art designing active setup work well thanks john sound like exactly",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "Representing Model Uncertainty in Reinforcement Learning and Classification_ Are Bayesian ConvNets Harder to Fool_.txt",
    "text": "source reddit machinelearning title representing reinforcement classification bayesian convnets harder fool content comment",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "Ressources for Bayesian time series_.txt",
    "text": "source reddit datascience title ressources bayesian time series content working time series problem hierarchical structure like one presented m5 competition instance target variable sale volume within company sale divided across country department product type product number also strongly interested forecast let say lot thought using bayesian hierarchical approach however think empirical result shown neural network outperform simple bayesian get feedback resource could help direct work efficiently would also love insight worked project thanks comment",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "Variation of MSE loss for regression tasks.txt",
    "text": "source reddit computervision title variation mse loss regression task content back listening talk erroll wood paper think cvpr 2022 even though paper mentioned used specific loss function landmark regression e landmark regressed coordinate pred well scalar sigma loss loss true pred true pred 2 sigma 2 log sigma 2 thought pretty neat idea since automatically force balance sigma actual deviation ground truth since tried case estimate would help get great result read quite ml cv paper come across loss paper even paper cited found presentation anyone seen used experience name comment represent landmark position probability distribution use negative log likelihood nll loss function loss contains two parameter mean distribution position variance",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "Whatâ€™s the Best AI Model for Differentiating Jewelry Pieces with Subtle Differences_.txt",
    "text": "source reddit computervision title best differentiating jewelry piece subtle difference content case jewlry working machine identify fine grained difference jewelry piece specifically gold ring look similar slight variation e g different engraving stone placement subtle design change need fine grained classification differentiate similar ring broad category like ring v necklace high accuracy subtle difference goal recognize nearly identical piece work well limited may around 20 image per sku training comment different method open suggestion really depends want input got 3d scan use shape analysis method going image going difficult use case guess would use generic ring detector get mask traditional cv algo 20 image tough could potentially try dinov dinov2 well almost identical object pair relatively say query object look similar e g two different object database signaling case double match good thing raise alert checked human bad would pick one mention potential confusion funny story implementation jeweller exactly need closely understand piece differentiating ring train getting thousand image ring use pre trained like resnet inceptionv3 unfreeze layer train top many epoch get closest result broadly categorize much top gemstone v gemstone train hard negative last piece search vision first hey everyone company achieved 1 trained ml achieved 90 accuracy jewellery image 2 work category recognizes differentiates b w identical peices etc 3 live manufacturer 1 million design bank working great upload customer photo poor lighting blurry pic bad angle yet search software identified exact match top want detail check link contact happy help thanks unlikely find something box best bet probably something like chatgpt carefully designed prompting know chat gpt point location though probably basically two doffeeent piece jerwlery indicate different ml development skill could built something better necessarily simple yeah 3 day still good result exploring method right like sift know mean hey might work know stock management piece sku serial number want bring sku piece captured app us api hey interested collaborate oh goal getting difference want know specific piece like knowing specie bird instead know product sift probably best option fall apart shiny object basically try describe small visual feature feature reflection instead actually object going work well want go photo via app sku built similar system sound like need except different use case inventory management computer vision barcodes swingtags rfid actually finished testing welll suspiciously work well well infact know trust much training fast prediction ect look see method thanks sharing info reflictions shiny object take consideration guess couple sample picture unique object training one likely way recognize object third picture",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[Active Learning]Confused about retrieving uncertainty scores from an object detection model.txt",
    "text": "source reddit computervision title active confused retrieving score object detection content hello trying figure retrieve informativeness score image object detection getting score image classification relatively straight forward confused getting score object detection straightforward way seems training separate classification cropped image object object detection average score image depending number object picture seems like lot work wondering could somehow derive score within object detection without training separate classification thanks advance comment object detection likely score objectness part architecture label assignment two stage method typically objectness score proposal classification score generated processing roi second stage e g faster rcnn already suggesting single stage method also provide score detection basic single stage method predict classification score bounding box recent method combine classification iou prediction single score e g varifocalnet one issue keep mind output may exist 0 1 method carefully calibrated probabilistic value relationship precision recall predicted score varies different method",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[Discussion] Batch active learning with Deep Neural Networks for regression.txt",
    "text": "source reddit machinelearning title discussion batch active deep neural network regression content working graph based regression task could benefit active currently looking greedy strategy selecting point predicted near pareto front believe leveraging estimation could improve efficiency search process however unlike kernel based cannot set virtual label perform rank 1 update gnn afford retrain point queried able find many resource batch active regression using deep imagine literature must exist looking wrong place literature search far returned several batch active deep applicable classification task get suggestion relevant work look please comment checked active literature might outdated last month right example saw classification problem however independently task test active method kind cross problem mean method applied regression detection whatever problem work directly loss treated blindly single number",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[Discussion] Bayesian NN with Neural Ordinary Differential Equations.txt",
    "text": "source reddit machinelearning title discussion bayesian nn neural ordinary differential equation content anyone use bayesian neural net neural ode want quantify weight weight neural network try dynamic finding good result yet idea use variational inference learn posterior weight neural network defines dynamic neural ode anyone used something like experience please let know comment think two main way approach 1 bayesian weight standard neural ode think pretty straightforward technically write prior weight predictive likelihood presumably apply stochastic variational inference hamiltonian monte carlo gradient based approximate inference method without modification disagree u jiamengial think could still use plain ode solver sampled parameter ode setting still finitely many parameter uncertain remember right anyone tried yet 2 bayesian weight infinitely deep bayesian neural network case one infinitely many parameter integrate depending take limit end stochastic differential equation describing prior weight network activation function depth express approximate posterior weight using sdes although infinitely many parameter need integrate sde give approximate variational posterior finitely many parameter parameter put posterior sde better approximate true posterior stochastic variational inference outlined fit approximate posterior sde student started working idea workshop paper still early day mainly planning investigate way reduce gradient variance power arbitrarily flexible approximate posterior also paper add noise solver neural ode however like fixed probably dropout approach probably better thought regularization strategy rather method quantification relatively unexplored area think anyone know work well practically yet fun thing think though probably quite difficult done correctly thinking since dynamic system modelled stochastic neural net initial condition time 0 immediately get marginal distribution dynamic e gradient move next time step assume discretised time every one infinitely many gradient would need followed would induce infinitely many new gradient new time step essentially marginalise possible path taken get answer specific time 1 ode solver might useful case since dynamic given trajectory fixed approximate maybe every time step could approximate marginal distribution gradient simpler distribution would probably something like von mises fisher distribution work hyperspheres though would assume magnitude gradient always 1 something build neural net though point whole thing begin resemble sort markov process essentially markov chain continuous time continuous space trainable parameter guess like continuous time maybe make discrete mcmc algorithm trainable parameter train parameter maximise probability target label time 1 sure train though bayesian neural ordinary differential equation paper address topic one point view full set tutorial diffeqflux jl turing jl documentation accompanies bayesian neural ode nut bayesian neural ode stochastic langevin gradient descent general usage differential equation solver ode sdes ddes turing probabilistic programming language focus discovery scientific machine aspect cool thing discovery portion gave u way verify structural equation receiving robust noise exact parameter could change universal differential equation way symbolic regression embedded neural network give nice way get probabilistic statement percentage neural network would give certain structure could show certain case least get symbolic output even variation posterior working sandia testing larger scale covid 19 u full validation estimate since cannot share give u way share method code associated people looking uq equation discovery pick run throw",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[Discussion] Bayesian NN with Neural Ordinary Differential Equations.txt",
    "text": "mnist portion good measure result still early everything usable today pick code play think hyperparameters probably still optimized interested topic might want check lafi 2021 conference join julialang diffeq bridged turing chat channel www julialang org slack also paper add noise solver neural ode however like fixed probably dropout approach probably better thought regularization strategy rather method quantification relatively unexplored area think anyone know work well practically yet thank much extensive reply first idea described also initial idea place gaussian prior weight neural network defines dynamic help elbo unbiased monte carlo gradient estimate variational inference postorior weight sadly approach modelling dynamical system reach convergence algorithm like case using maximum likelihood without quantifcation also want try second idea saw video latent sdes discovered github library nice library comment also found workshop paper continous depth bayesian neural network sound theoretically promising want achieve need learn sdes idea approximating posterior weight via stochastic differential equation sound promising plan able dynamical system field physic also give estimate sure prediction hope approach help thank mentioned paper thank interesting reply whole idea behind bayesian neural network able quantify behind prediction idea order quantify neural ode happy hear see posted github link jupyter notebook github render large jupyter notebook case nbviewer link notebook want run code binder link start jupyter server try bot feedback github main goal would efficiently sample distribution possible trajectory method exists maybe could help space dynamical system discretised finite might make certain sampling method applicable mean cared initial condition take specific initial position run bayesian stochastic neural ode infinitesimal time step 0 1 many many time look distribution output get however one initial condition gonna correlate hugely get another initial condition however care initial condition happens time 1 space discrete finite ordered got discrete time state could coalesce time neural ode change w r time position lot ifs maybe could obtain sample therefore calculate bayesian neural ode process using exact sampling thank reply thought idea using discrete finite time space using resnets dynamic system definetly think",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[Discussion] Bayesian regression with noise injection_.txt",
    "text": "source reddit machinelearning title discussion bayesian regression noise injection content bit experience bayesian regression using pymc3 also heard people using noise injection better regularizer dropout e g add small amount gaussian noise output layer neural network tried simple linear regression pytorch trying setting bayesian linear pymc3 e g pymc3 would normal mu sigma mu beta x alpha sigma halfnormal 0 1 beta normal 2 1 alpha normal 0 1 1 def x beta torch ab b sd torch randn x shape b mu alpha torch ab sd torch randn x shape mu mu beta x alpha sigma torch ab sig torch ab torch randn x shape sig mean torch ab sigma torch randn x shape mu return b mu variable torch tensor 2 0 requires grad true b sd variable torch tensor 1 0 requires grad true mu variable torch tensor 0 1 requires grad true sd variable torch tensor 1 0 requires grad true sig variable torch tensor 1 0 requires grad true sig mean variable torch tensor 1 0 requires grad true loss fn torch nn mseloss size average true lr 0 0001 optimizer torch optim sgd params b mu b sd mu sd sig sig mean lr lr train like ordinary linear regression using sgd mini batch synthetic noisy linear training standard deviation variable actually reflect amount variance training particular b sd variable actually end close pymc3 give using exact bayesian inference mcmc question exactly method noise injection technique w sgd seems way something like variational inference conjugate prior prior posterior variable gaussians still mechanic variational inference sure case practical standpoint even giving exact variance standard deviation value seem quantify useful probably regularizing well comment first question sample beta normal non zero mean noise injection stochastic vi noise injected weight follows variational inference principle stochastic gradient langevin dynamic noise injected gradient update follows hamiltonian principle",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[DISCUSSION] How do the different versions of the bootstrap work with deep neural networks_.txt",
    "text": "source reddit machinelearning title discussion different version bootstrap work deep neural network content looking deep method possible way imputing missing lead question deep neural network interact various version bootstrap statistical missing context goal estimate posterior predictive distribution missing conditioned observed fill missing draw distribution leading version dataset observed missing vary analysis time use simple formula combine analysis give nice clean picture much missing reduces confidence whatever hypothesis testing ok deep net part statistic literature missing idea properness say want posterior distribution missing reflect source used fill missing filling missing either need fully bayesian approximate fully bayesian possible simplest way approximate proper posterior missing world often use either parameteric nonparametric bootstrap parametric bootstrap 1 train say mu sigma linear regression 2 sample predicted value outcome variable ie make new outcome variable sampling learned mu sigma 3 retrain using sampled value new dependent variable 4 make whatever prediction inference want second bunch time aproximate bayesian posterior nonparametric bootstrap resample replacement train resampled repeated many time get approximate posterior question deep neural network 1 advantage disadvatages deep net using variational bayes versus deep net using either version bootstrap known expected bias either form bootstrapping deep neural network 2 cause bias train network scratch multiple iteration bootstrap would problem example resample carry weight previous bootstrap iteration 3 good resource changing fully bayesian deep network change sort choice make dropout batch normalization activation etc comment sorry say know answer generally curious see people respond advantage disadvatages deep net using variational bayes nearly variational work seen neural network effectively treat parameter independent linear would mean covariance posterior constrained diagonal sort tricky generally necessary many parameter e g linear layer transforming embeddings n n matrix getting covariance mean 2 n 2 500 x 500 embedding transform 500 x 500 x 500 x 500 62b parameter still diagonalizing inter layer connection wanting explore low rank plus diagonal variational need k n parameter tunable k since reparameterization trick work well never made time year since really followed bnn progress take grain salt",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[Discussion] Modeling uncertainty using deep learning models.txt",
    "text": "source reddit machinelearning title discussion modeling using deep content existing literature deep work deal capturing like bayesian like example result predictive distribution low variance test point similar training high variance test point differs greatly training think far input space comment dropout bayesian approximation representing deep google search post title verbatim return paper first result quite way literature explores approximate expectation propagation variational inference sampling based method laplacian approximation bagging like approach get reading two paper 1 2 bagging approach explored 3 main contribution 1 2 3 david mackay new work bayesian method neural network check edward library also bayesflow variational inference well always make bayesian neural net would gold standard way really cross valiadtion still work also weight neural network simple bootstrap approach get estimate paper show dropout method produce poor result basic underlying function another simple somewhat ad hoc approach proposed simply lop output layer training whole network replace bayesian regression general agree interesting problem still remains fairly open particular lack frequentist approach literature computationally efficient dropout inherently going underestimate since unit zero ed forward process actual range hidden activation take upstream bounded really mean ton work yes despite popularity probably since simple think dropout proper method get predictive variational argument appears pretty tenuous connection family approximating distribution seriously limited think hinton averaging perspective much reasonable view dropout numerous work cf empirically shown dropout approach lead degenerate behavior especially worrisome initial goal estimate equivalent variational inference approach yes correct wrong seen serious effort anything like confidence interval deep neural net prediction research idea thrown nothing major point right direction im mistaken confidence interval special case predictive distribution bayesian approach result bnn complicated predictive distribution confidence interval sufficient statistic also idea 25 year old bnns received less attention alex graf nip 2011 paper gaussian process attractive alternative interest resurged bnns scale large easily also absense consensus hardly argument applicability competing method area active research bnns common benchmark many approximate inference method could point significant paper bnns thanks think one started wray l buntine andreas weigend bayesian back propagation complex system 5 6 603 643 1991 cited page 28 one important know connection variational inference mdl clear point hinton geoffrey e drew van camp keeping neural network simple minimizing description length weight proceeding sixth annual conference computational theory acm 1993 mackay also quite thing especially laplace thingy mackay david jc practical bayesian framework backpropagation network neural computation 4 3 1992 448 472 gotterdammerung bnn return probably one graf alex practical variational inference neural network advance neural information processing system 2011 suddenly kingma diederik p tim salimans max welling variational dropout local reparameterization trick arxiv preprint arxiv 1506 02557 2015 hern ndez lobato jos miguel ryan p adam probabilistic backpropagation scalable bayesian neural network arxiv preprint arxiv 1502 05336 2015 gal yarin zoubin ghahramani dropout bayesian approximation representing deep arxiv preprint arxiv 1506 02142 2015 blundell charles et al weight neural network arxiv preprint arxiv 1505 05424 2015 thank recently structured efficient variational deep matrix gaussian posterior think integrating parameter latent variable",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[Discussion] Understanding Tensorflow Probability .txt",
    "text": "source reddit machinelearning title discussion understanding tensorflow probability content trying find good example online predictive maintenance training pipeline built making use estimation using tool tensorflow probability let say dataset sensor reading time failure fair understanding training regression predict time failure mean know certain ttf output anyone good reference example using tensorflow probability regression estimation looking learn appreciate help comment output probability distribution use log probability log probability density continuous observed value define loss function flip sign going minimize optimization known maximum likelihood estimation since objective let say standard deviation distribution along distribution make sense 2 output layer defined concatenated achieve would nll loss handle understand standard deviation distribution along distribution",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[Discussion] Using uncertainty in raw labels in loss function of model.txt",
    "text": "source reddit machinelearning title discussion using raw label loss function content generated dataset multiple numeric score prevalence phenomenon range 0 1 image dataset 100k image image labeled least twice separate user exploration found user consistently rate image 20 30 higher user also little bit user consistency testing mixing set image rest work image 35 time domain expert course 3 month found user rating always within 15 average score ie ground truth 0 3 resulting label always 0 15 0 45 underlying stats label change person person time way incorporate knowledge loss function one idea set target image sampling n score mean score std sampling every time image used gut feeling help generalize label fixed always within domain expert labeled image another idea per sample weight loss image weight function standard deviation score assigning weight score label tend agree know starting get realm bayesian probabilistic neural net wondering anyone taken kind half step comment something like recent yet unpublished paper used fuzzy set express class label loss applied called optimistic superset loss find paper h llermeier et al want chat depth would like share thought finally got around reading h llermeier paper think concept optimistic superset loss exactly thinking case would parameterizing osl observed deviation label getting point current project might implementing see go thanks glad help",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks_.txt",
    "text": "source reddit machinelearning title discussion sota quantification method neural network content recently started look topic curious method sota used production specific interested modeling aleatoric epistemic neural network ideal setting tell encounter input distribution express given input respect system noise edit mainly working regression problem thanks advance comment easiest method multiple sampling inference drop enabled look recent 2024 paper benchmark various bayesian neural network method specifically quantification regression task paper preprint liked look conformal prediction variety method umbrella including density based memory usage problem deep ensemble hard beat otherwise simply regularize nn spectral normalization train density estimator latent space e g gmm likelihood embedding serf estimate see example would say deep ensemble likely especially factor implementation complexity driven bayesian neural network community bit mad would also recommend possible fancier alternative removed iclr oral might interesting think feature based methos popular people try make modification training like including stuff like spectral normalisation stuff end using feature based method take feature based method myt get good estimate like virtual logit matching gaussian modelling etc experimenting technique physic code remindme 2 hour yes stumbled upon concern quality estimate monte carlo dropout know concern relevance practice thank look plan share corresponding git repo publication gonna post one conformal prediction nice property assumes training test exchangeable true situation distribution shift e new distribution training sound like one thing op would like detect number paper tried develop method overcome limitation knowledge able certain case given certain assumption example know distribution changed shift distribution meet certain criterion sure well kind assumption fare real world probably depends ensemble perform bad ood prediction right weired see ensemble predict bad ood region understand author emphasize see figure 1 also shown clearly sngp spectral normalization specifically might guess method work agnostic orthogonal neural network architecture choose specific feature based method mind work well messaging 2 hour 2024 05 15 13 54 46 utc remind link click link send pm also reminded reduce spam parent commenter delete message hide others info reminder",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[Discussion] [Research] Variational Bayesian Inference vs Monte-Carlo Dropout for Uncertainty Quantification in DL.txt",
    "text": "source reddit machinelearning title discussion research variational bayesian inference v monte carlo dropout quantification dl content reaching ml community see experience implementing exposure quantification method deep cnns specifically working towards implementing variational bayesian inference medical imaging task interested knowing point variational method become coarse mc dropout superior method implement understand theory correctly enough sample posterior distribution deep network converge true posterior mc dropout thus providing exact metric hand variational method always approximation produce result efficiently md dropout probably suitable situation anyone thought would love hear great reference really anything felix laumann dropout bayesian approximation representing deep gal man comprehensive guide bayesian convolutional neural network variational inference bayesian convolutional neural network variational inference comment authoritative voice topic experience neither actually well suited practical application regarding mc dropout really give posterior assumes posterior look like bunch delta function basically likelihood close actual posterior also concentrate bad sign converse implication concerning widen little mobile excuse bad link formatting relevant pretty good point accompanied pretty poor behaviour imo regarding variational method really sure panacea either bayes backdrop etc normally make heavy independence assumption make thing tractable riffing source back work like lottery ticket hypothesis seems suggest correlation potentially even crucial performance independence assumtion would therefore absolutely awful term accurate posterior estimation lot experience building bayesian traditionally gelman mceldrith school show hard thing even moderately high posterior quite unintuitive thing one million parameter certainly multimodal would beast make good inference also computationally edit add also wary proof infinite limit etc may provide motivation need working method stupid example look convergence taylor series exp v sin converge infinite limit different property truncated personally derived mcmc sampling scheme theoretical infinite convergence target distribution terribly practice problem computational issue make effective implementation nearly impossible estimate deep still really open problem first let clear several point enough sample posterior distribution deep network converge true posterior mc dropout true could say enough sample would cover estimated approximate posterior certainly true posterior mc dropout make strong assumption distributional form approximation objective minimises questionably bayesian variational method always approximation produce result efficiently md dropout mc dropout best variational method weakness really two different method variational method approximating gaussian distribution argue suitable capture posterior distribution bernoulli style posterior mc dropout currently push back well suited mean field variational method including mc dropout neural network float alternative method mention ensemble often outperform variational method arguably simpler scalable e g finally work explores connection ensemble bayesian posterior maybe also add ensemble based method mcmc comparison list thanks thought coming type content beginner 1st year electrical engineering master engineering physic undergrad think easy get caught theory lack implementation experience fortunate enough talk ml scientist deepmind working similar content even said implementing bayesian network becomes hassle also know gal osband back forth interesting hear ian say dropout appreciate link thought thanks ton thought really insightful stumbled across paper talking issue predicting posterior issue coupled difficult optimization implementation might make prohibitive initially expected lot churn think ensemble method might interesting path look thanks",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D]  Uncertinity Quantificationfor time seriese prediction (RNN)_.txt",
    "text": "source reddit machinelearning title uncertinity quantificationfor time seriese prediction rnn content time series predicts one two class step 0 1 using rnn sequence sequence new topic quantification uq directly apply common method deep ensemble mc dropout simply expect everything work caveat checked two library torch uncertinity uq box nothing mentioned time series comment hello developper torch see reason deep ensemble mc dropout could applied problem anything related time series yet library feel free create issue need help since actually classification probability use may calibrated treat prediction instance instance case could calibration conformal inference could also try using mapie",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Any good works on uncertainty estimation in imaging tasks (e.g. segmentation)_.txt",
    "text": "source reddit machinelearning title good work estimation imaging task e g segmentation content quite bored hundred annual paper providing slightly different architecture segment specific dataset estimation segmenting medical image seems like hugely useful imagine great use ml asking segment image also outputting map place unsure radiologist manually correct however seen many novel paper area simply use monte carlo dropout bayesian nn ensemble network measure basically boil multiple passthroughs image looking variance prediction higher variance higher anyone know cool interesting paper tackle topic different way work related area comment paper wayr two week ago vision task instance segmentation specifically though still good read uncertainty need bayesian deep computer vision monteiro et al stochastic segmentation network modelling spatially correlated aleatoric neurips 2020 look specifically case segmentation get structured modelling corellations pixel explicitly may biased one author think interesting addition literature look different aspect structure rather get better aleatoric epistemic estimate dropout ensemble etc previous work pretty curious well interesting paper seen far specifically mention radiology could check staple derivative work see original recent stuff kind thing looking least expect anyone claiming improve methodology cite original since pretty well known original others large area study general google return many paper area would day though disparage contribution slightly different architecture even small bump performance could save thousand life scaled population critical application healthcare also add incremental progression get larger aggregate improvement quality people life thanks posting long fan paper kendall gal deep network see add something qualitatively different previous approach readable",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Best practices for Bayesian Deep Learning.txt",
    "text": "source reddit machinelearning title best practice bayesian deep content hello r machinelearning currently working image recognition project implementing cnn followed dense interpretation layer looking replace dense layer bayesian neural net better account epistemic noise wondering suggestion best practice bayesian deep normally take epoch converge parameter adjust size layer would appreciate help could give comment plan use mean field approximation dense layer dropout mean field make sure use local reparameterization trick since help speed convergence lot initialize variance small initialize large initially basically never train much noise recommended parameterizing log variance rather variance constrain positive mean field bnns take longer converge general using optimizer worry early stopping test loss go go go bit patient adjust size layer check uncertainty need bayesian deep computer vision kendall gal training time affected testing time likely think ought treat epistemic prior epistemically grounded really mean much classic bayes rule",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Capturing forecast uncertainty for a SVR model.txt",
    "text": "source reddit machinelearning title capturing forecast svr content anybody suggestion quantify forecast svr parametric basic ols forecast interval accounting parameter observational fairly straightforward similarly neural network quantified adding dropout layer modelling distribution prediction however entirely sure best using svr idea greatly welcome comment bootstrap estimate epistemic part add noise estimated rss",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Comparison of the Model prediction uncertainty of two different models.txt",
    "text": "source reddit machinelearning title comparison prediction two different content career scientist ever faced situation compare quality predictive estimation machine old statistical already use bnn trained experimental statistical developed department depends parameter estimated classic mcmc method seems agree well experimental wanted compare quality predictive thought comparing level calibration sure test dataset due bnn entire dataset due fact old statistical use mcmc method entire dataset comment assume bnn mean bayesian neural network designed specifically estimate parametrization weight distribution inference several time get slightly different result run order forward pas network sample weight learned distribution measuring std prediction something statistically clever estimate way estimation like monte carlo dropout method work arbitrary network personally really trust check conformal prediction quantification deep neural network hi something similar figured log likelihood know bnn work well rmse test set almost equal one get employ old rmse ameasure accuracy alone wanted check one two give better estimate ah quantified neural network know compare uq neural network old statistical use think bayesian neural network assume point independent likelihood product normal distribution old mathematical developed department make assumption us full covariance matrix close diagonal matrix anyway old multivariate normal statistical v univariate normal bnn compare distribution residual method get point see point looking average marginal likelihood need find way modelling joint covariance bnn paper kwon break aleatoric epistemic component may may relevant full matrix approach mean aleatoric epistemic contains full covariance matrix per sample single point maybe would help could try taking determinant covariance matrix compare mcmc result tell calibrated prediction 80 must right 80 case average according weak notion average calibration average marginal likelihood find uhm split aleatoric epistemic regression problem update opening post idea compare level calibration since mcmc require splitting dataset find position bnn require split dataset training set calibration set test dataset old mcmc instead use entire dataset want compare level calibration use test set entire dataset compute likelihood datapoint average result measure likelihood point without considering dependency",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Data augmentation with ambiguous examples_.txt",
    "text": "source reddit machinelearning title augmentation ambiguous example content feel like missing keyword maybe guy help figure search recently thinking occurs give typical classifier completely random sample effectively distribution still return label instance train classify mnist feed vector gaussian noise classify completely random vector digit anyone attempted augment training datasets random noise random combination training example strategy intuition injecting essentially negative counter example want label vector high amount entropy think counter example would high degree aleatoric comment believe work depending noise image might indeed little closer digit others assign equal probability digit noisy image would ironically introduce lot noise maybe wrong think would help positive example example indeed digit image mean random erase cutmix label smoothing label smoothing could thing also thought specifically training range image enforce equal probability class never tried real application yes noise associated label introduce would need depend careful consideration distribution perhaps appropriate technique example would convex combination image different class label vector class weight combination would least give intution close ambiguous real image though reminded fact fourier spectrum gaussian noise uniform whereas spectrum true sample almost certainly follows meaningful distribution final thought would add label uncertain output rather forcing choose high entropy prediction true label dimension ambiguous would assigned label thank googled cutmix mixup similar concept suggesting adding new label negative sample seems like great thing try",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Difference between probabilistic and Bayesian approaches for modeling uncertainty_.txt",
    "text": "source reddit machinelearning title difference probabilistic bayesian approach modeling content notice approach hpu u net use generated sample imply bayesian bayesian segnet directly modeling epistemic aleatoric slightly confused 2 approach essentially modeling differentiate 2 far understand probabilistic generative approach cannot directly represent basically implies based generated sample correct interpretation thanks comment let take step back two class first latent variable lvm includes vaes kalman filter gaussian mixture etc bayesian latent state deterministic parameter generative joint distribution second class would call bayesian e g gaussian process bayesian neural network typical conditional distribution regression classification bayesian parameter epistemic aleatoric probabilistic graphical first p x second p x stop though bayesian generative like beal variational kalman smoother gaussian process latent variable performing inference much harder less common thank comment first category refer latent variable encoder decoder architecture due latent variable representing prior probability distribution also quick maybe dumb clarifying question notice said latent state latent variable bayesian also use categorize second group part make bayesian fact represent probability distribution opposed scalar value instance know paper seems say bayesian finding distribution weight conditioned lastly thanks last paragraph essentially saying output 2 compared take variance v multiple forward pass latent variable thus getting multiple possible interpretation output would equivalent aleatoric epistemic right definitely epistemic since distribution weight aleatoric since variance v would represent ambiguity input label pair opposed input sorry multiple question kalman filter p x estimating posterior distribution latent variable class probabilistic graphical random variable observe directly view autoencoders lvms pca vaes etc many others random variable prior posterior distribution bayesian subjective term use using probability describe example pca pc deterministic quantity probabilistic pca view linear gaussian lvm gaussian distribution pc similarly regression classification parameter also bayesian however using bayes rule probabilistic automatically make bayesian sure understand last question basically lvm p x p x p x x unobserved regression p x x observed bayesian lvms see variational em etc hard always useful since know x start question talking tasking vaes network fixed parameter epistemic sampling x give sample estimate p statistical aleatoric kalman filter perhaps imprecise referring inference linear gaussian dynamical system kf forward pas latent variable smoothing backward pas posterior see detail thank much sorry couple question regarding last paragraph mind basically lvm p x p x p x x unobserved x representing input weight kind confused x unobserved since aware state input weight x latent variable latent variable lvm would recommend reading one ml text bishop murphy barber etc learn",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Does prediction uncertainty of prediction depend on the choice of model_ (for example, a Random Forest, Gaussian Pro.txt",
    "text": "source reddit machinelearning title prediction prediction depend choice example random forest gaussian process neural network content seems interest rise know prediction example find prediction random forest paper find uncertainty neural network used computer vision general gaussian process famous getting case non parametric regression question uncertainty property method depend choice follows thought experiment let say set fixed set fit bayesian random forest b neural network c gaussian process get new input x wonder three would give prediction point x comment author say 2 type uncertainty assume interested epistemic definitely depend choice represents much particular certain prediction depends choice well hyperparameters another way approach objective estimate create ensemble consists multiple different hyperparameters extract variance prediction idea behind bayesian nn approach presented sample multiple instance interpret variance output hope helped think worthwhile go source come parameterized w given dataset typically interested identifying single best parameter w space w explains bayesian point view unless enough find exact w settle single parameter configuration put distribution space w maintain degree plausability possible parameter w need make prediction merely average prediction possible w weight according belief enough find w entire mass belief essentially w everything else would 0 one way characterize measure agreement w observed dataset likely w think single prediction certain prediction agree uncertain break aleatoric epistemic think asking difference b c assumption behind underlying instance gaussian process simplest form basically linear regression instead finding single w maintain distribution w make assumption magnitude noise observation maximum rate change prediction neighbourhood observation far know work well interpolation cannot extrapolate well bayesian neural network similar gaussian process maintains distribution parameter come rate agreement possible parameter configuration fully understand bias neural network clear linear regression type intuition happening bayesian random forest story end day product assumption would unlikely b c give class function make different assumption problem definitely would want investigate well calibrated estimate eg prediction basically depend extrapolation property extrapolate well get realistic measure prediction poor extrapolating predict wrongly high confidence random forest poor extrapolation guess gaussian process give better estimate 3 try predicting function value point far training comparing prediction course different also differ within class based choice make gaussian process large class prediction quantity vary drastically based choice mean covariance function could said random forest neural network class many choice made e g many layer neural network huge impact resulting inference prediction say sample multiple instance difference instance trained otherwise trained give output use dropout inference dropping neuron evaluating different network multiple forward pass exactly",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Estimating uncertainty using dropout at inference only.txt",
    "text": "source reddit machinelearning title estimating using dropout inference content different approach estimating active using convolutional neural network come across different paper using monte carlo dropout got wondering one necessarily use dropout training test time estimating well using dropout inference estimate would suffice lot paper read show former latter reasonable way approach estimation comment think two thing fundamentally different estimation asks given info know variance prediction monte carlo dropout inference would amount asking bottleneck info pas variance prediction time interpretation dropout trained network ensemble many network slightly different architecture perspective variance prediction reflection variety estimate multiple different trained yes could think proxy said think going underestimate since multiple trained shared weight likely similar prediction trained independently architecture effectively represented quite similar many thing ml though best way resolve try empirically independently discussion quantification using dropout test time network trained using dropout result poor performance uncommon using vanilla dropout see better test error train error training dropout testing without training without dropout testing going produce nonsense often intuition within uq context well though one explanation dropout training ensemble enormous amount less wide neural network constraint network must share parameter regularization standpoint idea encourages robust network since sub network must also perform well even missing feature explicitly modeling parameter hard mc dropout sample distribution parameter defined distribution dropout mask aka ensemble implicitly trained train time dropout produce mc estimate predictive marginalizing parameter train dropout network optimized sub network performance adding inference time dropout training time dropout essentially change architecture without dropout treated different loss function minimized different manner sidenote think many people forgetting estimation context specific class family estimation change keep change make something like bayesian estimation inherently subjective",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Have any Bayesian deep learning methods achieved SOTA performance in...anything_.txt",
    "text": "source reddit machinelearning title bayesian deep method achieved sota performance anything content link paper result curious even metric like accuracy bdl method actually achieved better result calibration quantification v say deep ensemble comment understanding field bdl currently still much stymied challenge training actually fitting posterior even relatively shallow less complex becomes expensive quickly implementation end relying method like variational inference introduce accuracy cost eg via oversimplification form posterior currently really good implementation bdl seeing bayesian rather bayesifying non bayesian like applying monte carlo dropout non bayesian transformer propagating gaussian process final weight bdl ever get anywhere come form vi lower accuracy tradeoff kind trick make mcmc based method work faster aware bayesian deep method sota anything since radford neal variable importance competition like early 2000 using combination shallow neural network fit hmc dirichlet diffusion tree another pretty cool idea scale abandoned long time ago since think issue bayesian approach always going behind pareto frontier given point time computationally intensive unreliable better way spend flop trying force work say bayesian thinking useful lot bayesians working bleeding edge deep apply directly training neural network generative learned variational inference essentially kind posterior lot sota algorithm thought instance bayes rule example link diffusion variational inference 1 diffusion thought infinitely deep vae making connection exact lead better performance 2 another example connection rule bayesian natural gradient descent 3 also nuanced point marginalization key property bayesian dl important neural network underspecified almost time specifying becomes important marginalizing possible hypothesis explain lead better performance compared account possible hypothesis better articulated andrew gordon wilson 4 1 variational perspective diffusion based generative score matching huang et al 2021 2 variational diffusion kingma et al 2023 3 bayesian rule khan et al 2021 4 counting energy based bayesian deep yes use uniform prior map estimation work pretty well deep neural net lot prior wrong useful anything sure recent year sure work decently come estimation tbh search top conference like nip aaai cvpr etc word bayesian show quite bayesian deep paper likely breaking sota benchmark since paper published top conference edit yeah agree comment vi basically subset bayesian method sota method deal vi e g vaes also relation bayesian dl sota use type mcmc tencent paper using ad click prediction posterior simulation estimation let sophisticated explore exploit trade offs make lot sense ad rec sys online system around estimation distribution shift let see come iwai guess also semantic discussion around actually bayesian simply ensembling bunch nns really bayesian fitting laplace approximation weight learned via standard method also dubiously bayesian imo kind trick make mcmc based method work faster intuition somebody dabbled trying get thing perform better past path forward assuming exists one probably mcmc entirely separate approach fundamentally outperforms mcmc cute trick ultimately feel like hopefully local minimum path less already reached sure improvement still possible going breakthrough many order magnitude type would necessary could entirely wrong course hunch worth much lot bayesians working bleeding edge deep apply directly training neural network would mind linking one whose research like bayesian slowly",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Have any Bayesian deep learning methods achieved SOTA performance in...anything_.txt",
    "text": "looking toward machine trying figure work bayesian despite name hmmm never used energy based maybe akin post bayesian method likelihood necessarily well defined probability distribution although mentioned never used energy based guess say estimation always confused unconvinced specify prior parameter bayesian deep meaningful obtain meaningful estimate simply ensembling bunch nns really bayesian bayesian neural network posterior really like izmailov et al 2021 comparing deep ensemble hmc finding bad recommend paper think bayesian least heading good direction semantics bayesian follows probability theory bayes theorem else easy learn",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] How do you measure confidence_uncertainty in tree based ensemble models _.txt",
    "text": "source reddit machinelearning title measure confidence tree based ensemble content tree based ensemble mean mainly random forest gradient boosting know random forest classification idea averaging fraction majority class different leaf different tree idea distribution confidence score could also give interesting information question 1 regression random forest regression forest kind 2 classification regression gradient boosting introduce weight different tree classifier thank comment random forest regression similar averaging fraction majority class take value leaf record sample variance point boosted tree could try quantile regression sklearn gradientboostingregressor loss quantile along least squared predicts mean train two additional predict 5 95 1 yes exactly metric think oob bootstrap estimate naturally arises forest",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] How does everybody statistically compare different NN training methods over multiple test datasets with different nu.txt",
    "text": "source reddit machinelearning title everybody statistically compare different nn training method multiple test datasets different number test sample content say method method b example method could training neural network dropout method b training neural net without dropout train method say three time 6 trained different test datasets different number sample care accuracy imagine training testing test set notice big difference one test set know whether difference statistically significant e would probability obtain higher difference one datasets chance multiple source due sampling due sampling also bias multiple hypothesis testing combine come single probability value due sampling refers variability accuracy bootstrap sample given dataset decreased increasing number test sample datasets due sampling refers variability test accuracy different would obtain training way multiple time due initial weight initialization etc decreased training comment short answer one solution might unpaired test probably sophisticated measure though also random weight initialization required break symmetry various way sometimes major impact performance",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] How to incorporate data uncertainty in Neural Network model_.txt",
    "text": "source reddit machinelearning title incorporate neural network content seeking advice best way incorporate neural network given particular application available priori knowledge application regression task domain 3d modelling estimate scalar value f 3d point x given set training point associated scalar value f e g spatial interpolation priori knowledge position training point point x x z point x coordinate x x similarily z coordinate moreover point larger z coordinate larger amount see three option 1 adding noise using normal distribution coordinate training point based coordinate epoch sample distribution 2 using bayesian deep technique hope learned posterior distribution weight bias nn parameter somehow capture inherent uncertainty 3 combination 1 2 wondering anyone experience incorporating could provide advice feedback option noted help much appreciated comment number 1 sound like great way perform augmentation task add alternative sampling distribution could also simply provide mean variance sampling distribution input directly network way 3 vector would become 6 vector depending much one might perform better another mobile providing term look bayesian neural network bnnpriors repo affiliated found helpful edit havent seen already mention yeah go option one first thing came mind know framework using use custom generator kera would trivial add noise set varies epoch epoch one drawback see would need lot epoch accurately represent distribution may past early stopping point way around would drop rate significantly run really long time train augmented sampling distribution use dropout training estimate prediction making prediction un augmented dropout still used number 1 sound promising alternative bayesian nns monte carlo dropout train nn inference multiple time dropout different unit get dropped time get distribution f find easier develop train bayesian nns alternative option mention gaussian process regression easy try since implementation sklearn mixture density network type generative easily condition e g conditional normalizing flow pyro make easy thank suggestion trying see work practice cool library example see bayesian neural network far prior always applied weight bias network parameter know prior applied input e g feature process distribution network parameter effect capturing inherent uncertainty using pytorch framework one drawback see would need lot epoch accurately represent distribution may past early stopping point way around would drop rate significantly run really long time yes thought well thanks suggestion dropping rate test see practice work yep working pyro tutorial get handle type approach although assumption implicit maybe true extent cannot assume algorithm really learns think map loss function want train argmin p theta p theta p theta p lh implies x already observed frequentist pov thus uncertainty x using bayesian interpretation along bayes rule rh treat thus input uncertain although denominator often omitted training since assumed flat non flat prior e g due domain knowledge include denominator practically mean weight sample differently according likelihood training thus similar using sample weight imbalanced classification problem note however specific bnn applied well formulate map example gp bayesian regression edit practical approach would augment prior distribution requires map may apropriate higher capacity dnns worry let know go interested hear well work",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Intersection between active learning and model calibration_.txt",
    "text": "source reddit machinelearning title intersection active calibration content hi currently working calibration method active hard time nailing solid problem statement research couple question 1 important research question intersection active calibration 2 paper established trained using pool based active strategy like sampling result well calibrated able find literature 3 many time calibration done post hoc requiring label budget devoted towards hold calibration dataset would worthwhile devote research effort towards active sampling scheme result calibrated without needing allocate label budget additional calibration set far mainly working pool based active familiar background literature calibration modern nns revisiting calibration nns beta calibration appreciate relevant paper suggest comment hi actually recently started asking question research yous first post found able find good source delve question seconding u pk thundr interested discussing question",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Inverse machine learning_ What if we knew the actual model, how to infer the parameters_.txt",
    "text": "source reddit machinelearning title inverse machine knew actual infer parameter content say domain expert come u ask infer parameter actually know need bound parameter would explanation many problem get choose classifying cat dog get choose neural net svms finding cluster get choose k mean pca domain expert actual need find parameter example let say known x exp x b x sin c x sample point x uniform 0 1 observation x normal 0 variance image example code generating image tl dr find confidence bound b c comment bayesian compute posterior let lump b c variance single vector theta already given u likelihood single datum p x theta n x variance likelihood entire dataset product p x theta prod 1 n p x theta talking real physical system expert prior p theta compute posterior p theta x p x theta p theta p x p x p x theta p theta theta almost never available closed form often sampled using mcmc collect enough sample simply histogramming sample give u approximation distribution knowledge theta given x prior p theta contained posterior probability density look small area compute probability true theta lie within area asked point estimate confidence bound element theta tell u nothing covariance element also general may possible explain either high low value say parameter b moderate value case posterior would multi modal cannot show point estimate plus confidence interval posterior multivariate gaussian diagonal covariance matrix fully described mean variance giving point estimate plus confidence interval interested parameter happy posterior final answer given new point x want predict compute posterior predictive distribution p x theta p theta x theta integrated likelihood new p x theta posterior p theta x give u distribution given new x use single fixed theta would imply 100 confidence value theta take account theta posterior sharp spike single value p x x would approximately p x theta note much ml assumed high certainty parameter estimate hope made sense stuff took age wrap head around see community pretty nasty people ignore obviously working hard understand stuff anybody ask non linear regression non linear function parameter problem method solve problem non linearity make difficult one thing could use descent method find parameter term differentiable would probably converge local optimum believe easiest way proceed would use non linear regression library available python matlab r would need observation x pair right could mle mcmc sampling deleted field statistic devoted finding parameter mathematical describing physical phenomenon measurement phenomenon infer probable parameter think problem exactly fit may even simple case theory called calibration technique bayesian see work kennedy hagan example p anyway others right could simply use mle optimize using global optimization algorithm gradient descent tell serious question problem infer parameter quantify field statistic studied last 100 year textbook good primer statistical inference casella berger statistic wasserman mle u boomkin94 mentioned non linearity make difficult might success bootstrap see slide also take look empirical likelihood approach nonlinear regression please read martin abadi etc google brain two paper actually got machine skip statistic entirely",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Inverse machine learning_ What if we knew the actual model, how to infer the parameters_.txt",
    "text": "e grid search op really appreciated post willingness helpful thanks answer reading stuff bad even understand question op train new using given x classic bevington like u jimboskinbo answer though nice see thing written bayesian friendly agree going go bayes like bda",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Is it possible to also quantify _epistemic uncertainty_ for denoising diffusion probabilistic models_.txt",
    "text": "source reddit machinelearning title possible also quantify epistemic denoising diffusion probabilistic content currently applying ddpm score based generative dataset one problem trying solve limited training size wonder work could also quantify epistemic kind generative example come mixture gaussian get one sample distribution ddpm training sample generated would exactly training point possible assign prior distribution like gaussian distribution underlying distribution training set small trained ddpm would produce sample prior training seen ddpm could produce sample true underlying distribution comment sound lot like variational diffusion variational inference discern epistemic aleatory though could try fiddling gaussian kernel variational since diffusion usually deep meaningful way would highly non trivial maybe hope using domain tailored parametric least dnns denoising function little differently scientific method denoising edits many variable whereas scientific method isolates one variable scientific method test accuracy world whereas probabilistic diffusion treat input ground truth intuitively would check whether input match fitting latent space select closest fitting pas top p good luck finding tokenizer hull latent space denoise separately respect variable isolate variable denoising isomorphism top p fitting tokenizer input interpolate probability variable specific isomorphism evaluate whether prompt match fitting repeat isomorphism prior best matched fitting check whether best interpretation match ground truth seems like difficult task could outsource epistemics government owned alignment team think true vi posterior parameter exactly describes epistemic see principle parameter decrease assumption one true set parameter describe generating process aleatoric come likelihood p x theta p x theta depending context nah parameter posterior posterior interpretation assumes vi using regularization generative inference often used sampling different equally valid outcome latents",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Leveraging Dropout for Uncertainty Quantification _ Adversary Rejection.txt",
    "text": "source reddit machinelearning title leveraging dropout quantification adversary rejection content hey use quantification lot work production risk mitigation made write underlying idea background put demo notebook would love feedback ml engineer company thanks comment great visuals suggestion point curious reader reading one famous paper subject great point obviously much innovative refactored giving citation idea would key thanks time",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Linear model for variables that have an associated uncertainty.txt",
    "text": "source reddit machinelearning title linear variable associated content consider standard linear regression wx b problem every x associated e come normal distribution associated mu sigma corresponding best guess true mean mu certain guess sigma could use mu ignore sigma seems like principled way dealing example could imagine something like optimizing gradient descent scaling rate sigma high good way deal comment known measurement error error variable recommend using one well established existing method task think typical bayesian inference problem prior knowledge x n mu sigma would form prior distribution simply run map estimation mcmc familiar kind name error variable regression might name specific field familiar basically simple kind latent variable run always infer assumed mu directly easy code expect probably way marginalize something know sigma always use weighted linear regression idea basically let observation confidence influential less confidence done specifying positive diagonal assuming iteration course generalize matrix weight using weight transform x matrix linear regression come example mean different sized population weigh mean number sample used calculate n 1 variance central limit theorem r let specifying weight lm function called orthogonal regression total least square mainstream scientific computational software something solves problem know scipy sampling distribution x also corresponding hierarchical would solve sampling x single another issue look modeling predictor stochastic method like gibbs mcmc easily handle might overkill end though looking point estimate interval prediction solution edit mentioned another comment total least square way go know x x level otherwise bespoke necessary wikipedia page map prior parameter known observation prior parameter still applicable multiple x ie w 1 x 1 w 2 x 2 b really need would nice point estimate fine might misunderstood answer question total least square way go know x x level know x different every x different every training example would total least square applicable case total least square least square technique uncertainty x ordinary least square assumes know x absolute accuracy variation thereof theoretical requirement tl standard least square linear regression e uncertainty x come normal distribution applicable time worthy mention tl always perform better run mill linear regression first try fit assuming x apply tl compare method also go tll approach parsimonious pragmatic many training sample point estimate difference estimate differ much anyways pointed another reply ols might better use depending overall purpose mathematically size error term predictor good luck sound like working something interesting",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Literature_blogs introducing the idea of uncertainty in Machine Learning models from the basics_.txt",
    "text": "source reddit machinelearning title literature blog introducing idea machine basic content spot difficulty figuring start introduction would helpful comment maybe give resource bayesian neural network might one way blog reference",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Loss Function, Uncertainty.txt",
    "text": "source reddit machinelearning title loss function content hello member soo question suppose architecture image classifier end trained mnist image need train image passed classifier outcome result prediction need use order develop loss function train whole use true label image resource idea related helpful pls share suggestion appreciated thanks comment understand question correctly want know train aware case check following work many article written topic estimation suggesting newer newer method surprised see well classic technique ensambling dropout augmentation work easy implement would probably start ensemble dropout see go mean pretrained want train another w access label yes pre trained classifier cifar train denoise another using prediction classifier classifier another attached one access label correct come mind 1 basically guy explicitly train high entropy distribution sample proxy ood sample use original lot noise require label would trust prediction pretrained dataset 2 really estimation author show speed training process new already another pretrained basically prioritized training loss value pretrained proxy priority loss high good chance point noisy corrupted etcetera hence point skipped overall would focus augmentation like loss augmented basic rotation stuff maybe mixup highly differs loss original maybe indicator enough compute could try unsupervised train feature extractor triplet loss something like train classification head help pretrained thing much robust noise pretrainedl unsupervised challenging remember article saw though make sense relate",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Machine Learning _ Explaining Uncertainty Bias in Machine Learning.txt",
    "text": "source reddit machinelearning title machine explaining bias machine content interesting topic one attempt extract meaningful interpretation bias machine anyone know related paper topic already read several paper ribeiro marco tulio sameer singh carlos guestrin trust explaining prediction classifier proceeding 22nd acm sigkdd international conference knowledge discovery mining acm 2016 lipton zachary c mythos interpretability arxiv preprint arxiv 1606 03490 2016 paper try interpret certain produce prediction interesting explain uncertain point thank much help comment paper explains prediction identifying training point responsible given prediction directly explaining estimate probably relevant training point weakly influencing prediction would explain uncertain couple suggestion bot bleep bloop someone linked thread another place reddit r trusting ml machine explaining bias machine follow link please respect rule reddit vote thread info r totesmessenger contact message compose r totesmessenger looked explainability literature really look thing like platt scaling ml terrible modeling certainty neural net might slightly better method slightly without tuning thank suggestion thank suggestion explainability estimation thank suggestion explainability estimation thank reply yes actually interested bringing explain ability prediction searched explainability literature dont know platt scaling term new suggestion start course wikipedia already searched thank neural net might slightly better method hi reference regarding claim see filled away hi rmfajri platt scaling found pretty good post hoc method calibrate probability classifier alexandru niculescu mizil rich caruana predicting good probability supervised icml 2005 basically calibration closed issue estimation quoting well calibrated classifier probabilistic classifier output predict proba method directly interpreted confidence level instance well calibrated binary classifier classify sample among sample gave predict proba value close 0 8 approximately 80 actually belong positive class recently guo et al also address problem calibration modern neural network tend overconfidents prediction probability hi arlinp thank reply literature basically calibration closed issue estimation mean calibration already able solve estimation looking way interpret prediction e give 0 8 sample thank answer trying understand predicted probability 0 8 0 7 0 4 sample belongs class c indeed looking explainability estimation address problem evaluating confidence prediction far know embrace several topic failure prediction assess whether prediction might correct wrong classic baseline neural network use maximum class probability distribution detection imagine given trained several picture dog breed user ask decide dog breed using photo cat may adequate classify sample prefer discard calibration said answer question output probability match notion confidence refers frequentist point view robustness common adversarial perturbation slightly change image like robust noise long human still able classify see estimation varies regarding application want use",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] MCDropout and CNNs.txt",
    "text": "source reddit machinelearning title mcdropout cnns content hello working implementing mcdropout mine order obtain estimation prediction particularly important dealing regression problem referring method proposed gal ghahramani z 2016 dropout bayesian approximation representing deep understand correctly basically necessary include dropout layer every layer leave active also inferece far good easy implement doubt arise fact working cnn particular resnet far know dropout used convolution layer required something like spatialdropout2d tensorflow dropout2d pytorch usually employed wonder convolutional part neural network would better use regular dropout layer paper prof regular dropout lead network equivalent bayesian neural netowork variational inference approach proof work also 2d dropout case use dropout convolutional part nn another question evaluate performance neural network function dropout rate use regular neural network would able test nn validation dataset find optimal dropout rate minimizing whichever criterion choose case since prediction neural network estimation estimate best would imagine play role found piece code gal defines log likelihood use determine best somewhat standard approach thank much help comment agreed upon way implementing mcdropout cnns add dropout2d layer pooling layer max pool avg pool whatever evaluate depends want measure could try show segmentation classification performance change dropout rate measuring estimation tricky mean could show network better calibrated confidence using log likelihood even better expected calibration error brier score lastly evaluate whether measure something like standard deviation entropy prediction help predict performance network future image experiment find dropout good task ensembling test time augmentation better choice loss function important 2nd question might interested followup paper also gal et al concrete dropout treat dropout rate variational parameter sample code gal github got good answer direct question already may also interested aaai paper easy step improving reliability usefulness estimate use stream task sure final task may value one thing worked well past using stochastic depth inference time think may overloaded term context talking mean training inference randomly drop entire feature map instead neuron e one conv layer 100 feature map output probability drop 1 average feature map outright zeroed used technique get active seemed perform better using dropout fully connected layer end used popular pytorch implementation efficientnet see talking line 127 understand code pretty straightforward modify forward pas allow stochastic depth inference may also want modify network include less stochastic depth term corresponds bayesian neural network way want say sure although bat would say almost definitely based intuition dropout killing neuron probability whereas stochastic depth killing set neuron e 20x20 feature map 400 neuron probability would say work better application go edit u sugar scoot comment look like referring dropout2d function pytorch best go almost exactly thing project adding dropout dense layer considered using bootstrapping instead estimate least ground truth carefull mcd really give even paper using minist mnist letter instead number unable distinguish least look e ot equally uncertain also match personal experience domain like reinforcement plotted overall memory found gaussian distribution nearly mean std entire randomly generated set also need careful wit bootstrapping network common core essence issue mcd thing found far give true real ensemble trained",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] MCDropout and CNNs.txt",
    "text": "fold bootstrap sample dataset two highly correlated still want use whatever reason least evaluate give anything meaningful thank much help could please point paper talk mc dropout cnns need justify choice supervisor xd p case dealing regression task output nn generating function better parameter distribution used sample input neural network make sense ensembling test time augmentation feasible scenario time budget though right practical method scenario happened bayesian neural network concrete dropout still used nowadays find recent implementation make thing really catch thank much take look thank yes look like dropout2d indeed take look see literally dropout layer used efficient net would love use bootstrap generating dataset really time consuming task training unfeasible generate number datasets order magnitude would regular bootstrap approach trying see feasible train smaller dataset give bit predictive power better estimation bootstrap look like best way estimate current setting considering deep nn unfortunately problem bootstrap option take long generate dataset train exploring several option get understanding error prediction completely sold mcdropout seems would give reasonable understanding confidence prediction nn prediction completely wrong feel like guarantee error bar enclose correct answer also try channel wise dropout entire channel kept dropped called dropout2d pytorch paper talking exact thing go also one far undersand bayesian nn thriving problem estimate posterior really deep unfeasible impossible either shallow enough train bnn get reasonable result get uncorrect posterior thus uncorrect error estimation example far understand cannot train bayesian resnet50 test time augmentation ensembling totally fine test time inference time still order millisecond less course training time impacted ensemble training cnns long undertaking even former experience use dataset entirely distribution still produce margin average also used train dataset would expected something like gamma beta distribution entire dataset certain large part long tail lower certainty though mcd produce gaussian unfortunately dont remember title paper might able find later though approach think interesting never tried use cyclic rate use last n lr minimum ensemble approach dont need train multiple one though still highly correlated sure work sound interesting enough try imo",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Monte Carlo Dropout for pretrained model.txt",
    "text": "source reddit machinelearning title monte carlo dropout pretrained content know apply dropout training testing get several result prediction considering pretrained add dropout layer fully connected layer every convolutional layer read paper implementation one applies dropout fully connected layer using pretrained however using custom usually one add dropout every conv layer using dropout fully connected layer considered actual estimation comment simply cannot add dropout pretrained without least fine tuning trchnically speaking add dropout every layer paper mentioning wrong also dropout quite crappy estimation mechanism use ensemble variance network much better result source phd thesis topic estimation deep neural network see oh going fine tune implicit mind sorry could point paper support usage ensemble variance network mcdropout approach also thesis available phd still ongoing thanks advance score everything cool thing mc dropout easy adapt vanilla deep net perform probabilistic prediction also circumstance okay good best opinion mc dropout fill two criterion fairly well agree still better method discovered know mc dropout bayesian good comparison various regression estimation approach mc dropout score worst good comparison various classification estimation approach mc dropout score worst currently writing thesis graduating may june year main reason picked mcdropout easy adapt factor right okay estimation enough also believe one apply dropout every layer backbone true mc dropout estimation paper us dropout fc layer estimation thank much good luck thesis phd thesis topic unsupervised classification medical imaging application studying estimation day funny downvoted anyways dropout play well convolutional layer common consider dropout dense layer definitely play around see work better lol anyways thank much running baseline gonna try mc dropout approach",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Outlier analysis in machine learning.txt",
    "text": "source reddit machinelearning title outlier analysis machine content trained multiple ml noticed certain sample consistently yield high prediction error like investigate sample harder predict whether due inherent noise quality issue limitation make sense focus sample high error outlier would method e g estimation gaussian process appropriate comment always consider kl divergence nothing surprise anymore considered tried shallow ml like ridge lasso regression along tree based like random forest xgboost used 2 small set 150 350 sample bit vector lenght 2048 feature",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Research on predictive uncertainty _ confidence_.txt",
    "text": "source reddit machinelearning title research predictive confidence content interested advance modeling deep particularly classification know paper google 1 2 work bayesian nns significant change improvement code implementation also appreciated 1 2 comment new 2016 quite simple way estimating monte carlo dropout mentioned paper mention basically simply keep dropout enabled inference time instead disabling like would normally make multiple prediction dropout enabled give distribution prediction balance amount dropout nn get good estimation though hope link help try search varational inference neurips 2020 tutorial practical estimation distribution robustness deep google people pretty recent takeaway simple baseline get general work well dataset shift monte carlo dropout many dropout average prediction recalibration temperature scaling minimizing loss respect recalibration dataset running deep ensemble restarting training network different initialization also hyperparameters bootstrap resampling dataset replacement retrain work well ensemble author recommend probabilistic ml bayesian nns ensembling many nns different running deep ensemble affiliated w google summarised tutorial blog post look like get full 11 hour tutorial good primer non nn method classification regression generic approach know evaluating inference time calibrated looking predicted probability using ensemble tta test time augmentation icu survival prediction incorporating test time augmentation improve accuracy ensemble based publishing soon ieee maybe check approximate bayes approach easy use pytorch however facing huge difficulty finding optimal hyperparameters training checkout principle deep ensemble quite easy bnns always best choise might interested concept distributional generation focus average train test set error much entire distribution prediction e g softmax ed output word make training test set prediction similar output distribution rather similar loss accuracy work related bias variance tradeoff overparameterized interpolate training applies neural net another recent paper build look calibration connection distributional generalization look evidential deep train predict higher order dirichlet distribution instead direct label beautiful way capture nns wrong using quantile regression binary classification discrete target ah sorry",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Train a model from multiple data sources with different uncertainty_.txt",
    "text": "source reddit machinelearning title train multiple source different content two datasets b contains 5k high quality say confidence 0 9 b contains 500k low medium quality confidence 0 6 0 7 feature format aligned pre processing step question trained synthetic way get score use predict know ensemble concern dataset enough train individually advice help train better scenario welcome thank advance comment union two set use library allows row based weighting record weighting weight high confidence stuff highly low confidence stuff poorly gaussian multi fidelity process 1 tried pypi statsmodel give statistical estimate 2 wish use ensemble give b appropriate sample weight case set weight say 10x 100x higher large volume low confidence training set b drown way prioritize point using training set b well really high confidence sample dataset could use biquality algorithm famous algorithm reweight l2rw gold loss correction glc example thank found take look thank answer take look",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] UNCERTAINTY IN GRADIENT BOOSTING VIA ENSEMBLES.txt",
    "text": "source reddit machinelearning title gradient boosting via ensemble content paper hi paper explores use using single meaning ensemble tree generate technique implemented catboost question implemented xgboost technique look easily applicable would expected implemented already 2 year old reason applicable xgboost figure 1 paper showing virtual ensemble comment answer pretty simple made yandex yandex owns catboost course implement framework hmm suppose author prerogative implement research order achieve goal publishing paper making reproducible software engineer library writer research good ideally someone else could make pr make question whether useful research necessary understanding topic judge",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Uncertainty Quantification in Deep Learning.txt",
    "text": "source reddit machinelearning title quantification deep content article summarizes classical paper measuring deep neural network overview article felt quality article much higher typical getting started ml kind medium blog post people might appreciate forum comment indeed well written article thank sharing learned lot thank sharing presentation month ago theme well someone interested always prefer call estimation instead quantification new overlap area ml interpretability would explain individual prediction change recalibrate nn another seed usually calibrate multiple nn different random weight initialisation take best performing one short path individual prediction stability would make sense average top n prediction wrote post topic main content linked pdfs used calibration plot calibration error evaluate estimate also found deep ensemble mc dropout increase accuracy calibration using cifar 100 believe 1 bit estimate method give estimate measurement true underlying certainty would require datapoints pair label instead maximum likelihood training would full kl divergence different training scheme see detail general get estimate deep known learn random datasets exactly heart kill 1 distributional parameter estimation set mean label var 0 2 quantile regression get true quantile information 3 ensemble estimation bayesian method depend prior distribution know true prior deep neural network kernel gp dataset kill 1 gaussian process 2 dropout based method fix using hold train estimate e g use distributional parameter estimation sample mean trained use hold fit prior gp nobody time quantification estimate accurate output actually ml interpretability interpreting whole really accurate without much interpretability yes usually expect variance prediction depending initialization source randomness like sgd combining several called ensembling common technique e g random forest ensemble decision tree training many nns course expensive averaging make sense regression classification majority voting elaborate random datasets exactly heart defeat point getting estimate seems aforementioned method aim estimate true give metric useful downstream task agree current dl estimate pretty questionable would cause statistician cringe statement really correct aleatoric need holdout verify quality estimate learned training exact situation evaluating original prediction prone overfitting estimate epistemic situation much nastier even described problem want able quantify input might come completely different distribution one underlying training thus amount holdout distribution help truly assess quality epistemic estimate rather need application interest assess useful estimate application context particularly encountering rare abberrant event exception course bayesian inference unrealistic setting likelihood prior correctly specified deleted ah right make sense network enough power learn dataset heart information left quantify e get information point training dataset say nothing certain actually worst case going mislead e g ensemble method based tend regress mean absence information give high confidence far away outlier e g everything based gaussian kernel maybe get something based relative variance point e g variance less sure could actually proof need holdout verify quality estimate counter example regression task true underlying variance 2 learns training heart selection give best return variance 1 hold mse 3 quality estimate error mean learns dataset heart return loss 0 case see different slope pinball loss quantile information left talk deep linear regression true f x e e n 0 2 mean predict e",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Uncertainty Quantification in Deep Learning.txt",
    "text": "x memorizes training hold memorized tend look much worse via say mse different mean accurately approximates f x base predictive memorized training would never chosen first place proper selection procedure sure mean hold mse 1 sufficiently large hold set basically impossible hold mse much less 2 bayes risk example estimator output variance 1 see mse 3 hold reasonable selection procedure estimator choose estimator instead favor one estimate variance 2 point everybody already us hold selection right thing whereas seem claiming people using training selection clearly wrong nothing estimate also wrong selection based training original predictive estimate e x talking regression talking classification pinball loss applied nn anyway allow train extend execute validation frequently enough early stop simple talking regression point x g x epsilon epsilon n 0 1 learns f x pinball loss 0 measure take longer mean early stop likely get proper quantile information think time place snarky answer validation check quality center also quantiles take forecast center earlier epoch quantiles much doable btw good reason assume error normally distributed",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Using gaussian processes with varying uncertainties_.txt",
    "text": "source reddit machinelearning title using gaussian process varying uncertainty content try use gaussian process modeling mostly smoothing interpolation measurement different uncertainty uncertainty known vary lot e g point error 1 others 20 case measurement error gaussian process perform well either give high noise estimate give much weight bad point option take error account read material modeling error nested gaussian process fit case error including nearby point unrelated comment calculate training covariance add diagonal want different training example add different known estimated value instead estimating global noise variance adding every term diagonal could try searching research heteroskedastic kriging get reasonable estimate test datapoints add noise term covariance matrix particular cov x test x test want true function value test point noisy measurement point former add noise term test point latter need way estimating noisyness measurement typical approach use another gp noise variance function input location need former variance true function value latter obviously impossible error different point totally unrelated case thanks adding noise term test point seems work reason thought result zero variance estimate two source really something like measurement wonderful gps make explicit imho one main reason much beauty",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Utilizing Uncertainty Estimation to increase Auto-Label efficiency for Active Learning.txt",
    "text": "source reddit machinelearning title utilizing estimation increase auto label efficiency active content first post read reddit pretty much everywhere many claim smart assisted labeling tool actually help build quality datasets faster towards published article recently wrote improve labeling efficiency auto labeling estimate active wanted share community would love hear thought comment quick read comment think post like belong community post done behalf commercial entity basically patented special approach xyz use product noise disingenuous waste time reader approved post even ignore superb specific aspect think post value providing overview estimation method word remove section 5 would still interesting article decided yes agree somewhat disappointing blog post structured existing method suck pay u use method instead however think reflected simply upvotes general reaction blog instead existing method fix issue want implement use service also advantage think would better received also surge article hosted medium medium let u read couple free essentially pay walled would glad see less post regardless value content sharing knowledge free charge always part scientific community",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] What are some good alternatives to Monte Carlo Droupout that you've come across_.txt",
    "text": "source reddit machinelearning title good alternative monte carlo droupout come across content looking different method estimation quantification deep graph neural network originally came across mc dropout however based thread subreddit come conclusion likely considered good estimate exactly bayesian either lead question title working something inherently probabilistic gaussian process meaningfully get estimate come across anything reading research make method stand especially comparison quick estimate like mcd comment recently reviewed topic journal club deterministic method posting two recent paper seem benchmark well alternative mc dropout 1 spectral normalized neural gaussian process 20 interesting approach us gp final layer imbue distance awareness spectral normalization add lipschitz constraint distance awareness easier achieve 2 distance aware bottleneck icml 24 approach based rate distortion theory combine variational information bottleneck rate distortion theory make set codebooks encoders training dataset using compute distance aware estimate test set believe various benchmark seem perform least par mc dropout deep ensemble require one forward pas computationally intensive mc dropout deep ensemble good review benchmarking various quantification method minus distance aware bottleneck practicality deterministic epistemic give broad overview alternative approach mc dropout conformal prediction uq easy implement agnostic nice theoretical guarantee also read paper recently showed single bayesian layer final layer non bayesian neural net good uq fully bayesian network get nice property bayes fraction compute cost look come different field take everything say solid grain salt switch something fully bayesian take sampling based estimation strategy mcmc smc inject noise need also reasonably performative variant mcmc smc sampler large dimensional constructing posterior like density neural network straightforward well also get measurement require dropout far easiest common way perform quantification opinion poor uncertainty encountered likely due base necessarily method dropout prediction good initial source transformed well calibrated prediction see paper example accurate uncertainty deep using calibrated regression experience ensembling work quite well afford also couple comment estimating mean standard deviation get epistemic aleatoric removed could estimate mean standard deviation sample back propagate pdf see doc thank seem excellent look article linked ooh thank look briefly read conformal prediction though understand correctly actually provides u prediction interval confidence interval forgive silly question seriously im new would know one want assumed confidence interval important originally seems prediction interval pretty decent uq well silly question number 2 implementing bayesian layer essentially mean also interestingly time tried run bnn different problem prediction ability amazing ig main benefit would would get estimate reference usecases bayesian performs competitively never came across hmmm yeah initially read continued looking found allegedly may great according comment pretty much entire post reddit working science focused problem assumed mcd would preferred case def take look linked though thanks definitely since required space compute ability ensemble mind go something like k fold cross validation multiple different holdout set ensembling result fold would correct already admitted come different field type application probably thinking something help op looking mechanism replace dropout provides measure bayesian technique fill hole aside think bayesian always get bad rep quite understand lot frequentist method understood limit case bayesian method also estimation reallly",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] What are some good alternatives to Monte Carlo Droupout that you've come across_.txt",
    "text": "time consuming use outdated method plus exploring multimodal feature interesting could usually retrain different initial seed weight initialization enough get diversity think problem scaling think successful bayesian deep neural network laplace approximation fully bayesian either computable obtaining parameter sota technique probably right played around stochastic gradient version hmc proposed long time ago admittedly worked decently enough smaller ignore atrocious code well designed proposal surrogate distribution could get close would guess suggested resource came across name article didnt read much time ymmv look paper laplace redux effortless bayesian deep paper corresponding implementation laplace approximation torch probability ok nice quick resource also look chapter 4 bishop prml awesome thank",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] What exactly counts as â€œuncertainty quantificationâ€_.txt",
    "text": "source reddit machinelearning title exactly count quantification content trying wrap head around exactly meant quantification uq context bayesian ml sequential decision making uq specifically estimating thing like confidence interval posterior variance general like estimating full predictive distribution since quantify parameter example fit mixture approximate distribution already considered uq since essentially quantifying method like expected improvement value risk integrate distribution give single number reflects something considered uq method acquisition utility function use estimate rather quantify came currently writing section related topic trying draw clear line uq acquisition function think blurrier get especially context single line acquisition function like ei ei clearly fit uq field us full distribution often gaussian unclear part referred uq non gaussian process understand might open ended question would love hear different opinion people might topic comment acquisition function function measure utility acquire new hence name quantification mean putting number mean many different thing depending field approach simplest variance prediction making gaussian assumption know mean variance fully characterize gaussian distribution always gaussian distributed related typical acquisition function want measure like think quantification process describe information lack transferred output might coming measurement imprecision naturally stochastic generation process parameter come sure best describes want work much transferred output given sense application bayes theorem necessarily explicitly use mixture question probabilistic generative try capture sense indeed quantifying however mixture modelling alone take account within parameter e g many equally accurate mixture likely closest true generation process quantification half source unless full bayesian mixture parameter prior still plenty effective main source e g overlapping class question far trivial imo research spend suprising amount time come satisfying conclusion anyway research question evaluate predicted regression moment personal conclusion anything rmse point prediction metric like mae measure considered quantification context predict threshold obviously quantification rmse say anything go predicting full distribution etc guess generalization classification would anything beyond thing calculated based predicted class label e even thing using probability auc classification predict kind default sure spent much time thinking one uq evaluation step regression talk ei var cvar step use quantified active paradigm decide next move either decide next probing point space adjust parameter control problem lot quantification e g gaussian process polynomial chaos expansion course nn related method like mc dropout found work karniadakis group give nice high level point view uq although research focused scientific machine",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] What is the current consensus on the effectiveness of Active Learning_.txt",
    "text": "source reddit machinelearning title current consensus effectiveness active content lot research going active feel nothing conclusive coming field lot method struggling beat random sampling baseline example used publish promising paper using drop estimation people tried use performing well since sure dl method properly bootstrap identify know feel people urged publish paper easy fake number reportedly beating baseline next guy try reproduce result skepticism justified method link appreciated productively used industry edit narrow discussion mean active sense finding next sample label maximize improvement current performance comment active general tricky work well really easy make best effort tuning parameter baseline method get really bad result even outperforming random baseline furthermore shown hard predict particular method performs best ahead time mean collected non random sample couple method finding best one may well better spend total budget collecting truly random sample use normal ml also study year ago finding significant gap current sota could potentially achieve among several finding interesting one seems best sample sequence collect quite homogeneous uniform sharp departure based sampling focus decision boundary 1 active work try choose say 20 example labeling get performance good labeling say 70 example practice short example label label 2 active often assumes example valid labeling say train activity classifier active approach require image person work well 3 active tend work better better architecture like smart student ask relevant question stuck nearest neighbor cant much beyond sampling space evenly personally think active interesting area might lead big jump architecture lot low hanging fruit heyo worked active paper tangentially relate year 2cents active v random sampling active bit like sensor placement imagine 2d map building want security camera strategic location cover area randomly placing camera work lot camera invariably cover everything extremely clever perfectly place sensor use absolute minimum active somewhat two greedily pick iterative fashion next best sensor place conditioned sensor placed thus far instance place sensor cover uncovered area far one really cool result certain regime greedy strategy slightly worse absolute perfect strategy popular selection point expensive see random sampling strong baseline well sampling random sensor placement probably really good requires 100 perfect coverage time regime 1000 sensor matter carefully placed practice lot time random sampling work well match active small amt sensor large amt 1000 sensor sweet spot 100 active competes random sampling often time random sampling sampled regime forget said active much used implicitly practice industry instance classifier work well cloudy image go get labelled active partly human driven company scale lot active active often evaluated simulation instance take equally long label account fact hard instance classifier often hard human well recent jamia paper actual user study cost aware selection strategy practical difficulty use active kind wedded classifier biased sense since feature classifier might change future reluctance use active real project could jeopardize thing small project think probably good option overhead setting probably stop people old post came across organically maybe useful active module named activelab open source cleanlab package research behind able beat random sampling us additional novel algorithm crowdlab module take",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] What is the current consensus on the effectiveness of Active Learning_.txt",
    "text": "account annotator consensus metric super easy use cleanlab multiannotator import get active score score labeled score unlabeled get active score multiannotator label pred probs pred probs unlabeled uploaded paper compare deep batch active method regression 15 large tabular set see corresponding reddit post regarding question found studied active method effective set sometimes needing 25 rmse useless set found good indicator well active work ratio rmse mae initial training set rmse much larger mae expect active beneficial course regression specific know similar criterion classification established would also expect active useful much label noise great industry setting number label dollar spent fewer label needed project higher roi honestly idea consensus research world would love hear think active trying solve np hard problem think simplified version one hard problem pool x x sample random variable multimodal probability distribution boolean indicate whether x fall one distribution mode general setting start sample hope cover mode set ideally want active help u find point mode least close hard imagine even theoretically achieved multidimensional multimodal distribution almost fair say always start almost zero due curse dimensionality even though may seem ton active method could solve problem would probably solve np hard problem great insight thanks especially linked paper uniform sampling mean get natural occur oversample every class uniform distribution last point wonder variability homogeneity within given class lead result problem highly homogenous class make sense sample high region example ocr three pretty much look limited hence active make sense want limit number certain three distinguishing cat dog wide variety breed angle action etc within class make sense diversity might optimal focusing highly uncertain image could give set german shepherd black tabby ton cat dog butt practice short example label label really interesting read thank researching active natural language processing domain uncommon lot lot unlabeled example e g year company email report also least setting labeling much 20 dataset usually infeasible unless large company money issue example image label 20 document dataset consisting million multi page technical document promise fulfilled worst scenario spend development time choose suboptimal sample label uniform w r original distribution word imbalanced optimally collected seem need sample minority class look uniform distinctively non random performs significantly better random sampling several visualization paper show homogeneity find characteristic collected markedly non uniform could one reason tested fashon mnist much diverse mnist nlp datasets hard quantify class homogeneity definitely easy mnist extremely hard measure well al work practice 2x work find strategy less work work end practice instead tightening feedback loop production able react quickly something go wrong something go wrong using something like seal help find similar sample make dataset mitigate risk thing happening future domain less unlimited mistake costly might work everyone yet able create dataset deployed production turn huge blind spot rare event op right active work depends field believe state publication reflect state research industry mistaken many niche field top tier researcher conducting cutting edge research yet publish nothing due secrecy one field active shine semiconductor due labeling cost check last year cvpr",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] What is the current state of the art in confidence scoring, calibration and out of distribution detection_.txt",
    "text": "source reddit machinelearning title current state art confidence scoring calibration distribution detection content work able find excluding bayesian backdrop bayesian nns dropout mc method chen et al confidence scoring using white box meta linear classifier probe 2018 generates us probe individual layer nn classifier create confidence score nn output lee et al training confidence calibrated classifier detecting distribution sample 2018 us gan generate borderline ood sample train classifier uncertain sample devries et al confidence distribution detection neural network 2018 sideloads second output penultimate layer classifier predicts confidence score train confident interpolating output network output softmax true value scaled confidence score confident score close output softmax unconfident score close true value mandelbaum et al distance based confidence score neural network classifier 2017 essentially seems simultaneously train siamese network penultimate layer entropy softmax output order make distance penultimate layer meaningful subramanya et al confidence estimation deep neural network via density modeling 2017 find p x assuming p x n z z output network x training true label learned variable though sure learned guo et al calibration modern neural network 2017 use temperature scaling calibrate confidence softmax output liang et al principled detection distribution example neural network 2017 enhancing reliability distribution image detection neural network 2017 odin distribution detector use temperature scaling inverse adversarial training scale input towards confident result rather away order improve separation distribution distribution example lakshminarayanan et al simple scalable predictive estimation using deep ensemble 2016 ensemble different network use adversarial training get confidence score hendrycks et al baseline detecting misclassified distribution example neural network 2016 mostly defined term metric paper used also defined abnormality side loaded auto encoder penultimate layer trained output softmax auto encoder using distribution dataset get confidence score anyone aware work area seems work best practice anyone experience method work work etc comment anything new add list wanted say prop homework subject asking question draft obtain state art best knowledge use large realistic database imagenet 22k train network low confidence outlier supplied large database network exposed many outlier reliably detect outlier new distribution could thought loosely analogous adversarial training network trained cope adversarial example anyone aware work area two work spring mind bayesian hypernetworks krueger et al confidence invariance image transformation bahat shakhnarovich updating comment simple unified framework detecting distribution sample adversarial attack lee et al open category detection pac guarantee liu et al anyone experience method difficult replace surpass maximum softmax probability ranking distribution example many work build detector author allow tune hyperparameters specific outlier distribution recent work beating maximum softmax probability attainable expense external validity future work may include 1 applying research much larger image segmentation video 2 reliably detecting anomaly highly implausible local statistic input pixelwise average two image 3 refining detector notion known unknown unknown fooled title thinking generic e neural net centric recent work icml accurate uncertainty deep using calibrated regression remindme 2 day confidence estimation remindme 2 day confidence estimation thanks draft look next day network exposed many outlier reliably detect outlier new distribution agree theory like idea explicitly training",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] What is the current state of the art in confidence scoring, calibration and out of distribution detection_.txt",
    "text": "outlier distribution sample two problem space distribution example far large even hope span meaningful way image space example cartoon furries playing chess even abstract art unlikely represented current dataset likely different enough image current dataset network recognize also know recognize likely try classify something lee paper got around generating boundary distribution much tractable outside vision space getting ood datasets hard especially collect collect definition distribution seen yet distribution make avoid method use ood general two work spring mind bayesian hypernetworks krueger et al confidence invariance image transformation bahat shakhnarovich thanks ran across bayesian hypernetworks paper seen confidence invariance paper author allow tune hyperparameters specific outlier distribution recent work beating maximum softmax probability attainable expense external validity recent work tuning hyperparameters specific ood datasets another reason fan training ood sample u danielhendrycks opinion paper reliable estimate deep neural network using noise contrastive prior also think bayesian inspired method struggled replace surpass maximum softmax probability ranking thanks regression work talk classification well messaging 2018 07 23 09 20 57 utc 09 20 57 utc local time remind link click link 2 day send pm also reminded reduce spam parent commenter delete message hide others comment message delete e2rvjd4 faq inside square bracket else default faq 0a 0anote forget add time option command 0a 0aremindme reminder reminder message myreminders feedback extension space distribution example far large even hope span meaningful way mixed feeling first saw outlier exposure helped network generalize new distribution point fact show svhn classifier exposed natural image better detect anomaly blob emojis street view letter getting ood datasets hard also test 80 million tiny image database image scraped web recent work tuning hyperparameters specific ood datasets see appendix h paper appendix b paper paper look competently executed add stack paper read think bayesian inspired method struggled replace surpass maximum softmax probability ranking know strong mathematical reason attempted improve performance mc dropout severe limitation many state art architecture resnext use dropout see appendix h paper appendix b paper thanks missed first read mixed feeling first saw outlier exposure helped network generalize new distribution point fact show svhn classifier exposed natural image better detect anomaly blob emojis street view letter interesting wonder network detecting anomaly seen also test 80 million tiny image database image scraped web unfortunately kind thing",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] What is the difference between Confidence and Uncertainty of a ML model _.txt",
    "text": "source reddit machinelearning title difference confidence ml content love thesis yarin gal deep also liked paper chuan guo et al called calibration modern neural network talk guy think difference confidence simply antonym comment classification produce confidence probability assigns belonging given class confidence typically given point estimate using bayesian method measure given confidence example variance estimate tldr confidence mean thing well opposite important distinction confidence v calibration without getting technical like think intuitive manner take weather forecasting example say 40 chance raining tomorrow expressing prediction world rain tomorrow confidence 40 next determine reliability prediction alongside confidence one could ask certain prediction confidence combination actually reflective reality calibration come asks past time gave prediction 40 confidence many time prediction occur ideally well calibrated good world actual event raining would occurred 40 time think confusing arises confidence basically meaning thing others conflate one concept calibration r statistic could probably give better discussion recent reddit also discussed topic might like check distinguish three concept risk calibration first two defined following story go casino coin tell sure coin come head 95 time conclusion low sure number low risk ambiguity outcome almost always head another time go casino time coin come comment sure bias coin probably 51 conclusion high know bias sure also high risk involved outcome could well head tail reasoning could certain coin high risk bias 0 50 uncertain coin low risk bias 95 point gathering one reduce one never reduce risk think like 1 confidence intuitive confidence almost never probability calibration paper use air quote like use confidence reason like say score pretty arbitrary actual number confidence system trained cross entropy loss usually sigmoid relationship class probability like orange line picture trying push example either 0 1 many loss deep produce calibrated probability without bayesian method frequentist confidence interval thing real carl yeah right confusing term guo paper making sigmoid flatten actual probability",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Where to start learning about hierarchical Bayesian models of behaviour and the hierarchical Gaussian filter_.txt",
    "text": "source reddit machinelearning title start hierarchical bayesian behaviour hierarchical gaussian filter content hi sure correct subreddit post wondering guidance place start topic title background psychology knowledge computational would like able general understanding following two paper 1 mathys c daunizeau j friston k j stephan k e bayesian foundation individual front hum neurosci 5 2011 2 mathys c et al perception hierarchical gaussian filter front hum neurosci 8 2014 advice greatly appreciated comment",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] Why is the predictive variance of Bayesian models can be interpreted as epistemic uncertainty_.txt",
    "text": "source reddit machinelearning title predictive variance bayesian interpreted epistemic content direct link explanation predictive variance bayesian interpreted epistemic many epistemic quantification method example mc dropout deep ensemble claim bayesian justify approach quantifying let say claim valid method truly bayesian bayesian method supposed estimate well comment true definition general probability bayesian interpreted measuring epistemic bayesian probability mean assume following thing 1 underlying relationship trying deterministic 2 chosen capable fully accurately computing relationship 3 process fitting dynamical system converges fixed point depends initial condition e initial parameter converges single unique fixed point limit infinite 4 finite seems like true ensemble different version fitted different initial parameter distributed according distribution represents epistemic available key axiom probably 3 assumed unique set fitted parameter limit infinite true ensemble actually encoding combination epistemic well due issue fitting assume make rigorous nuanced e g perspective probably still work fixed point corresponding infinite unique output invariant choice fixed point nuance probability bayesian modeling interpreted derived lack information e g epistemic fundamental derived actual stochastisticity aleatoric combination especially posterior predictive distribution depends distribution looking let give counterexample work say generation process work like x 1 n generate pair x coordinate infinite line 2d follows equation 2 3x n step start return value equation 3 2x number datapoints smaller n capable fitting two line switchpoint 5 degree freedom fitting procedure greedy try available minimize error achieves error rate zero available procedure switchpoint neccessary explain extra capacity switchpoint matter many ensemble create agree 100 prediction quantify epistemic aleatoric right bayesian algorithm uncertainty given question bayesian algorithm gain ability quantify epistemic feel like might better explanation definition lack information e g epistemic fundamental derived actual stochastisticity aleatoric combination especially posterior predictive distribution thing distinguished ever know aleatoric opposed epistemic thanks good example sure discredit fundamental idea suggests need another axiom say something like base maximally complicated expressive course one could still object saying something like ah happens spuriously fit exactly think compelling objection kind objection ultimately amount something like correct uncomputable seems like silly question might well ask know thing unknowable degree meaningful concept seems like expressible via ensemble whose entropy measure amount full usually intractable bayesian definition encode prior belief accurately parameter space also space e g somehow specify assign prior probability parametrizations would consider entirely impossible rarely given tractable bayesian read epistemic given choice correct fully bayesian would multiply subjective prior probability correct szch integrate would assign positive prior probability even cannot even think right assuming performing exact inference bayesian neural network likely accurately reflect bayesian epistemic even would might reflect mine due different prior belief also look pac probably approximately correct think scott aaronson wrote interesting thing pac might find googling said bayesian principle practice pragmatist consider process fundamentally stochastic especially based quantum mechanical principle like radioactive decay counting single photon interacting measuring device white noise radio receiver pseudo stochastic like throwing dice possibly repeatedly generally speaking aleatoric refers associated outcome repeatable experiment people might even claim true stochasticity bohmian mechanic considered valid approach physicist would really recommend read",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D] [P] Variational Inference for Neural Network Weights in High-Dimensional Spatio-Temporal Models_.txt",
    "text": "source reddit machinelearning title p variational inference neural network weight high dimensional spatio temporal content hey everyone currently working spatio temporal prediction project bayesian ml class using combination gnn message passing style lstm goal recursively predict mean standard deviation target variable multiple future step right optimizing negative log likelihood predicted gaussian capture aleatoric far feeding past value target input though plan bring auxiliary variable physical feature etc later seen skepticism subreddit around using variational inference vi quantification particularly expressiveness scalability still curious viable approach capturing epistemic via vi neural network weight especially high dimensional setting wondering best way epistemic ideally variational inference network weight pretty high dimensional 3d structure time space feature method would need scale reasonably technique come mind bayes backprop mcmc dropout maybe even low rank approximation anyone success applying vi large like gnn lstm hybrid way intractable would love hear others tried recent paper worth looking thanks advance comment involve vi library pytagi closed form bayesian inference nn scale allow quantify epistemic aleatory uncertainty lstm architecture already implemented actually used lot lot vi including advi available term scalability able fit milions parameter issue ever track latent variable prior throw advi get distribution estimate remember advi variable isotropic gaussian vi would probably better get full mcmc estimate parameter need theory get done pyro custom guide however sure worth effort",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[D](Active Learning)Confused about retrieving uncertainty scores from an object detection model.txt",
    "text": "source reddit machinelearning title active confused retrieving score object detection content hello trying figure retrieve informativeness score image object detection getting score image classification relatively straight forward confused getting score object detection straightforward way seems training separate classification cropped image object object detection average score image depending number object picture seems like lot work wondering could somehow derive score within object detection without training separate classification thanks advance comment using going use score sdd faster rcnn using score pick informative picture active use least image bring performance fast possible problem classifier misclassifies object find object finding object looking sort objectness score ascending find hardest image going wading lot image per datapoint get think speaking different thing problem want able derive informativeness picture directly object detection without use separate image classifier think would classification classification task image one object task example classifying picture 3 class dataset cat dog rabbit prediction picture softmax confidence score category picture 1 cat 90 dog 5 rabbit 5 picture 2 cat 45 dog 35 rabbit 25 would use sort score likely pick second picture uncertain informative picture object detection one score object detected make hard calculate score basically want get softmax score bounding box mean go diggin within architecture mean go diggin within architecture yes exactly need hard though bounding box usually sort objectness score class score difference classifier need choose way aggregate score chosen per image",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[P] chess-cv_ CNN-based chess piece classifier.txt",
    "text": "source reddit machinelearning title p chess cv cnn based chess piece classifier content hi r machinelearning weekend project chess cv machine project train lightweight cnn 156k parameter scratch classify chess piece 32 32 pixel square image achieves 99 85 accuracy synthetic training generated combining 55 board style 256 256px 64 piece set 32 32px chess com lichess rendering piece onto different board background extracting individual square learns robust piece recognition across various visual style dataset accuracy f1 score macro test 99 85 99 89 s1m0n38 chess cv openboard 95 78 openboard unbalanced class distribution many sample empty square class accuracy representative happy hear feedback comment know read cheese pie classifier first glance second thought cnn determined movie best next move based image chess board",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[P] cleanlab_ accelerating ML and deep learning research with noisy labels.txt",
    "text": "source reddit machinelearning title p cleanlab accelerating ml deep research noisy label content hey folk today officially released cleanlab python package working kink three year first standard framework accelerating ml deep research software datasets label error cleanlab neat feature 1 output already predicted probability dataset find label error one line code output two line code 2 researcher dealing datasets label error cleanlab compute estimation statistic noisy channel latent prior true label joint distribution noisy true label etc 3 training noisy label 3 line code 4 cleanlab full example find label error imagenet mnist noisy label etc full cleanlab announcement documentation link github pypi example find label error dataset pytorch tensorflow scikit learn mxnet fasttext framework 1 line code compute psx n x matrix predicted probability favorite framework first classifier sure compute psx sample way e g cross validation label error ordered likelihood error first index output list likely error cleanlab pruning import get noise index ordered label error get noise index numpy array noisy label psx numpy array predicted probability sorted index method normalized margin order label error cleanlab logo cheesy attempt slogan p happen work google cleanlab incorporated internal code base july 2019 p p work google version stray open source version comment awesome awesome work chance extension regression ml coding assistant vaporware website ton upvotes functional tool thematically relevant ml practitioner piddly upvotes never change subreddit hope former mostly driven bot activity hi thanks question cleanlab work reasonably well right regression discretizing target 100 1000 class finer granularity support downside lose explicit information class 0 12 0 13 highly correlated however modeled implicitly estimate joint distribution label noise really kind sure get picked reddit okay noisy label regression come discrete count object instead 11 12 object count never lack discretizing treating classification problem deviation order 1 probably fine right people add tiny perturbation like purpose call soft target act form regularization complexity reduction e improve generalization test set accuracy deviation order create buck count 0 max scale exponential create exponentially scaling bucket promise lot option indeed deviation 1 fine problem arise due annotator count 1 2 count right ie within 1 behavior dependent interested fwd paper analyze behavior",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[P] Deterministic Objective Bayesian Analysis for Spatial Models.txt",
    "text": "source reddit machinelearning title p deterministic objective bayesian analysis spatial content working project provide deterministic inference prediction algorithm gaussian process using noninformative reference prior developed 1 2 paper code overview method maximum likelihood estimation give poor result gaussian process likelihood strongly peaked point 3 contrast bayesian method fully account parameter require prior distribution specified due lack information difficult specify subjective prior gaussian process ad hoc approach using constant prior lead improper posterior situation truncating parameter space good solution produce result highly dependent bound chosen quoting 4 related common misconception avoid difficulty improper prior one need choose extreme bound parameter space confine analysis bounded space posterior presumably proper instance common attempt avoid possible posterior impropriety using constant prior choose prior constant large bounded region solve problem however posterior resulting constant prior improper ensuing inference often highly dependent actual bound used answer obtained truncating k could different answer obtained truncating 2k reference prior approach 5 6 develops prior naturally adapts spatial design point give weight region parameter value influential lead prior perform well frequentist coverage simulation 2 1 first develop reference prior gaussian process approach extended 2 handle gaussian process noise nugget effect briefly suppose gaussian process z specified x represents known regressor function represents known correlation function 2 parameter likelihood function given reference prior approach first integrate 2 using conditional prior deriving compute fisher information matrix l associated jeffrey prior combining conditional prior give full reference prior see equation 24 2 sketch deterministic algorithm deterministic inference prediction need efficient way integrate posterior algorithm project adaptively construct sparse grid chebyshev node interpolate reparameterized form integrated posterior four step sparse grid provides efficient way approximate posterior arbitrary parameter value relatively straightforward get marginalization distribution prediction distribution 1 using trust region optimizer together exact equation gradient hessian reparameterized posterior algorithm find parameter maximize posterior 2 starting optimum algorithm construct rectangular region oriented along eigenvectors optimum eigenvectors bracket probability mass posterior small threshold 3 algorithm reparameterizes rectangular region center corresponds region high probability 4 following algorithm 7 8 algorithm adaptively build sparse grid chebyshev node rectangular region approximate posterior error tolerance met example algorithm work set zinc concentration measurement along flood plain meuse river 9 goal predict log zinc concentration notebook step 1 4 algorithm result sparse grid approximate integrated posterior set sparse grid used interpolate posterior meuse river set sparse grid result predicts log zinc value predicted log zinc concentration associated credible set length meuse river set reference 1 berger james oliveira victor de sans bruno objective bayesian analysis spatially correlated journal american statistical association 2001 96 456 1361 1374 2 ren cuirong sun dongchu chong objective bayesian analysis spatial nugget effect journal statistical planning inference 2012 142 7 1933 1946 3 berger james liseo brunero wolpert robert l integrated likelihood method eliminating nuisance parameter statistical science 1999 14 1 1 28 4 james berger case objective bayesian analysis bayesian anal 1 3 385 402 september 2006 5 james berger jos bernardo dongchu sun formal definition reference prior ann statist 37 2 905 938",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[P] Deterministic Objective Bayesian Analysis for Spatial Models.txt",
    "text": "april 2009 6 james berger jos bernardo dongchu sun objective bayesian analysis relationship frequentism 7 klimke andreas modeling using fuzzy arithmetic sparse grid 01 2006 40 41 8 jakeman john robert stephen g local dimension adaptive sparse grid interpolation quadrature 2011 9 pebesma edzer j bivand roger class method spatial r r news november 2005 5 2 9 13 comment see posted github link jupyter notebook github render large jupyter notebook case nbviewer link notebook want run code binder link start jupyter server try bot feedback github bot bleep bloop someone linked thread another place reddit r datascienceproject deterministic objective bayesian analysis spatial r machinelearning follow link please respect rule reddit vote thread info r totesmessenger contact message compose r totesmessenger",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[P] How would you measure uncertainty in a classification task_.txt",
    "text": "source reddit machinelearning title p would measure classification task content hello everybody using dropout deep ensemble evidence bayes backprop generate sample measure sample compute mean variance per class see bar overlap sum variance see big number see two mean high say reliable mean think lot method wondering standard guy good idea measure classification thank much comment think term looking calibration well calibrated output represent output least classification problem see previous discussion think asking getting uncertainty prediction next classification book reading practical machine r fred nwanganga mike chapple demonstrate use logistic regression classification create confusion matrix use value derive predictive accuracy sure help typically sample classified human double checked may use compare actually calibrated important keep separate training otherwise space become diluted longer represent accurately reaching expected classification since trained used validate also problem overfitting may occur based solely image longer classifying generalized transformation sure understand post correctly guess look distribution prediction variance test set make arbitrary threshold basically trade performance e auc classification case sample refuse make prediction production time great probably look thanks yeah discussion occurs let say mine way generate sample thing sample thanks answer well exactly n sample let say classified image dog would measure confidence classification generate n sample one image mean across sample give score image class highest score class algorithm selects correct variance get variance per class guess best way measure hence question oh thanks already know thing haha question moment classification statistical question machine one commonly output network interpreted confidence prediction say softmax 3 class 0 9 0 07 0 03 implies 90 confident prediction sample belongs first class evaluated using calibration metric understand well calibrated e confident value also accurate something along line mmmh common misconception softmax classification good indicator confidence give high score input completely outside sample f e high score car rabbit dog classifier another measure needed us statistical sample yeah correct calibration come",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[P] ICML2020, NGBoost_ Natural Gradient Boosting for Probabilistic Predictions.txt",
    "text": "source reddit machinelearning title p icml2020 ngboost natural gradient boosting probabilistic prediction content hi everyone ngboost many u interested output predictive typically form probabilistic prediction many u also fan gradient boosting ability produce high accuracy especially tabular input far simple way get time ngboost brings probabilistic prediction capability gradient boosting generic modular way x parametrized probability distribution differentiable parameter including importantly multi parameter distribution normal weibull etc also support multiple scoring rule mle crp supporting generalized natural gradient also possible survival prediction ngboost personal use case project key conceptual difference gbm ngboost gbm performs functional gradient descent space function whereas ngboost performs natural gradient descent space statistical manifold riemannian metric statistical manifold implied choice scoring rule mle crp hope find project useful project comment come dope user guide time ago worked essentially named 2nd order boosting mle weibull target good enough practical purpose cheap implement xgboost remember make eta small make converge probably something application natural gradient help lot never put enough care experiment though happy see kind solid development comparison non probabilistic prediction method difference functional gradient descent natural gradient descent thank sharing promising algorithm already tried regressor prediction probability distribution look good however problem ngbsurvival implementation really understand output example ngboost import ngbsurvival ngboost distns import lognormal lifeline datasets import load rossi rossi load rossi train rossi rossi iloc 400 test rossi rossi iloc 400 train test train rossi week test rossi week e train e test train rossi arrest test rossi arrest x train x test train rossi drop week arrest axis 1 test rossi drop week arrest axis 1 ngb ngbsurvival dist lognormal fit x train train e train preds ngb predict x test dists ngb pred dist x test fig ax plt subplots 1 1 dists 0 params scale dists 0 params scale mean var skew kurt lognorm stats moment mvsk x np linspace lognorm ppf 0 01 lognorm ppf 0 99 100 ax plot x lognorm pdf x x r lw 5 alpha 0 6 label lognorm pdf ax plot x lognorm cdf x x b lw 5 alpha 0 6 label lognorm cdf prediction value higher even max value test set value plot probably wrong align also try add scale plot look quite terrible transformation needed output plot prediction distribution correctly maybe shed light extend documentation regarding survival implementation thank much important module interface project first time come across story like someone trying come way something like probabilistic prediction boosting thank note hi u vmgustavo thank interest comparison estimate limited likelihood based metric ngboost support parametrized probabilistic prediction compare point estimate non probabilistic prediction method paper two blog post found give good introduction topic e g see example code readme work ngbregressor sorry get section paper asking look promissing definitelly try may ask calculate interval point estimate method wow blog thank point estimate method compared ngboost point estimate mean predicted distribution obviously could compare interval point estimate method cool thank yes paper table random forest column 2 97 0 30 boston dataset 0",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[P] ICML2020, NGBoost_ Natural Gradient Boosting for Probabilistic Predictions.txt",
    "text": "30 come multiple execution different seed ah thanks clarifying error bar around rmse based different fold cross validation different prediction example form interval got thanks tried kinda complex good using decision tree regressor whan changed random forest regressor worked really nice actually dive deeper great work right",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[P] Improving mulitclass classification accuracy with Jain's Fairness Index.txt",
    "text": "source reddit machinelearning title p improving mulitclass classification accuracy jain fairness index content light implementation idea paper leveraging uncertainty softmax decision making low power iot device instead finding uncertainty added jain fairness index addition loss function gist comment thanks sharing",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[P] Lace v0.6.0 - A Bayesian nonparametric tabular data analysis engine for rust and python.txt",
    "text": "source reddit machinelearning title p lace v0 6 0 bayesian nonparametric tabular analysis engine rust python content lace bayesian tabular inference engine built hierarchical dirichlet process rust python designed facilitate scientific discovery instead question lace ingests pseudo tabular learns joint distribution table user ask number question explore knowledge extra modeling lace generative discriminative allows user determine variable predictive others predict quantity compute likelihood number feature conditioned number feature identify quantify attribute variance epistemic missing feature generate manipulate synthetic identify anomaly error inconsistency within determine record row similar others whole given specific context edit backfill append without retraining v0 6 0 release lace focus user experience around explainability changed way epistemic computed jensen shannon divergence total variation distance added functionality attribute prediction anomalousness inconsistency determine anomaly attributable explain predictor important prediction visualize state main page github crate io pypi comment awesome seeing bayesian nonparametrics action great blog post guy made e g infinite mixture rust curious rust limited understanding hierarchical dirichlet process essentially growing array number mixture component baysian magic tricky implement rust thanks man rust implemented language c python haskell rust rust performant easier develop c also python interop story better imo pyo3 growing shrinking thing issue rust graph pain pattern make simple like arena pattern",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[P] Neural Process Family (meta-learning+uncertainty estimates).txt",
    "text": "source reddit machinelearning title p neural process family meta estimate content friend u gordonjo wrote online book series blog neural process np natural way incorporate estimate small regime meta distribution predictor stochastic process online book yanndubs github io neural process family code motivation following people interested np believe large potential improve dl small regime estimate required e g medical diagnosis provide code implement important np cnp np attnnp convcnp convnp well pretrained provide code replicate figure found many np paper unifying framework mathematically implementation wise help research np teaser single np adapt sample different gaussian process convnp adapting sample different gps single np trained celeba 128 increase resolution image impute missing pixel distribution image convcnp upscaling celeba 8x8 image 128x128 convcnp trained celeba performing zero shot interpolation ellen selfie disclaimer co author convcnp convnnp paper comment sweet read could relate np gps dgps get started something article say article 2 type neural process namely use latent variable latent np conditional np roughly speaking latent np conditional np deep gps gps e add latent variable expressive make training complex focus conditional np gps intuitively speaking np bring gps major benefit neural network fast inference n n 2 np instead n 3 gps instead expert define kernel function np use neural network trained intuitively think kernel function implicitly learned rough intuition kernel function np come cost following disadvantage np require large datasets np nice mathematical guarantee gps would suggest reading introduction article get accurate sense np useful thank",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[P] [D] How are you approaching prediction uncertainty in ML systems_.txt",
    "text": "source reddit machinelearning title p approaching prediction ml system content return point estimate sort regardless task situation e g finance risk management prediction important prediction people dealing scenario usually turn generative e g probabilistic program bayesian inference written thought engineer production system deployed kubernetes using pymc bodywork open source ml deployment tool contribute given simulation based nature generative perhaps unsurprising resulting system little slow really interested get feedback approach hear alternative comment general try ensemble leveraging invariance augmentation function prior might interested neurips 2020 tutorial estimation robustness cover modern recommendation current setting traditional belief really work like bootstrap taking confidence mle trained l1 l2 penalty caveat one presenter relevant tutorial icml 2020 tutorial bayesian deep probabilistic perspective construction neurips 2019 tutorial deep bayesian principle couple month ago play neural network learns distribution parameter mean suppose yo trying predict demand input feature output pair n p parameter negative binomial distribution bootstrap build use calibration library sklearn purpose use production pretty happy approximative poor bayesian deepmind recently produced nice solution called distributional rl agent represent probability distribution rather average value state impressive likely big future sure generative particular forecasting often modelled using known conformal prediction coupled probabilistic forecast could try look hopefully find something help possibly give idea another simple approach go quantile regression one area make concerned often come situation point evaluated well represented training type approach fit variance like term work situation learned variance quite wrong outside area well covered training found systematic way deal yet us dropout regularization technique concrete dropout useful measuring 2 cent used kfold cross validation train classifier regressor per output used get mean variance class probability regressed variable also introducing noise input inference time perturbe little bit make sense salary used feature might used estimate confidence think used explainable tool generally explainable approach provide confidence insight using different technique though might adequate transactional use production environment finance use real time prediction monitor well real time die faster others thanks dig bit depth exposure bayesian dl thus far via thomas wiecki introductory example pymc3 discus passing post infer parameter network way struck profound albeit obvious think wow wish watched started master able share sound really interesting done bit demand prediction work past never crossed mind try approach label task set price volume pair something similar bootstrapping entire set sample training set result average error metric one specific instance need score bit like adding standard deviation output regression assuming understood correctly bootstrap us 63 bootstrap actually performs really poorly performance depends fitting dataset fact bootstrap randomization often provide outside random initialization parameter brings question whole idea behind resampling thing ml people think used ensemble decision tree understanding method solve different problem back probability class label given internal concept probability e g support vector machine still estimate probability thanks recommendation first time heard method thread turning gold mine reference area",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[Research] Depth Uncertainty in Neural Networks.txt",
    "text": "source reddit machinelearning title research depth neural network content paper code tl dr one way get estimate ml multiple like nn ensemble use disagreement prediction estimate computationally expensive requires training evaluating multiple nns tend heavily overparametrized hypothesize single network excess capacity used make diverse prediction specifically perform probabilistic reasoning depth neural network different depth correspond subnetworks share weight disagreement among prediction yield exploiting sequential structure feed forward network able evaluate training objective make prediction single forward pas cool gif depth network training comment elaborate significantly different dropout sure dropout randomly set activation 0 network make prediction different subset activation dropped leading performing random forward pass lead diverse prediction thus robust estimate dun paper keep weight fixed instead network architecture depth treated random variable mean uncertain deep network single forward pas neural network yield activation every layer use obtain predictive function increasing complexity summary computing estimate dun requires single forward pas opposed multiple pass dropout dun predictive posterior obtained averaging diverse function empirically find make dun estimate expressive dropout tend suffer due mean field assumption see",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[Research] Detecting Errors in Numerical Data via any Regression Model.txt",
    "text": "source reddit machinelearning title research detecting error numerical via regression content year ago showed world possible automatically detect label error classification datasets via machine since moment folk asked whether possible regression datasets figuring question required extensive research since properly accounting critical decide trust machine prediction pose unique challenge regression setting today published new paper introducing effective method detecting error numerical via regression method find likely incorrect value numerical column dataset utilizing regression trained predict column based feature added new algorithm open source cleanlab library algorithmically audit datasets error use code application like detecting entry error sensor noise incorrect invoice price company client record mi estimated count eg cell biological experiment find error regression line code extensive benchmark reveal cleanlab algorithm detects erroneous value real numeric datasets better alternative method like ransac conformal inference like learn check blogpost research paper code tutorial run comment based description sound like regression imputation used fill missing check potentially erroneous point",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[Research] Instance-aware Image Colorization (CVPR 2020).txt",
    "text": "source reddit computervision title research instance aware image colorization cvpr 2020 content video link project link paper pdf abstract image colorization inherently ill posed problem multi modal previous method leverage deep neural network map input grayscale image plausible color output directly although based method shown impressive performance usually fail input image contain multiple object leading cause existing colorization always colorization whole image absence clear figure ground separation cannot effectively locate learn meaningful semantics object level paper propose novel deep framework achieve instance aware colorization network architecture leverage shelf object detector obtain cropped object image fed instance colorization network extract object level feature full image feature extracted similar network fused object level feature via fusion module predict final color colorization network fusion module learned large scale dataset experimental result show work outperforms existing method different quality metric achieves state art performance image colorization author jheng wei su hung kuo chu jia bin huang comment",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[R] A snapshot of few-shot classification.txt",
    "text": "source reddit machinelearning title r snapshot shot classification content video richard zemel april 15 2020 shot classification task adapting classifier unseen class given small labeled dataset important step path toward human like machine present key advance area focus fundamental issue overfitting shot scenario bayesian method well suited tackling issue allow practitioner specify prior belief update belief light observed contemporary approach bayesian shot classification maintain posterior distribution parameter slow requires storage scale size instead propose gaussian process classifier based novel combination p lya gamma augmentation one v loss allows u efficiently marginalize function rather parameter demonstrate improved accuracy quantification standard shot classification benchmark shot domain transfer task conclude discussing open question area comment",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[R] Accurate and interpretable modelling of conditional distributions (predicting densities) by decomposing joint distri.txt",
    "text": "source reddit machinelearning title r accurate interpretable modelling conditional distribution predicting density decomposing joint distribution mixed moment content developing methodology e g accurate modeling joint distribution decomposing basis orthonormal polynomial coefficient similar interpretation mixed moment expected value variance skewness kurtosis e g relation time evolution nonstationary time series nicely see growing likelihood prediction conditional distribution adding information succeeding variable people used predicting value put excel table get better prediction modelling entire conditional probability distribution starting additionally getting variance evaluating predicted value e g expected value using orthonormal basis density predict coefficient moment independently difference standard predicting value separately predicting mse e g moment linear combination interpretatbility could use e g nn instead finally combining predicted density implementation develop kind could suggest use preferably complex low dimensional statistical dependency ml method compare slide recent paper overview comment familiar may find interesting read related variational inference bot bleep bloop someone linked thread another place reddit r trusting ml r accurate interpretable modelling conditional distribution predicting density decomposing joint distribution mixed moment follow link please respect rule reddit vote thread info r totesmessenger contact message compose r totesmessenger indeed kind combine method moment moment problem combining standard moment cumulants density generally tough moment problem instead coefficient basis orthornormal polynomial similar interpretation moment chosen moment problem becomes trivial also mse estimation predict moment independently minimizing mean square error linear regression neural network finally combine predicted moment using method moment predicted density method moment statistic statistic method moment method estimation population parameter start expressing population moment e expected value power random variable consideration function parameter interest expression set equal sample moment number equation number parameter estimated pm exclude exclude subreddit faq information source downvote remove v0 28 concern similar problem one much simpler normalize variable uniform 0 1 e g sort value assign position order uncorrelated would 0 1 let distortion rho 1 polynomial using orthonormal polynomial basis coefficient similar interpretation mixed moment independently calculated optimizing mse e g predict rho x x considered moment x separately modeled linear combination mixed moment linear regression predicted moment get prediction polynomial sometimes get zero used max polynomial 0 03 needed normalization density integrate 1 mle went way sure e g take final mse parameter perform mle gradient ascend step maybe squeezing bit additional likelihood loosing nice property like uniqueness inexpensiveness independence coefficient prefer focus scaling automatic basis selection better understanding generalization problem simplified bit handling higher dimensional finally maybe building neural network neuron joint distribution polynomial continuously updated allowing flexible change inference direction avoiding bayes",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[R] Announcing Confident Learning_ Finding and Learning with Label Errors in Datasets.txt",
    "text": "source reddit machinelearning title r announcing confident finding label error datasets content hi reddit excited share confident characterizing finding label error datasets joint work co author lu jiang google reserach isaac chuang mit promote standardize future research noisy label weak supervision also open sourced cleanlab python package post title confident estimation dataset label abstract exists context yet notion confidence typically focus prediction label quality confident cl emerged approach characterizing identifying noisy label datasets based principle pruning noisy counting estimate noise ranking example train confidence generalize cl building assumption classification noise process directly estimate joint distribution noisy given label uncorrupted label generalized cl open sourced cleanlab provably consistent reasonable condition experimentally performant imagenet cifar outperforming recent approach e g mentornet 30 label noise non uniform cleanlab also quantifies ontological class overlap increase accuracy e g resnet providing clean training paper code top 32 label issue 2012 ilsvrc imagenet train set identified using confident label error boxed red ontological issue green multi label image blue comment would work case imbalanced datasets sampling applied would affect joint probability right mirror work regression problem well would work multi label setup case would analysis similar binary one v classifier label isaac chuang ml nice work would interesting compare recent work like o2u net wonder happens apply random regular random label deep shown fit training set rather robustly even nonsense case course generalizing test set would cl able notice set train random dataset look like good read definitely add read list thanks question short answer cl unaffected class imbalance except discretization error discretization error mean 3 example class true probability class b flipped c 0 7 best could 3 example 2 3 0 67 long answer cl take predicted probability noisy label input predicted probability random cl work theory section paper prove realistic condition cl exactly find label error even error predicted probability every example class use trick deal class imbalance compute predicted probability loss weighting bagging augmentation etc practice fine similarly class imbalance extreme produce wonky predicted probability practice fine example class imagenet training set fewer example using cl observe less accurate label error detection class verified human note predicted probability computed sample use four fold cross validation paper computation increase fold better result thanks question cl work classification however depending granularity need discretize target label example confident work well imagenet 1000 label fairly granular regression target probability bounded 0 1 possible cl could handle 3 decimal target granularity regression reasonable theoretical conjecture run experiment throughout work use term label refer class label reserving target regression target value deal noisy label hi thanks question address multi label paper cleanlab python package fully support multi label term work simple binary one v confident intrinsically work multiple label change needed line like filter np array k l l multi label else k instead considering example given label k instead consider example label k set label ctrl f multi label codebase inspection paper presume single class datasets multiple label handled notion collision thanks question completely random label predicted",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[R] Announcing Confident Learning_ Finding and Learning with Label Errors in Datasets.txt",
    "text": "probability assuming regularization tend uniform application confident empirically checked mnist cifar imagnet result random performance prediction one label often also result edge condition pruning like removing one class example removing example error signal see thanks answer congratulation nice project sure deep dive time use per class probability define threshold understand indeed affected class imbalance however still important calibrated predicted probability output able correctly distinguish noisy noisy example right think way predicted probability bad due whatever class imbalance bad choice etc even zero label error use cl still bad prediction yep make sense thanks answer opening project",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[R] Beating Baselines with Geometry_ Introducing GMC, a Fast and Well-Calibrated Classifier.txt",
    "text": "source reddit machinelearning title r beating baseline geometry introducing gmc fast well calibrated classifier content technical writer ambition prove technical writer yearned learn machine prove try towards achieving developed new classifier geometric mixture classifier gmc seeking feedback community submitting arxiv conference problem linear lr svm interpretable fail multi modal non linear rbf svm mlps effective often operate black box wanted interpretable expressive idea gmc represents class mixture hyperplanes soft union half space us soft log sum exp within class softmax across class like mixture expert without separate gating network interpretable see local expert hyperplane responsible prediction performant competitive rbf svm rf mlps standard benchmark efficient cpu friendly scale inference faster rbf svm par mlp calibrated produce reliable probability algorithm analogy similar baseline accuracy outperforms linear competitive strong non linear baseline speed 2 40 inference time per example see table calibration low ece improved temperature scaling would incredibly grateful feedback core idea differentiation moe maxout clear experiment comparison fair convincing related work might overlooked general feedback clarity presentation find detailed copy algorithm please feel free test algorithm geometric mixture classifie comment understand maybe simple step step description algorithm one 2d dataset e g moon might help figure 1 paper nice shaped curve separating two class get three hyperplanes assume 2d straight line generate smooth curve boundary",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[R] DeepMindâ€™s Epistemic Neural Networks Open New Avenues for Uncertainty Modelling in Large and Complex DL Systems.txt",
    "text": "source reddit machinelearning title r deepmind epistemic neural network open new avenue modelling large complex dl system content research team deepmind present epistemic neural network enns interface modelling deep proposes kl divergence target distribution precise metric evaluate enns quick read deepmind epistemic neural network open new avenue modelling large complex dl system code open sourced project github paper epistemic neural network arxiv comment flag planting",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[R] Diffusion might be a better way to model randomness in PPLs than Markov chain Monte Carlo or VI.txt",
    "text": "source reddit machinelearning title r diffusion might better way randomness ppls markov chain monte carlo vi content probabilistic programming language ppls like stan simplify modeling inference still slow inaccurate markov chain monte carlo precise sometimes slow variational inference faster drawback diffusion better way probability new technique called diffusion variational inference dmvi us diffusion approximate probability distribution faster accurate automated inference btw part trend noticed lately researcher increasingly applying diffusion diverse problem like mapping heat flow robot obstacle avoidance anomaly detection dmvi set guess distribution using diffusion run reverse introduces new way calculate marginal likelihood better fitting also adjusts parameter even better fit early test show dmvi make inference generally accurately ppl method similar compute cost limited tuning needed tldr framing inference diffusion problem potentially overcome limitation current method dmvi might become core part ppl toolkit full summary paper comment two cent without diffusion one problem vanilla vi choose approximating distribution downstream consequence quality result denoising diffusion literature seen far work well high dimensional multivariate gaussians approximating class already well described mean field variational inference clear distribution long tail exotic shape think banana horseshoe distribution better addressed diffusion edit brain fart commenter right meant vi full rank gaussian approximating family something like mind notice diffusion getting tested domain make sense since image generation already complex task issue diffusion 1 best way get direct translation variation needed fact opposite requirement 2 n inference time computationally heavy becoming less issue time go nvidia arm chip vram may longer issue seeing ram mac chip offering think diffusion best want easy step step denoising applicable beyond image anyone applied diffusion text generation surely could bring novel advantage bidirectional generation painting first time diffusion based score function estimation used general bayesian inference fact arnaud doucet several paper already fairly major error equation 1 anyone else spot clear mean field assumption poorly describing posterior variance multivariate normal correlated parameter mean field assumption parameter independent wrt posterior sure talking variational inference algorithm assume fully structured mvn option stan advi mvn constrained covariance matrix currently popular ppl implementation far aware actually comparing dmvi mfvi dmvi would much accurate mfvi non diagonal covariance mvn distribution although agree diffusion work well mvn probably another vi algorithm efficient designed learn normal distribution think paper example trying show dmvi would much better though yes fair number time one read hi u gwern great wonderful special article gpt 3 scale gans seems like special article chatgpt gpt4 diffusion stablediffusion powerful important gpt 3 write looking forward article much much written many thing would hard write article want write gpt 3 stylegan got early enough case gpt 3 playground could still count number serious user appendage could write interestingly comprehensive article gpt 4 sd outran ability cover within day focus thing still make difference example dropcap typography using diffusion one",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[R] Domain-Adaptive Object Detection via Uncertainty-Aware Distribution Alignment.txt",
    "text": "source reddit machinelearning title r domain adaptive object detection via aware distribution alignment content abstract domain adaptation aim transfer knowledge source annotation scarcely labeled target domain attracted lot attention recent year facilitated many multimedia application recent approach shown effectiveness using adversarial reduce distribution discrepancy source target image aligning distribution source target image image instance level however remains challenging since two domain may distinct background scene different object moreover complex combination object variety image style deteriorate unsupervised cross domain distribution alignment address challenge paper design end end approach unsupervised domain adaptation object detector specifically propose multi level entropy attention alignment meaa method consists two main component 1 local attentional alignment luaa module accelerate better perceiving structure invariant object interest utilizing information theory measure local region via entropy pixel wise domain classifier 2 multi level aware context alignment muca module enrich domain invariant information relevant object based entropy multi level domain classifier proposed meaa evaluated four domain shift object detection scenario experiment result demonstrate state art performance three challenging scenario competitive performance one benchmark dataset comment code meaa also found cityscape v foggy got better oracle possible mean learned domain invariant feature expressive simply train cityscape foggy datasets due extra noise caused fog since meaa share backbone oracle version believe result still comparable",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[R] Fast, universal estimation of latent variable models using extended variational approximations.txt",
    "text": "source reddit machinelearning title r fast universal estimation latent variable using extended variational approximation content tl dr approach supervised matrix factorization 1 allows incorporate metadata 2 provides way identifying latent factor dimension meaningful bayesian get estimate abstract generalized linear latent variable gllvms class method analyzing multi response garnered considerable popularity recent year example analysis multivariate abundance ecology one main feature gllvms capacity handle variety response type overdispersed count binomial response semi continuous proportion hand introduction underlying latent variable present major computational challenge resulting marginal likelihood function involves intractable integral non normally distributed response spurred research approximation method overcome integral recent particularly computationally scalable one variational approximation va however research use va gllvms related hampered fact closed form approximation obtained certain pair response distribution link function article propose extended variational approximation eva approach widens set va applicable gllvms drastically eva draw inspiration underlying idea laplace approximation replacing complete likelihood function second order taylor approximation mean variational distribution obtain closed form approximation marginal likelihood gllvm response type link function simulation study application testate amoeba set ecology demonstrate eva result universal approach fitting gllvms remains competitive term estimation inferential performance relative standard va laplace approximation approach computationally scalable practice link r package comment",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[R] Forecasting_ theory and practice.txt",
    "text": "source reddit machinelearning title r forecasting theory practice content link abstract forecasting always forefront decision making planning surround future exciting challenging individual organisation seeking minimise risk maximise utility large number forecasting application call diverse set forecasting method tackle real life challenge article provides non systematic review theory practice forecasting provide overview wide range theoretical state art method principle approach prepare produce organise evaluate forecast demonstrate theoretical concept applied variety real life context claim review exhaustive list method application however wish encyclopedic presentation offer point reference rich work undertaken last decade key insight future forecasting theory practice given encyclopedic nature intended mode reading non linear offer cross reference allow reader navigate various topic complement theoretical concept application covered large list free open source software implementation publicly available database comment potentially strong reference paper traditional forecasting section machine pretty awful 4 ml section page long section describing neural network look like written 2008 describes mlps rbfs generic rnns subsequent section briefly describes deepar primarily focused general concept probabilistic forecast next section describes forecast ml generally focus particular final ml section focused lasso interesting paper comparison forecasting method posted day paper method problem aware review would cover better latest deep approach forecasting",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[R] Google Researchâ€™s Prediction Depth_ Understanding the Laws that Govern DL Data Processing.txt",
    "text": "source reddit machinelearning title r google research prediction depth understanding law govern dl processing content team google research proposes prediction depth new measure example difficulty determined hidden embeddings study reveals surprising fact prediction depth given input strong connection confidence accuracy speed point quick read google research prediction depth understanding law govern dl processing paper deep lens example difficulty arxiv comment valuable interesting research couple question wish author addressed though leave paper bit less comprehensive first distribution affect metric author allude distribution sample within class class significant impact difficulty particular sample primary mode class nature far sample tends dominate gradient descent based training extent easy sample easy simply part primary subclass memorized network canonical example class opposed structurally distinct class would interesting see training set resampled weighted better balance common uncommon subclass effect metric expect would interesting shift sample difficulty category second exclusively consider supervised training would interesting compare network trained unsupervised technique contrastive similar understand influence label trained network",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[R] GP-BART_ a novel Bayesian additive regression trees approach using Gaussian processes.txt",
    "text": "source reddit machinelearning title r gp bart novel bayesian additive regression tree approach using gaussian process content paper paper abstract bayesian additive regression tree bart ensemble method extensively successfully used regression task due consistently strong predictive performance ability quantify bart combine weak tree set shrinkage prior whereby tree explains small portion variability however lack smoothness absence covariance structure observation standard bart yield poor performance case assumption would necessary propose gaussian process bayesian additive regression tree gp bart extension bart assumes gaussian process gp prior prediction terminal node among tree illustrate simulated real compare performance traditional modelling approach outperforming many scenario implementation method available textsf r package texttt rgpbart available comment would interesting could show gp bart outperforms bart causal effect estimation task bart shine realistically bart famous outperforms xgboost like prediction task rather really good atlantic causal inference competition overall effect heterogeneity estimation skepticism comment paper saying actually need slight modification neural network architecture everything difference criticism shocking 3x complexity improvement handful datasets also read paper detail noticed lack confidence interval score random factor gp bart look like increase order magnitude number parameter compared standard bart leading overfitting case standard bart would still fit well since gp bart strictly requires training bart argument gp extension good use extra opposed well known ensemble nice bayesian statistic interval frowned read paper lack quantity surprised fact anything related frequentist statistic major sin confidence interval evaluated using prediction interval gp bart sensible better aspect compared handful datasets number parameter good measure complexity bayesian somehow doubt relevant discus number parameter boosted enough regularisation effective degree freedom control anything 2017 arxiv 2020 bayesia anal bart paper hahn et al specifically aim develop regularization prior nonlinear sure understood mentioned overfitting happen well reason bart still kept shrinkage prior gp therefore one main advantage inherent smoothness gp happen bart love interval bayesian stats called credible interval regard score nothing underlying algorithm rather stochastic nature tree necessary run technique multiple seed identify average improvement use whatever statistical technique like determine whether true improvement algorithm performance number parameter also base parameter space independence feature assumption orthogonal",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[R] Graph node binary classification_ Training set labels contain false negatives.txt",
    "text": "source reddit machinelearning title r graph node binary classification training set label contain false negative content hey working graph node classification problem given graph g v e vertex v set feature x edge e may also set feature want predict binary class node belongs given training set node labelled issue training label based experimental totally reliable sure positive class label correct training contains false positive however negative class e fraction negative class label incorrect want come way try filter false negative label preprocessing step use negative class label sure may result unbalanced class set ok train graph cnn thinking use unsupervised embedding clustering algorithm come dense representation node based feature neighborhood choose negative label node far positive label node space know much node embedding clustering also graph working actually mesh highly regular therefore algorithm based graph structure probably useful wondering anyone idea suggestion approach could look try ideally something fancy preprocessing step clean comment positive unlabeled may useful problem",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[R] InfoSHAP_ Explaining Predictive Uncertainty with Information Theoretic Shapley Values.txt",
    "text": "source reddit machinelearning title r infoshap explaining predictive information theoretic shapley value content paper title explaining predictive information theoretic shapley value presented neurips 2023 link paper link code tl dr paper extends shap way used explain prediction rather prediction could various application example active application sampling decision made based predictive case modern approach like batchbald answer question like decide annotate particular instance reinforcement application decision explore curiosity driven based reward setting used explain agent explore way several application regard explanation feature selection active feature value acquisition covariate shift detection distribution detection highlighted paper abstract paper researcher explainable artificial intelligence developed numerous method helping user understand prediction complex supervised contrast explaining output received relatively little attention adapt popular shapley value framework explain various type predictive quantifying feature contribution conditional entropy individual output consider game modified characteristic function find deep connection resulting shapley value fundamental quantity information theory conditional independence testing outline inference procedure finite sample error rate control provable guarantee implement efficient algorithm performs well range experiment real simulated method application covariate shift detection active feature selection active feature value acquisition comment",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[R] Master Thesis on Bayesian CNN using Variational Inference and Application using PyTorch.txt",
    "text": "source reddit machinelearning title r master thesis bayesian cnn using variational inference application using pytorch content read complete thesis code available thesis involves proposal bayesian convolutional neural network variational inference show way estimate every prediction pruning done reduce number parameter finally applied computer vision task image recognition image super resolution gans let know guy think recently similar thing implemented tensorflow comment somewhat related note guy recommend using getting started vi seems many option different level abstraction feel like understand theory pose variational family optimize elbo use mean field approximation exponential family make nice still really know vi specifically good first step use pyro tfp pymc3 stan roll python use fancy probprog language reading lot bayesian inference dealing special case gaussian process whenever conjugate prior math paper found good way play stuff would intuitive look nice give read look cool may ping advice good tutorial guide implement bayesian nn using pytorch good discussion going sure feedback welcome found check code simple enough get started good tutorial mxnet yes code well written easy follow maybe noob friendly tutorial",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[R] Model Learning with Personalized Interpretability Estimation.txt",
    "text": "source reddit machinelearning title r personalized interpretability estimation content high stake application e g cancer treatment cannot used lightly recklessly need trust achieve trust interpretability key factor field explainable xai concern method explain behavior black box deep neural network method generate white box e interpretable think e g sparse linear small decision tree symbolic expression good reason latter desirable former see e g famous paper cynthia rudin proposed new proof concept work look whether xai interpretable generation personalized follows essentially taken abstract fact current algorithm synthesis potentially interpretable rely objective regularization term represent interpretability coarsely e g size designed specific user yet interpretability intrinsically subjective propose approach synthesis tailored user enabling user steer synthesis process according preference use bi objective evolutionary algorithm synthesize trade offs accuracy user specific notion interpretability latter estimated neural network trained concurrently evolution using feedback user collected using based active maximize usability user asked tell given two time one less complex experiment two real world datasets involving 61 participant find approach capable estimation interpretability different different user moreover user tend prefer found using proposed approach found using non personalized interpretability index preprint accepted appear ec dm workshop gecco 2021 comment",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[R] Neuro-symbolic Neurodegenerative Disease Modeling as Probabilistic Programmed Deep Kernels.txt",
    "text": "source reddit machinelearning title r neuro symbolic neurodegenerative disease modeling probabilistic programmed deep kernel content abstract present probabilistic programmed deep kernel approach personalized predictive modeling neurodegenerative disease analysis considers spectrum neural symbolic machine approach assess predictive performance important medical property interpretability reasoning efficiency leveraging domain knowledge bayesian approach combine flexibility gaussian process structural power neural network biomarker progression without needing clinical label training run evaluation problem alzheimer disease prediction yielding result surpassing deep practical advantage bayesian non parametrics probabilistic programming paper link comment",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[R] ProSelfLC_ Progressive Self Label Correction Towards A Low-Temperature Entropy State (v2).txt",
    "text": "source reddit machinelearning title r proselflc progressive self label correction towards low temperature entropy state v2 content tldr two takeaway 1st takeaway exist three well accepted finding deep easily fit random noise b deep network learn simple semantic pattern fitting noise c modern deep neural work tend confident paper disclose new insightful one complement deep neural network become less confident semantic pattern fitting noise label noise rise 2nd takeaway new technical proposal inspired new finding miscalibration analysis introduced decrease entropy self knowledge concretely propose use annealed temperature learn towards revised low temperature entropy state though research study deep machine finding quite consistent deep human people start learn truth noise low confidence gradually compress truth piece noisy world increasing confidence gradually comprehensively understanding noisy world surrounding u read interested highly related work modern miscalibration analysis calibration modern neural network revisiting calibration modern neural network knowledgedistillation labelcorrection modelcalibration confidencecalibration predictiveuncertainty labelefficiency transparentml interpretableml robustml noisylables missinglabels semisupervisedml sequencetransformers proteinclassification proteinengineering humanlearning bias noise supervision comment",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[R] PyTorch Implementation of the Natural Posterior Network.txt",
    "text": "source reddit machinelearning title r pytorch implementation natural posterior network content sure find spotlight paper natural posterior network deep bayesian exponential family distribution iclr 2022 want take opportunity highlight publicly available pytorch implementation natural posterior network natpn provides fast high quality estimate used problem target distribution belongs exponential family notably includes classification regression count prediction task importantly natpn requires little change existing architecture produce estimate without need distribution thus chance good easily extend standard deep natpn architecture therefore put serious effort publicly available implementation facilitate usage natpn 1 provide intuitive interface enables using easily scikit learn estimator 2 follow modular design allows customize build upon different level abstraction check github comment removed compare full batch hmc nothing useful paper consider low quality approximation true posterior posterior lol work sequence classification simple setting might train reading speed limit street sign using image taken day testing real world give perfectly accurate answer turn dark though algorithm performance deteriorates continues provide number would expect correct worked well day since trained image taken day however lack knowledge inferring speed limit image taken dark therefore would desirable algorithm reason whenever make prediction also provides measure prediction general construct many example whenever training set fully cover domain used might impossible domain huge useful estimate prediction yes certainly long map input fixed size latent space e g using last hidden state lstm natpn used",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[R] The Functional Neural Process.txt",
    "text": "source reddit machinelearning title r functional neural process content functional neural process abstract present new family exchangeable stochastic process functional neural process fnps fnps distribution function graph dependency top latent representation point given dataset define bayesian without explicitly positing prior distribution latent global parameter instead adopt prior relational structure given dataset task much simpler show learn demonstrate scalable large datasets mini batch optimization describe make prediction new point via posterior predictive distribution experimentally evaluate fnps task toy regression image classification show compared baseline employ global latent parameter offer competitive prediction well robust estimate comment",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[R] What is the best method for model uncertainty_.txt",
    "text": "source reddit machinelearning title r best method content hi experimented one interesting topic compared four method softmax temperature scaling mc dropout deep ensemble glad check result interest best method recent please give information comment gave talk topic sometime ago interested honest want estimate today dl would go ensembling simple capture multiple mode true many method difficult achieve result independent random initialization also compare multiswag paper might helpful based ece everything worse keeping softmax would going heavily published also note none paper assume use cosine schedule rate would induce specific property process might follow theory method think like markov chain sampling loss landscape see changing step size pattern like fashion mean parameter space also take specific range value would suggest swag paper actually use rate schedule based idea sampling markov process rather pure optimization point view class known class test train set yes think might using ece incorrectly introducing extra problem distribution detection mix ece indicates calibration prediction calibration iid split train test conclude using metric moreover recently shown many issue check recent paper verified estimation kumar et al neurips thank sharing think pretty high confidence class see large change rotate seems like nearly method set high confidence set couple exception video talk thanks work check table 2 3 4 based ece softmax mc dropout ensemble show similar performance figure temperature scaling best option ece yes heavily result paper used common trick method check suggestion split half target class dataset test set come known dataset check figure unfortunately session recorded got thanks oh well slide still great thanks",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[R] Work In Progress_ A Drop In Replacement For Softmax With Uncertainty Calibrated Scores.txt",
    "text": "source reddit machinelearning title r work progress drop replacement softmax calibrated score content working activation function could work drop replacement softmax main original intention apply principle maximum entropy prior would allow activation function output better calibrated confidence score probability term us formula came convert correlation score 1 1 probability 0 1 idea certainty measured 1 certain 0 fully 1 certainly way convert 0 1 probability setting 0 correlation scale 1 n probability scale relevant neural net sense pre activation signal 0 given node basically mean everything cancelled map maximum anyways want give away many detail given unpublished lot time effort theorizing experimenting many variant formula think something work train network new activation function output layer instance mnist test notmnist resulting output seem much less overconfident equivalent network trained softmax sigmoid however accuracy mnist change much fact task new activation function get similar result softmax used output layer sure useful actually one place might affect performance though point tested larger sota tried using place softmax attention head transformer network language modelling result seem promising know scale work across task toy problem able run single gpu like know experienced researcher whether project worth trying make paper publishing think may something sure bias wanting time effort spent wasted another problem finding course time worked project accumulated many variant activation function determining one actually best task scale proving tricky concerned several version work situationally perform better reliably sense family activation function simpler elegant others one deal kind comment understood replacement softmax increase entropy output easy try experiment verify approach 1 try cifar 100 using different resnet vgg densenet measure calibration error distribution test sample sure inducing entropy get better performance uncalibrated baseline perhaps also compare result entropy regularisation pererya et al 2 evaluation distribution distribution shift natural synthetic 3 avoid transformer evaluation vision side quite fresh existing method might use 4 ideally formal definition activation differs hyperparam select hyperparam using validation set else possible learn well heuristically choosing one red flag eye unless support good reasoning feel free dm experience problem calibrating neural network prediction kind trying measure distribution look cool train mnist test notmnist task meant show distribution sense however activation function concept measuring separately like bayesian neural net rather using prior information expected given number class way align output probability make sense afraid trouble following work done might write short paper blog post document idea would help open dialog want community feedback sound like bayesian posterior bit observables preactivations softmax probability coming want calibrated probability estimate output softened prior external parameter could formulate like concern whatever included loss function optimized always lead overconfident prediction unclear correct without sample maybe goal trained sample implied probability confident always train test gap empirical procedure would measure distribution sample fit linear logodds possibly non uniform sample weighting input predicted score output sample label",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "[R] â€œEvaluating Deepfake Detectors in the Wildâ€_ Fraudster Attacks (ICML 2025 Workshop paper).txt",
    "text": "source reddit machinelearning title r evaluating deepfake detector wild fraudster attack icml workshop paper content hi reddit ever thought difficult determine whether photo genuine deepfake might think discriminative task easier generative one detection straightforward contrary diffusion good detection impossible work reveal current state war deepfakes short sota open source detector fail real world condition work ml engineer leading platform kyc liveness detection setting must decide short verification video whether person claim deepfakes one biggest challenging problem known robust anti deepfake solution trying flex want say work problem daily see fraudsters actually try order bypass verification year kept trying apply research nothing really worked example research solution less robust simple zero shot clip baseline kept wondering whether issue lay setup research seems lot deepfake research overlook key wild condition core issue robustness ood even small amount test distribution leaking training set say 1k image 1m image test pool make trivial achieve great metric experienced computer vision expert push auc 99 99 without peeking however task becomes ncredibly hard paper demonstrates simple reproducible pipeline 1 deepfakes already built large image level dataset using two sota face swapping method inswapper simswap 2 real world condition use small transformation imperceptible human constantly see real world downscaling resize upscaling compression jpeg indistinguishable human detector must robust 3 evaluation test different setup e g 1 real predict real label 2 real v fake 3 real v compressed fake others sound easy every tested least one setting performance drop near random releasing another benchmark yet another deepfake dataset present pipeline mirror fraudsters actually observe production releasing code dataset 500k fake image even small deepfake game test detector detail please see full paper silver bullet solution deepfake detection claim one share teaser result promising setup using zero shot vlms detection post second icml workshop paper separately interested deepfake research would like chat even collaborate hesitate reach cheer comment use jpeg downscale attack mean resolution offer security come forensic verification source example 4 year old phone camera 50mp generative coherently generate image large first pas would think evaluating image large would able detect discrepancy real fake image make sense great piece love work bridge gap benchmark reality proud onboard error generating reply",
    "cluster": 1,
    "keywords": "bayesian, distribution, method, network, prediction, would, paper, neural, like, work"
  },
  {
    "filename": "11530ef7603e3145792d9ee22ad53e97.txt",
    "text": "08 app antonio de la vega de le n antonio de la vega de le n ml expert drug discovery 1 liked lot post found situation first junior senior think lesson quickly assimilated made thought similar experience heard discussed much see new performance claimed better lower experimental technique sound like huge milestone raise red flag think one example alphafold reported predict protein structure better resolution xray crystallography red flag associated result experimental technique define certainty ground truth machine lower opinion run risk overfitting specific experimental assay may relevant biological event measured word noise really care think best together maybe multitask result two orthogonal assay measure biological event interested sadly always available easy obtain agree issue account would adapt ml modelling strategy would great hear people think brandyn ewanek science teacher 1 perfect accuracy biggest red flag machine junior scientist see 99 9 accuracy celebrates senior see thing get worried perfection biggest red flag machine classic case overfitting learn pattern memorized traini",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "2b0f2c38a0ad4b10e38481723770f89a.txt",
    "text": "08 app joseph breeden joseph breeden 1 recently asked someone formerly senior position bank supervision solve explanability problem machine shared thought honestly never problem felt owned still day downtime due illness gave time ponder realized simple answer explainability problem machine problem expect every forecast explainable explanatory instability occurs high curvature region segment boundary real issue forecast explodes region cannot explain forecast exist instead business rule need change first measure forecast high need simple fallback said differently industry still using machine linear mindset machine great finding pocket predictability implicitly mean much less predictive region forecast highly dynamic explanatory instability vary widely note supposedly explainable ml piecewise linear everywhere explanatory stability continuous nonlinear linear appears give explainability forecast near boundary highly uncertain explanatory power purely illusory believe explaination uncertain forecast ran simulation math put whole thing paper hope someone find interesting pdf understanding forecast solves explainability problem machine researchgate net 84 11 eddy abraham 1 joe perhaps answer transparency",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "33b22e84503f85a5788c8666d39abf83.txt",
    "text": "08 app george lawton george lawton technology news writer 4 andreas horn provides interesting overview various mathmetical primatives underpinning system map interesting overview domain science different aspect machine prep wished map like growing help contextualize different aspect andreas horn head aiops ibm speaker lecturer advisor 5 proof 1600 1900 newton backpropagation without training gradient descent 1700 1900 bayes gauss fisher god without generative classifier decision making 1800 language vector matrix tensor without gpt dall e embeddings deep 1948 shannon entropy signal compression loss without llm tuning transformer stable output 1800 root reasoning structure behind algorithm without symbolic decision tree code generation logic prompt 1900 math grouping labeling combining thing without understanding datasets logic modeling yes even topology numerical method chaos theory geometry modern built century mathematical breakthrough scientific innovation magic math finally running scale honestly think beautiful kudos domain science map also video explaining detail highly recommend watch 8 7 930",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "5269b8a8e6286476b9281914867fe29a.txt",
    "text": "08 app yannik pitcan phd yannik pitcan phd 1 optimization theory click embarrassingly late fair still strong suit statistician trained think term estimator likelihood variance bias optimization always background afterthought something algorithm focused real math worked applied ml realized optimization tool use train mle solve optimization problem ridge lasso add constraint penalty optimize even bayesian inference map still optimization something break poor convergence weird result unstable metric almost always optimization pipeline silently screaming day find thinking geometry curvature convexity saddle point ever expected story really maybe quiet insight optimization teach u sometimes fine landscape built around need reshaping statistician moving ml ml engineer skipped theory strongly recommend circling back optimization start seeing differently 23 4 sid meka recent graduate master applied computational mathematics 3 hey aspiring mathematician deeply interested mathematical optimization looking post really true really interesting hope accept follow request continue chatting best sid gurunadh parinandi senior scientist 1 good reading material starting point took optimization course year ago forgot lot thing mathematical optimization course think material may b",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "Agile for data science projects - thoughts on making it work_.txt",
    "text": "source reddit datascience title agile science project thought making work content working tech e commerce site team tasked ranking search result recommendation module result partner team frontend infrastructure syseng using agile scrum manager looking u adopt style working well make work high level output project ie researching latest positional bias proposenity score final comment good thing agile force time effort box task yes outcome uncertain still break smaller task adjust next planning since scientist could work 3 year may follow ups deep dive arise task current spring either continue next one left backlog pick future another important point probably 2 kind epic research implementation research one answer whether something even feasible implementation small clear improvement taking research making component book topic found enlightening science podcast futurology ep 8 agile science felipe flores sfw apple podcasts google podcasts android anchor breaker castbox overcast pocket cast podbean radiopublic rss link different type episode recording presentation 300 scientist melbourne australia theme night agile science passion mine episode cover productivity gain individual team gain using agile method agile imperfect helpful tweaked agile fit science deliver value team bust main myth around agile much much show note presentation slide www datafuturology com podcast 8 always appreciate review follows like rating really help new scientist find u thank much enjoy show linkedin twitter facebook instagram official site second one thing dealing uncertain task focus task outcome example might look train x different evaluate performance rather tie hitting particular performance goal may may even possible given task hand 2 kind epic research implementation research one answer whether something even feasible would suggest deeming research feasible currently use offline evaluation completely ok research may checking whether could come good enough facebook comment sentiment analysis might jupyter notebook trying different approach library well pulling sample via fb api implementation epic would creating job store comment periodically database might done engineer implementing retraining pipeline scoring api great example thank",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "All the wrong things with predicting purchase, churn and similar targets in digital marketing.txt",
    "text": "source reddit datascience title wrong thing predicting purchase churn similar target digital marketing content recently reading lot common prediction task digital marketing like churn prediction predicting probability purchase user level article book read far understanding well several thing make task complicated different way pretty hard commit called type iii error build end answer real business question illustrated classical case churn prediction even though difficult build binary classifier clear end user prediction general useful uplift modeling creates another level complexity need conduct experiment first even collect necessary uplift modeling trying interpret result stakeholder another challenging part process one issue falsely obvious belief correlation implies causation graph get plot importance shap value really answer question like feature defines user behavior addition traditional metric used classification really useful often stated well known trade precision recall real situation especially distribution class heavily skewed personally encounter even business guy clear definition success want get insight usually try put way value auc score equal x overall measure classifier ranking ability top cumulative gain lift curve use understand useful opposed random classifier honestly speaking realize scientist answer question suggest something meaningful sometimes feel like end user idea really want end make everything super complicated specific target misconception recently found post frank harrell emphasizes idea real world case really need output probability informative general binary 0 1 answer appear make threshold related decision applies using improper scoring rule function selected threshold often taught machine course using accuracy evaluation metric might make sense iris dataset much less frequently real task currently working task expected predict probability subscription purchase problem easily boiled standard binary classification problem issue mentioned level come used marketing team simplistic naive idea pragmatic way around problem get decently calibrated classifier output probability technically speaking yet probability calibration hopefully something similar score sorted top x user based best value according cumulative gain plot selected form communication realize taking percentage user implicitly mean selecting threshold focus different even need report shamefully low value precision algorithm wise plain old gradient boosting xgboost lightgbm catboost feeling lot u encountered similar task field would highly appreciate advice discussion great resource discus task approach detail please mention well one freely available book like much comment perhaps unpopular opinion think part problem went straight ml approach experience business come request like one describe want insight causal factor fact final might less important thing experience almost always better start building heuristic based observation make e g identify pain point user churning point time action notification failed purchase crash build sort using observation good still missing process handcrafting often generates exactly insight business guy looking decent plus gone deep way building ml really quick easy alot say true generally found difficult part building figure use output effectively generally something science course focus modelling difficult using produce value tend use role use predicted probability try identify appropriate threshold targeting user instance something like predicting conversion would frame problem trying target user could persuaded convert rather target user likely convert logic user likely pay organically targeting something",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "All the wrong things with predicting purchase, churn and similar targets in digital marketing.txt",
    "text": "like sale would actually lose money since believe would willing pay anyway framing problem way clear tradeoff targeting user wirh higher predicted probability approach kind similar uplift modelling naturally introduces way stakeholder pick threshold e revenue cannibalisation tradeoff happy similar uplift modelling probably effective also metric never presented metric stakeholder understand care precision recall care revenue though ultimately true test whether using target user ab test provides uplift often taught machine course using accuracy evaluation metric might make sense iris dataset much less frequently real task concerned kind course seen point heavily hammered every course seen classification discussed score sorted top x user based best value according cumulative gain plot selected form communication realize taking percentage user implicitly mean selecting threshold seems backwards marketing team budget amount want spend targeting certain number customer cost per email number selected people budget cost per email number people extract use choose people likely respond choosing threshold marketing budget another approach would decile customer likelihood target top n decile approach tangible way measure impact baseline would randomly selecting x customer getting average conversion rate x customer build lift chart like one seen work topic find stakeholder find useful conducting shap analysis tell feature impacting churn purchase key metric actually using prediction make marketing easy definitely doable like assigning probability churn define various segment based low risk medium high etc program intervention order proactive around retention get experimental design causal analysis end predictive nice defining population interest noted comment churn prediction useless unless tied marketing action knowledge absent action interesting impact predicting churn easy helpful need predict churn reduced e g likely churn make intervention less likely churn people churn regardless already done leave alone effort wasted conversely people loyal churn happy service leave alone need know action marketing prepared take task predict respond action marketing target right offer group impact apply d social science field like marketing remember number usually inspiration point partial confirmation recommendation spoke much technical qualitative understanding market source research considered competitive effort well recording analyzing spot going result key question must always ask starting would strongly recommend reading first working analytics within marketing team forecast used benchmark compare actual number progress need okay way everyone understands forecast telling u actual performance act like guidance time read comment ops complete post think two link able propell forward uplift modelling jaroszewicz szymom article churn good best luck edit reading think op could benefit reading first link easy follow implement causal inference remindme one week remindme one week remindme one week remindme one week important get bottom business question start work know real business problem know really need optimise towards success measured know often feel like people making request suggestion know need job fully explore know least well need ask right question understand problem work digital advertising agency see time probing lot question often end different point initially thought wanted activated evaluated business unit metric campaign manager care brier score whether auc accuracy telling need something report boss try find way address directly",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "All the wrong things with predicting purchase, churn and similar targets in digital marketing.txt",
    "text": "build along software activates directly platform without needing go marketing long set matric check performance regular health check heavily consult internal platform expert manager well client marketing team still present score explain different concept focus marketing team much business impact rather metric job right checking make sure good need someone field evaluate whether good enough remindme one week honestly exact boat totally empathize situation sort predict proba report precision decile wise output instead whole way target certain set user run marketing campaign high precision unfortunately always work today struggling one able go beyond 40 precision top decile even ensemble catgbm lighgbm xgboost probing lot question often end different point initially thought wanted activated examined business unit metric provide digital medium marketing seo service pakistan focus grow business online digital s2dio great marketing agency focused maximize client profit feel free contact u 24 7 available service time highly recommend listening interview one topic touch upon prediction v actual decision making word predicting outcome acting usually long chain qualitative thinking act decision",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "Artificial life simulations.txt",
    "text": "source reddit biology title artificial life simulation content research machine drug sensitivity prediction drug work person cancer research basic level e nearly ready prime time one question thinking lately develop machine algorithm best class known e computationally generated biologically inspired datasets mean datasets derive boolean type expression basis describe biology drug work work gene mutated gene overexpressed pathway already shutdown etc could generate testing datasets coming random boolean phrase rather datasets closer biology sense system generates life like e product computational evolution simulation type google search artificial life chemical evolution simulation computation network motif hierarchical modular wondering anyone lead software program run result several specie related variation well almost cancer different genomic level hence respond drug ideally software evolves genome life contain modular hierarchical architecture real organism cell comment producer creatura probably quite far need working robust genome simulation system codon based syn nonsyn gene mutation vector genetic drift etc show kind output input expecting generate would happy implement expose api kind thing might useful pm send key wondering anyone lead software program run result several specie related variation well almost cancer different genomic level hence respond drug ideally software evolves genome life contain modular hierarchical architecture real organism cell think biogenesis color mod creates virtual life form fulfill requirement least idea could really useful thank check look like ton work thanks check",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "Data Learning Model Interesting Find [Cancer Reference].txt",
    "text": "source reddit biology title interesting find cancer reference content start saying know little biology kick as multi award winning scientist thingy think scientist term best describe dl developer bullshit let know better train good let know better train read article gentleman us logic analysis via largeset technicality ml way referenced article everyone wanted use word individual utilized known chemical composition biological effect develop principle could create drug wanted give try add touch make approximately 99 73 accuracy particularly impressive testing known known wanted share something really cool provided last night beautiful burning money h200s supposedly 89 6 expected composition removal weighting relates appears cancer see bottom article ref post disclaimer doctor biologist enjoy creating conducting driven machine project never made chemical smart enough make use chemical fault validated known known chemical composition one known known idgaf work want sell use bored also ever turn money exact match pubchem chembl scifinder current design specific hybrid benzimidazole phenazine glucose mimic tail doc composition found dl spoiler blocked make sure want see idk click black box c1 cc c c c1 n2c nc3 cc cc c3n2c4 cc c c c4 co occo c 2 4 1h benzo imidazol 2 yl phenyl amino 3 2 hydroxyethyl 5 hydroxymethyl phenazine 1 carboxylic acid c h n comment",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "Does anyone use systematic uncertainties anymore_.txt",
    "text": "source reddit datascience title anyone use systematic uncertainty anymore content come research background phd physic know come answering research question thing scientist spend time estimating systematic uncertainty transitioned private sector science scientific rigor used completely non existent seen mentioned propagating uncertainty number different occasion get blank stare d team well business stakeholder tell one ever uttered word thing never taught online course bootcamps countless d blog never touch concept give idea talking consider following scenario dataset quality 2 training label classification problem possibly wrong trained confusion metric compiled finally got people board listing statistical error thing like false positive rate seem convince people false positive rate never measured precision less 1 half time wrong label might right systematic label instance seen people use output report confidence score input large reported effect output greatly thus confidence score undoubtedly right thing propagate input output score recalibrate confidence score people punt stakeholder tell bad need anyone else experienced crazy old man yelling car anyone else think flooding science degree completely ignoring research based skill set used pertinent d edit know one mention case seen magnitude impact case mentioned serious business impact comment agree also feel like crazy person yelling car idea deal really propagate easily also regression ml mostly discriminative modeling x mean inherently conditional input feature x thus x need propagated even statistical point view already definition conditional assuming x take value generative joint p x would need consider x output label wrong know much time wrong label smoothing done account training beyond much ml analytical expression ci even everything perfect would use bootstrap cross validation bayesian ml special method like conformal prediction agree definitely much rigour compared research current project least attitude much build first mainly need show demonstrable output secure ongoing funding academic part really hate keep reminding research paper provide marginal gain business operation good enough sufficient also come phd physic yes one really get unfortunately need live business world though generally error going amount hill bean error everything else much bigger however show engineering cleaning might play role people listen especially plan mitigate otherwise shouting wind friend phd statistic spent 5 year national lab say e statistician really talk propagating uncertainty even though take uncertainty seriously generally trust much physicist meaning checking measuring wrong rather wrong error input expected approach modeling quietly incorporates folk limited background sure talking also folk lot background last time mentioned psych research method class reason know way dealing even come went grad school statistic benefit estimating systematic context work benefit e g reduced confidence score limitation tiring business leader rarely pat back pointing analysis crap think skill ignored incentivized stage research academia skill typically developed sound much like something learned research experience often represented d master bootcamp setting many u qualified seems like quality thinking done since undergrad think implement scenario thanks work group us science tool biotech application usually care lot uncertainty also visit lab discus experiment etc lot unreliable propagation e g bayesian statistic similar important field use concept agree experience",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "Does anyone use systematic uncertainties anymore_.txt",
    "text": "used say stuff time phd astrophysics fundamentally got fed job science title online course scikitlearn science longer scientist imho people take seriously honestly depends field domain year back worked e commerce marketing firm built target customer sending email send coupon even output bit okay stakeholder okay medical field completely different story removed loophole work maybe others called industry degree possible employee seek academic sponsor local university work real academic project along academic sponsor colleague degree already brings outside funding little academic rigor know common system interesting mentored anyone yet since phd company anyone want industry phd use imagine people putting much trust sometimes without evaluating well might crap say well test set must good thought good test set represent reality sound like good fit job ok hear scenario measure x ruler systematic bias bias magnitude upper limit train x deploy x measured different ruler systematic bias might something account error initial training set going answer deployed suck use separate systematically shifted sample give lower bound due expected systematic gathering initial training set argument text theoretical assumes training distribution good representation reality never changing good rarely case anything think practical sector even matter much arbitrary nonsense go business decision probably worth fixating thing like removed basically drift production one weakness discriminative x iid assumption independent identically distributed production shifted different distribution theoretical less sensitive problem drift empirical physical causal say derived diff eq physic way account training pretty much feed variety distribution simplest complicated use hierarchical fully generative joint otherwise guaranteed generalizable different population measurement error x already get complicated even simplest linear reg glms nonlinear parametric already general solution ml even analytical form top",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "Everything I've been doing is suddenly considered AI now.txt",
    "text": "source reddit datascience title everything suddenly considered content anyone else experience company pr website marketing say analytics d offering driven sudden machine method ols regression associated regression technique logistic regression neural net decision tree etc stuff around decade underpinning project front end solution considered senior management people sell buy realize larger datasets server power etc still personally care whether called one way another technically intelligence artificial basic calculator view find funny everything comment ride wave make sure saving crash imo greatest achievement company convincing others everything product year ago big still much also time technical definition pretty broad remember statistician everything considered science sudden 5 year everything called quantum like teenager sex everyone brag many actually pre 1994 called statistic 2004 called big analytics 2014 called machine 2024 called call whatever buzz word needed get venture capital dollar baby thing considered invented finite state machine prolog stop ml longer state art called 15 year ago iz d back reason different back like magic looked hood realized math magic gone call pr reason say company division today line trend like said d division 8 year ago removed marketing term science science marketing term statistic riding wave considered back still hype marketed way nothing special thing honesty think quite worrisome llm bubble going burst somewhere future possibly giving bad name good science considered could well job line well seems like course basically everywhere anything automatically detects something reacts robo customer service phone system image search clothes dryer detects laundry dry thermostat house seen people go change slide deck word lol deleted write receiving 6 digit paycheck get advantage hype leave semantics type nerd lol latest buzzword used people realize cloud new concept someone else computer executive love two thing buzzword obscurantism combination produce awe people see fluff claim use technically incorrect whole point exaggerate advanced organization capability many supervised unsupervised ml technique standard statistical analysis technique long time user claiming label caught industry",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "Explainable AI Scepticism.txt",
    "text": "source reddit datascience title explainable scepticism content context work regulated industry interpretability large emphasis business regulator use lot linear like ols logistic regression gam account non linear relationship recently science leadership pushing u explore machine see large predictive gain surprisingly xgboosts random forest among others show small increase predictive accuracy compared linear spend fair amount time fine tuning linear however still need show understand making prediction come opinion explainable technique poor job explaining anything meaningful thing like shap value lime okay instance stable seen often show bizarre relationship instance two observation theoretically close generating process close different space addition local interpretation technique really fail show anything globally blog post summarizes thought clearly anyways guess asking practicioners hold different view advancement space unaware know lot effort going explainable space right pessimistic even possible u good explaination many thought comment skeptic aka black box skeptic seen many people make fundamental mistake using explainable method know unexplained method making mistake biggest leakage way would trust black box method experimentation actually perform real world performs better comfortable assuming work looked something like interpretml explainable boosting seem address weirdness see applying local shap value global however still need show understand making prediction come opinion explainable technique poor job explaining anything meaningful linear inherently explainable factor thing like shap value lime okay instance stable seen often show bizarre relationship instance two observation theoretically close generating process close different space sorry misunderstood point feel like enforcing relationship figure relationship 1 thing often see misunderstood process explaining implicit presumption simple relationship confusing explainable linear increase b increase another aspect differentiate asking versus question could technically unanswerable least unsatisfactory end satisfactory either way process using result technique divorced deeper understanding feature kinda vague know addition local interpretation technique really fail show anything globally yes issue random aspect shap example could already unable run shap entire anyway start venturing semi large datasets large number feature ten feature interpretability joke part none even marginally interpretable interpretability thing set technique use trick client thinking understand know full well faintest fucking clue none u idea thing really understand tiny component really understand train whole advice make life lot easier pretend buy interpretability thing wanna point beef lot people explainable beef abuse terminology context people ingesting result interpretable tend technical across industry practitioner non practitioner find working reading interpreting output often conflation make prediction make prediction value see come anywhere care notion causality broadly care multiple marginal effect assuming design even allows hard practice like drug development especially stage 3 4 really want thing minimize patient risk approach address question fantastic ease use prediction sometimes package among others us cool theory explain come prediction good predictive one less likely one generates need causal double ml although term seems abused currently hot topic research situation use context thing like tmle use case narrow could use version lasso also statistician looking decade much smarter people talking pro con practical thing linear even treatment effect strictly linear still useful interpretable fair really throw ols problem find empty",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "Explainable AI Scepticism.txt",
    "text": "toolbox glms super flexible easy implement sort nonlinearity still natural human understand motivate causal standpoint apply domain knowledge thanks thoughtful response linear inherently explainable factor expand mean think get saying entirely sure guess explainable somewhat fuzzy term sorry misunderstood point feel like enforcing relationship figure relationship 1 thing often see misunderstood process explaining implicit presumption simple relationship confusing explainable linear increase b increase think wrong preconceived notion relationship either prior building theory suggests understand business might know exactly usually idea example build electricity demand forecasting find negative relationship demand temperature summer month sure wrong suspect finding see point case might idea relationship another aspect differentiate asking versus question could technically unanswerable least unsatisfactory end satisfactory either way process using result technique divorced deeper understanding feature kinda vague know fully agreed think explainable standpoint much less interesting important explaining ols easy explaining decision tree terribly interesting much important think map back understanding feature",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "fa186a76e61a477600808fb7873f181e.txt",
    "text": "08 app vizuara vizuara 21 912 3 scientific ml everyone people trying learn machine step 1 watch tutorial step 2 build titanic survivor predictor step 3 add ml expert linkedin bio uncomfortable truth toy kaggle project take far especially engineer scientist economist trying apply ml actual field mnist digit recognition help mechanical engineer working heat transfer movie recommendation connect structural engineer designing building gap clear ml expert domain expert people live intersection scientific machine sciml come sciml throwing black box blending mathematical algorithm making work real world physic finance biology climate make sciml powerful us existing knowledge ode pdes control system leverage neural network universal function approximators keep interpretable efficient domain relevant need c degree core mechanical engineer take formal c course yet work sciml every day ml democratized sciml proof want stop playing toy problem start working impactful ml research rooted domain path let stop pretending titanic prediction enough let build ml actually matter interested sciml research check sciml bootcamp designed student engineer researcher want publish serious high quality work share others think useful follow daily ml update",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "How alive is traditional machine learning in academia_.txt",
    "text": "source reddit datascience title alive traditional machine academia content still room research technique commonly used industry currently work scientist considering pursuing master ph machine however appears recent development focus primarily neural network especially large language llm despite extensively searching arxiv article little success finding research area like feature engineering probability tree based algorithm anyone know professor specializing traditional machine aspect please let know comment maybe arxiv best place look statistic thing like looking field journal try looking statistic faculty page university like often short list research interest available find lot looking deleted tried searching decision tree feature engineering archive org using google instantly found dozen recent paper 2020 2023 publication date sure mean probability separate field multiple sub topic need specific search probability iisc bangalore csa cd department lab focus classical ml india best research country offer reason quickly think 1 attention devoted explaining inferring finding causal mechanism observe hardly strength ml rather traditional whose structure understand pure prediction limited scientific value many application 2 ml shine large amount however many scientific task come large dataset era surrounded day day life enough may sound like foreign idea quite common actually ml opposite parsimonious fact writing reading paper highlighting much ml requires achieve accuracy conventional survival 3 skepticism scientist rightfully skeptical spectacular failure llm certainly help hallucination sensible current implementation llm leading witness 4 said forget foundation ml come almost exclusively academia many area see use sophisticated familiar bioinformatics biostatistics econometrics majority research done engineering life science shifted ml application mean new article bart tree based week arxiv stats suggests search term good good querying feature engineering would use surogate black box something interesting dont look c department look department like biology psych sociology etc thing citing already well stablished example common study undergrad student research ia purpose push field boundary hence people researching ia try develop new thing push thing ahead solve unsolved problem example development llm still active field research using method common use think calling field science made problem science industry call d part toolkit lot researcher specially statistic involved give one example using search algorithm improved reinforcement solve pdes even give new answer np hard problem grosso modo would choose want work research ia using method research thanks sharing classical approach still much used classical science msc engineering focus ml experience researcher engineering understand deep honestly think advisor belief deep barely surpassed simple concept multilayer perception belief deep gimmick want involve lab feel place research think lot traditional scientist feel way yet meet one seems understand state modern deep importance know much work happening c field end advancing classical approach though others said probably looking wrong place arxiv place discussing cutting edge modern computing want read modern work classical approach check journal mathematic statistic engineering maybe natural science good tip found promissing option thanks seems looking wrong place good journal refer could reason finding one week necessarily mean research area hot explainable ml cool topic understand point people mentioned still new branch study apart well stablished one",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "How alive is traditional machine learning in academia_.txt",
    "text": "one problem method science science science largely make prediction often tell much advance scientific understanding alphafold2 really told u much protein folding example obviously still useful science dude never heard alphafold",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "I need career advice (CV_ML roles).txt",
    "text": "source reddit computervision title need career advice cv ml role content hi everyone currently working autonomous driving domain perception mapping software engineer work well known large company current team involved production level development limit growth hand opportunity long term goal transition computer vision machine role big tech company ideally applied cv ml area like 3d scene understanding general perception however noticed big tech firm seem fewer applied cv ml position compared startup especially focused deployment rather architecture experience deploying optimizing perception improving inference speed handling integration robotics stack implementing existing however spent much time designing modifying architecture understanding deep fundamental relatively shallow planning start personal project summer bridge gap like get feedback professional realistic aim applied cv ml role big tech background would recommend focusing open source contribution personal research something else better path joining strong startup team applying big tech thanks advance advice comment feel like normally requires phd especially want work big tech company fact master good give chance feel like still insanely competitive difficult yeah many position ask publication top conference journal best luck startup mid size company care less lack phd paper value experience deployment integration performance optimization thing researcher usually pretty awful big faang company overrated working applied perception already got many people would consider dream job feel growth need met mean job hop might org problem skill set publication want experience customizing dl fundamental time pace imho big tech job mean give opportunity cvpr iccv eccv publication work multiple ml area thing deployment backend frontend spend majority time developing training tool developer like lot thing knowing whole pipeline said would caution working big tech working multiple area many different thing even know develop research novel algorithm put production may get stuck small part pipeline enjoy depends team course really enjoy mid small size company feel grow solve problem base salary comparable obviously stock option big tech insane said really want live big tech large company life money fading reputation big tech nerd go worked ex big tech devs many average fun work large scale project fulfilling career outside big tech many time learn may also find easier get cv position small mid size company getting big tech take bit luck good interview skill good interviewer leetcode grinding yeah think strengthening skill thing focus generally used method graduating omscs semester enough time work personal project answer three question probably already qualified join applied cv ml perception team faang want start interviewing waste time someone else open source project unless passionate would anyway nothing clout personal project also personal project get make decision move fast like want apply big tech barrier entry high applying research team may rank phd publication record applying product team care master plus work experience ample comment really helped organize future plan consider personal project others parallel interviewing thank much unfortunately graduated mechanical engineering b currently taking omscs georgia tech well never successful getting one afraid give good advice almost anyone knew managed get company insane resume know necessarily needed get accepted",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "I need career advice (CV_ML roles).txt",
    "text": "also master pretty well regarded school carnegie mellon simply likely run individual damn b mech master robotics trying focus cv u land role u see make sense try make insane resume see lucky enough get self study robotics topic able pas project based coding interview",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "New Grad Data Scientist feeling overwhelmed and disillusioned at first job.txt",
    "text": "source reddit datascience title new grad scientist feeling overwhelmed disillusioned first job content hi recently graduated degree science started first job scientist company focused staying ahead keeping hype train want team scientist except explore deploying agent specific use case issue background academic internship traditional machine regression classification basic nlp etc agentic llm based system project briefed nothing past experience solely concerned infuse workflow within product feeling depth worried expectation placed early career wondering anyone advice quickly get speed newer technique like agentic approach situation overall resource mindset tip career advice would greatly appreciated comment read recruiter take job 60 qualified 100 bored learn nothing know situation lot time people get head job expects perfection 50 would suffice problem run know possible always try meet thinking everyone else know word give time take opportunity get paid learn something new instead pay take course lol company hiring new grad scientist going need agentic agentic funny god want shit burst bad even bad career honest schedule discovery meeting smth sound super vague drill exactly want want use buzz word actually want build smth value fake till make bud listen carefully learn lot best know able walk walk api call buddy got long term company strategy dumb af thats conversation another day worry feeling depth long project relatively reasonably scoped agentic think big thing trying push back hammer looking nail mentality focus project truly highest impact value add agentic also reasonably feasible resource article newsletter thought really interesting current state agentic workflow find good medium substacks etc relatively easy keep finger pulse context engineering primer rag tldr tldr also really good general science engineering article problem u rn constant technology need upskill continually changing tool method rely past knowledge build rely ability learn actually good thing working thing solved yet ahead crowd thing studied give best least job lot u slowly spiraling existential crisis saving dwindles dust wondering got degree first place one thing realize everything tool necessarily need machine solve problem company project work prep analysis ml whatever follows figure problem workflow product optimized first apply whatever solution work best llm great know solve also great know solve great time learn possible try difficult may think tech evolves constantly job part new grad one expects know everything usually start youtube get basic dive task learn go feel overwhelming first starting anywhere better overthinking thing start click along way good luck normal happens people enter field welcome club two skill teach d school program 1 adabtability flexibility 2 deliverabilty e able deliver solution regardless challenge emphasis software engineering yet main skill industry look worry able learn practice vast amount material allocate time work use chatgpt purpose degree give tool method set able yo learn new emerging technique foundation going anywhere ignore bubble burst nonsense reddit job seems teach highly valuable skill go hate say gpt 4o probably better running new grad job going forward integrating workflow managing integration 2017 like science dead yeah literally nobody experience using llm right welcome cutting edge dude sound",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "New Grad Data Scientist feeling overwhelmed and disillusioned at first job.txt",
    "text": "fun hell d work ab testing measurement reporting etc fun awesome empowering skill set agentic pioneer",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "New Online Course from MIT_ Probability - The Science of Uncertainty and Data.txt",
    "text": "source reddit datascience title new online course mit probability science content world full accident storm unruly financial market noisy communication world also full probabilistic modeling related field statistical inference key analyzing making scientifically sound prediction build foundational knowledge science introduction probabilistic including random process basic element statistical inference course free try comment new couple instance course already order able experiment full course 300 dollar course material free access learner choose upgrade access graded assignment obtain certificate completion yeah cool thank much edx social meant experiment full course whay new mean new instance new material previous instance",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "Practical bayesian methods MOOC.txt",
    "text": "source reddit machinelearning title practical bayesian method mooc content hi everyone continuing develop coursera mooc practical bayesian method came prototype schedule wanted discus community think content order missing etc see detailed plan course going part advanced machine specialization take 6 week short version schedule 1 intro statistic review example conjugate distribution 2 em latent variable gmm probabilistic pca 3 mcmc 4 variational inference lda 5 stochastic variational inference vae bayesian neural network 6 gaussian process hyperparameter optimization summary one homework sure paper reading assignment since one course goal train listener well enough read paper bayesian may make homework along line read one paper summarize get feedback peer review another important decision final project one idea build self driving car training cnn map image camera simulated game signal wheel part simple non probabilistic probabilistic part combine signal different source work different condition e g usual camera work poorly night infrared camera work poorly raining estimate prediction car pas control human driver sure guy think especially concerned homework see detail gdoc comment first really thanks already one person signed course able follow individual course without going course specialization already background ml practicioner bayesian method outside reach far content exactly looking ask one thing search right balance breadth subject depth material taught example really need talk stochastic computational graph maybe would make sense go deeper pun intended probabilistic pca vae apart week 1 3 look great something might interesting either exercise small topic bayesian matrix factorization recommendation popular application area nice exercise derive math implement result 1 bayesian probabilistic matrix factorization using markov chain monte carlo salakhutdinov mnih icml 2008 2 scalable recommendation hierarchical poisson factorization gopalan hoffman blei uai 2015 using variational inference thanks lot feedback yes able follow one course specialization least told actually thinking mention stochastic comp graph short 5 minute video say trick shown reinforce reparametrization trick etc put kind general framework handle lot practical situation yes breadth depth tradeoff something would really like get right thanks feedback eta course past experience moocs ml noticed really admired programming assignment actually made implement debug algorithm take example andrew ng course ml hit sweet spot getting hand dirty actually making something work personally see value course told u algorithm run report result sorry asked promise date publicly answered private message would think programming template provided way implement relevant line algorithm better python think template great educational reason take away extra complexity come real programming allowing write code actually see work without many frustration honest opinion best implementation framework seen andrew ng ml course fill line code actually calculation loading plotting result already taken care also found important existence sort unit test basically provided u test case correct result given debug algorithm actually working intended",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "Reproducibility standards for machine learning in the life sciences.txt",
    "text": "source reddit bioinformatics title reproducibility standard machine life science content comment researcher increasingly use ml incredible reproducible almost funny many researcher release code generally full hard coded path code useless reproducibility however least inspected see terrible mistake analysis made bronze standard code available make plenty sense thing value silver standard dependency downloaded single command often unobtainable life science example hospital policy may allow public gold standard end end one click seems add little value use learn appropiate analysis done right way e version control deposition geo sra etc bronze difficult obtain see reasonable target fully trained bioinformatician hit think bioinformatician realistically attain silver gold standard project need financial incentive similar grant software lifecycle popular bioinformatics tool fact really exist seems agree assessment jump bronze gold add little value point feel pie sky",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "Should I attempt to do machine learning project without business need.txt",
    "text": "source reddit datascience title attempt machine project without business need content incompetent science manager organization low science expertise hired many scientist promoted numerous database engineer analyst science role management want hear machine save money refuse consider challenge involved fraud counterfeit problem directive receive hey scientist expert know machine used fraud detection save u x million dollar using machine however investigate closely find many discrepancy commonality type fraud organization type fraud machine successfully used word fraud manager either understand want understand happening close three year even joined company large organization scattered everywhere many quality issue also receive many requirement business used receiving business requirement working business determine machine use case proceeding justifying step however manager want u randomly come use case go talk stakeholder ever work large organization time even get reply busy even difficult determine worth working without involving business beginning business going ultimate entity implement solution without getting buy upfront experience navigating kind situation context size org 100 000 employee comment sound like win situation looking another job mean easy make another chatgpt house yeah large company leader told meeting alone manager tell pretty typical situation start progressing past junior stage around 3 5 year experience manager d general would say 1 junior unusual 2 already level find part job add value company thought leadership simply going case every initiative get clear requirement business mainly expert field like would even know ml opportunity even look like 3 generally speaking level d still pursue something concretely make sense add value business make reservation known disagree anyways give 100 industry leadership last word 4 general project buy spend lot time clear path market talk team thats case goal help 5 crucially d project like need 1 2 step ahead business mean everything fully specced could foundational 6 lastly remember part business d role bit unusual want succeed add value always asked ex imo core problem solver mean general sometimes necessary identify opportunity get whether others see yet balance v fact work company succeed long term without others good luck typical response someone asks build ml ml wrong solution build ensemble us ml plus whatever actually appropriate set rule ensemble favour know actually work totally ridiculous waste time usually delivers keep everyone happy management brag machine solution operation get something actually delivers useful response guy saying look another job sound exactly like bank worked past condition difficult ever truly impactful work instead mainly working pocs business care science never work without proper leadership beyond capacity scientist level two option become extremely adept company politics ride gravy train leave organisation think probably need business buy especially old school organization might less interested employee going rogue even end working providing value experience need least trust able pursue thing upper management may see passion project regardless whether end developing something beneficial organization hard sell lot orgs specific definition want developed want impact current infrastructure sure feasible experience stakeholder love love love complain guy get mouthful sometimes hear something enough solvable thing complain use case figure",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "Should I attempt to do machine learning project without business need.txt",
    "text": "one meeting one interaction sort hang least meet official unofficial way unscientific know eon ago moved division analytics guy first week vp threatening get fired fix whatever dug listened judgement soon got idea could prove worth solving problem perceived problem worked began come solve address much cordial began include meeting way even metaphorically sit stakeholder get know get know best way help mean 100 automation 100 success yeah hard even convey message clueless fraud protection make good ranking likley case fraud essence people get flagged rank high enough human must look done correctly reduces human work save money keeping fraud constant best case go better invent new type machine quick able get access need understand generated gotchas could free independently work promising solution pitch bos free connect people 1 3 level business lead get context need ensure modeling solves real business problem safe politics credit drive solution answer question yes would go ahead solving problem ideally confidante business side help navigate politics walking situation putting risk manager figure involved problem going blamed cost overrun failure need stop full walkthrough decision maker get blamed failure get blamed dirty number 000 thing go wrong amex join issue stem dysfunctional leadership fundamental misunderstanding science role business strategy expecting scientist independently find impactful use case without clear business requirement strategic direction ineffective especially large organization stakeholder engagement crucial approach might work smaller setting direct communication large company like lead wasted effort frustration without strategic approach alignment business need meaningful outcome unlikely would much working ultimately would looking another job seems situation need hunt good use case common large company start understanding company well company produce key metric key decision parameter involved decision improve ml much would impact metric make decision partner people level org young individual contributor eager better give example company make something hold inventory lead time prediction accurate would better inventory target forecast another common one situation would change company run away genius scientist cannot solve problem without business input curious type fraud organization mentioned counterfeit aware ml us counterfeit detection requires certain quality preparation may lacking organization would org focus capturing right anyhow worked incompetent management trying push ml problem solved simple sql query push back able explain solution work using reason mathematical proof scientist also provide alternative manager insists ml also able justify evidence example explain position weak perhaps incompetent scientist explain keep pushing back leave note argument fraud place ml applied type fraud justification mean scientist copy science code scientist solved problem float idea show challenge maybe work get done maybe hald baked solution kinda matter act interested see sway stakeholder sway direction least bullshit minimal work industry often add new business requirement related machine actual demand big relevant official project work try following 1 visit sub get sense latest trend science try answer question much possible 2 check towardsdatascience 3 buy latest related book read try write code book 4 participate kaggle competition communicate team 5 try writing blog share know get feedback hope give reference answering title",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "Should I attempt to do machine learning project without business need.txt",
    "text": "question absolutely taking use case stakeholder fraught danger likely happen really understand shrug say sure go ahead large amount work give final product never use",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "Simulating a single human neuron..txt",
    "text": "source reddit biology title simulating single human neuron content ibm developed neurosynaptic core chip facilitate building machine neural net response darpa challenge develop biological scale neural network ibm feat 500 billion neuron 100 trillion synapsis darpa target achieve neuron 14 synapsis human brain understand 11 neuron 15 synapsis one main question much computing one real human neuron wanted simulate real human much information need encode software term simulate single neuron would respond signal across thousand synapsis neuron would real brain moreover different kind signal might come single synapse signal variety enumerated way yet really asking smart single brain cell complex task even simulate behavior versatility one comment read r neuralnetworks u d1zz1 reference two article first one point numerous dimension complexity way signal processed half dozen chemical neurotransmitter activate different circumstance modulate signal processed across synapse also hint synapse strength synapsis evolve time general question neurotransmitter tend trigger locally single synapse time scale individual electrical signal brain wide region wide expression long time period like second even minute time human brain miraculous god great",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "Stats vs ML Pedagogy.txt",
    "text": "source reddit datascience title stats v ml pedagogy content enjoy auditing university course science topic least experience stats course tend explain even prove theoretical property different method e g estimator consistent asymptotically normal hand machine course see tend focus intuition implementation mechanic get bit hand wavy come justifying approach e g ensemble balance leading better predictive performance observed difference thought occurs comment difference based take class professor d class start explaining go behind basically never advance unless class full phd student grad class full time master pretty hard get hazard guess difference goal statistic tends much concerned rigorous emphasis causality machine tends much concerned usefulness predictive accuracy concern looking black box seems work machine embrace black box adopt extremely complicated statistician shun black box tend simplify point explain think applied computational statistic ensemble great work even better properly selected see lot divide bad really good scientist need walk line hung everything correctly work suffer boring likewise think understanding matter modeling well could responsibility issue quick answer people ask stats akin top ml bottom use math one proof work one practical theoretical want extract table map image depends stats course stats major gladfully graded differntly great feedback two big theme emerge think u single vacation427 probably best explains difference observed indeed ml class tend undergrad c course audience probably include many strong math background course understandably tend toward application said think u key addition1818 launched interesting thread miss accessible paper u pacific plywood u iamevpo highlight although black box discussion help appreciate philosophical difference traditional stats ml community e strength assumption structure underlying distribution new school still clearly power method mathematical theory example averaging see loss function yes feel like right thing importantly law large number tell u average converge true expected loss let know seen class really explores foundation guy need minimum karma post sub reddit want make post please upvote post thanks guy id say opposite student feel computer science student dont requisite math background find strange engineering student get taught relevant math without getting bogged proof nice way explaining statistic econometrics solves inference problem finding law generating process sample observarions machine solves task generalisation based predict outcome new arrives could add may also different department teaching course stats c best answer ever read perfect answer economist turned scientist cannot stress enough well description capture subtle difference new legislation focus explainability think black box approach long world interesting point ml scientist tend fear bias statistician fear variance adding summary class two article athey imbens summary article machine method economist know recently saw someone recommended statistic causal inference paul holland actually basic thats needed linear algebra calculus probability theory statistic understand ml see also two culture statistic true statistician like low variance however believe statement quite incorrect statistic often concerned obtaining unbiased estimate parameter contrast machine technique often introduce bias order decrease variance minimize mse squared bias variance order obtain better prediction therefore accurate say statistician fear bias ml scientist optimize predictive performance weird would said opposite",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "Struggled with the math behind convolution, backprop, and loss functions â€” found a resource that helped.txt",
    "text": "source reddit computervision title struggled math behind convolution backprop loss function found resource helped content working ml cv bit always felt like relying intuition tutorial came math especially gradient really work convolution layer backprop update jacobians multivariable calculus actually matter matrix decomposition like svd show computer vision task recently worked book project called mathematics machine tivadar danka written people like want deeply understand math without needing phd start scratch linear algebra calculus probability walk way concept power real ml including kind used vision system helped bunch reader make sense math behind code curious anyone else go resource helped bridge gap happy share free math primer made alongside book anyone interested comment course obscure book one know published publisher op work lmao record newbie get away freely available d2l also recommend intro deep course cmu taught prof bhiksa raj whose slide freely available online course actually tell get gradient update cnns generally quite mathematically rigorous think even find lecture video youtube highly recommend book folk want understand deep without getting nitty gritty math detail also need know math work ml need understand math parameter change behavior almost one coming new gradient descent algorithm whatever guide convolution arithmetic deep paper explain conv using la understand gradient work regular nn could help understand cnn gracias",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "The Mathematics of Machine Learning.txt",
    "text": "source reddit machinelearning title mathematics machine content akinfaderin adewale last month several people contact enthusiasm venturing world science using machine ml technique probe statistical regularity build impeccable driven product however observed actually lack necessary mathematical intuition framework get useful result main reason decided write blog post recently upsurge availability many easy use machine deep package sick learn weka tensorflow etc machine theory field intersects statistical probabilistic computer science algorithmic aspect arising iteratively finding hidden insight used build intelligent application despite immense possibility machine deep thorough mathematical understanding many technique necessary good grasp inner working algorithm getting good result worry math many reason mathematics machine important highlight 1 selecting right algorithm includes giving consideration accuracy training time complexity number parameter number feature 2 choosing parameter setting validation strategy 3 identifying underfitting overfitting understanding bias variance tradeoff 4 estimating right confidence interval level math need main question trying understand interdisciplinary field machine amount math necessary level math needed understand technique answer question multidimensional depends level interest individual research mathematical formulation theoretical advancement machine ongoing researcher working advanced technique state believe minimum level mathematics needed machine scientist engineer importance mathematical concept 1 linear algebra someone recently said linear algebra mathematics 21st century totally agree statement ml linear algebra come everywhere topic principal component analysis pca singular value decomposition svd eigendecomposition matrix lu decomposition qr decomposition factorization symmetric matrix orthogonalization orthonormalization matrix operation projection eigenvalue eigenvectors vector space norm needed understanding optimization method used machine amazing thing linear algebra many online resource always said traditional classroom dying vast amount resource available internet favorite linear algebra course one offered mit courseware prof gilbert strang 2 probability theory statistic machine statistic different field actually someone recently defined machine statistic mac fundamental statistical probability theory needed ml combinatorics probability rule axiom bayes theorem random variable variance expectation conditional joint distribution standard distribution bernoulli binomial multinomial uniform gaussian moment generating function maximum likelihood estimation mle prior posterior maximum posteriori estimation map sampling method 3 multivariate calculus necessary topic include differential integral calculus partial derivative vector value function directional gradient hessian jacobian laplacian lagragian distribution 4 algorithm complex optimization important understanding computational efficiency scalability machine algorithm exploiting sparsity datasets knowledge structure binary tree hashing heap stack etc dynamic programming randomized sublinear algorithm graph gradient stochastic descent primal dual method needed 5 others comprises math topic covered four major area described include real complex analysis set sequence topology metric space single valued continuous function limit information theory entropy information gain function space manifold online moocs material studying mathematics topic needed machine khan academy linear algebra probability statistic multivariable calculus optimization coding matrix linear algebra computer science application philip klein brown university linear algebra foundation frontier robert van de geijn university texas application linear algebra part 1 part 2 newer course tim chartier davidson college joseph blitzstein harvard stat 110 lecture larry wasserman book statistic concise course statistical inference boyd vandenberghe course convex optimization stanford linear algebra foundation frontier edx udacity introduction statistic finally main aim blog post give well intentioned",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "The Mathematics of Machine Learning.txt",
    "text": "advice importance mathematics machine necessary topic useful resource mastery topic however machine enthusiast novice math probably find post disheartening seriously aim beginner need lot mathematics start machine fundamental prerequisite analysis described blog post learn math go master technique algorithm know machine please subscribe newsletter www muoro io comment mathematical background clearly helpful disagree statement need math understand machine algorithm work algorithm scale quadratically need mathematics practice package available lot done experiment three four example say necessary 1 selecting right algorithm includes giving consideration accuracy training time complexity number parameter number feature even solid math background come crossvalidation intuition helped little bit mathematics algorithm usually work better found lot article 1 choosing parameter setting validation strategy theory validation strategy logic understanding mean know exponent help bit 1 identifying underfitting overfitting understanding bias variance tradeoff compare train validation score read article bias variance tradeoff 1 estimating right confidence interval concede proper statistical background help think mathematics help lot intuition behind thing get faster far necessary topic edit realized bit negative meant nice write part mathematics help necessity bit exaggerated opinion good coverage fundamental lol sick learn scikit learn add gilbert strang linear algebra course mit opencourseware best lin alg course seen apology already listed see looked list realize sound like trying way looking get ml want know math need quite understandable question best answer think math important think directed think important b less important think directed think tremendous amount math needed get started apropos experience true many people said thread similar one lot one time ml taken obviously non math activity maybe even 90 time could look fact say well go math needed think premature conclusion munging setting analysis pipeline parameter tuning etc look like gruntwork nothing math activity certain way know underlying algorithm working word rote work done service delivering high quality result using algorithm ensemble algorithm method understand understand transformed make sense push certain pipeline justification boil good understanding method involved rest among thing mathematical knowledge word knowledge theoretical basis algorithm able explain least chance able explain method yield certain result also rest largely knowing math underlies method aprorpos b others commented lot ml involves much math engineering able deliver quality result quickly mathematician part would love say math super important got know think statement really justified much guidance implement ml algorithm implement naively algorithm probably terribly slow maybe even unusable math alone take far one piece puzzle knowledge skill part computer science play arguably bigger role pure math focused largely pure math algorithm might appreciate need adapt method even chance working real world able program thing effectively well structured way team likely take much deep knowledge math behind programming resolve conflict b keeping mind really depends want ml looking get research side thing certainly need broad base mathematical knowledge looking tweak existing algorithm combine new way knowledge mathematical basis method work favor mostly interested building thing using standard ml algorithm probably need much math basic take far nothing wrong either approach need kind people honestly one become talented following",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "The Mathematics of Machine Learning.txt",
    "text": "functional analysis advanced probability statistic non linear optimization graph theory information theory complexity analysis understanding large portion machine breeze thank putting together someone interest machine already applying discrete setting job however lack math background really stuck particularly specific domain outlined approach math seriously enough university course university jmu time made weird decision neglect rigorous math course computer science degree plus 16 year since graduated skill pick largely atrophied spend hour couple day pick basic toolkits bit intimidated fact likely study full time full year pick lacking math skill correctly outlined great outlined good free resource enormous time commitment definitely acknowledge point made u dzyl least somewhat functional work despite limitation found like random forest classifier tend work reasonably well work box however stuff work well loss troubleshoot change approach characterize underlying well understand pca really work hyperparameter tuning selection without applying caveman approach trying everything hoping work hoping job completes reasonable timeframe others situation pick skill manage time commitment required would subscribe newsletter link seems broken would like point upcoming probability course computational probability inference according information given course website cover advanced concept probability machine probabilistic graphical thanks article novice appreciate dump math topic become comfortable coursera ml course good balance thought algorithm almost always presented reasonable detail andrew ng never really let slow intimidate newbie finishing course know want delve math master time pulled direction towards actually implementing thing scikit learn nltk finally reckon final paragraph reformatted list readable edit unrelated little trivial think bison io logo could use work trying much relevant practical source found linear algebra online check mit one like multiple perspective took applied linear algebra uni prof told wanted anything besides reciting proof taken applied linear algebra instead applied linear algebra reminded class sylabus signed actually applied linear algebra said oh well teaching looking forward trying something constructive deleted understand area ok try every possibe algorithm without understand trillion differents parameter generate two random string one diagnose disease another prescribe drug patient cured everyone say terrible doctor day patient came whit terrible ah 479fgf disease algorithm prescribed flu meddicine cured burned chemistry medicine book completely agree essential understand concept necessary get machine agree end depends whether want research ml kind research want think disregarding oftentimes particular algorithm fit need exactly understanding underlying math give flexibility tweak tune algorithm methodology fit problem much achieve ton without intricate understanding underlying math definitely need math need able understand logic behind mle example logic behind shrinkage need go math every time create digging deep math research people something mechanic engineer although quite accurate case sum general notion machine without math like physic without calculus possible probably taken class high school first year tried painful remember trying understand centrifugal force mathematics necessary strictly speaking pain free route goal think thing actually important mathematical expertise ml kind formal correctness decision process ability consider suite mathematical object method comfortable tool achieve one goal say environment making new thing every tool way thinking turn towards trying example useful able think need understand variable",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "The Mathematics of Machine Learning.txt",
    "text": "influence eachother okay gradient okay something blowing let normalize need network decide parameter image transform let try transform 2x2 matrix need layer 4 output correctness verified fact part less concerned comfortable using available tool going determine creative flexible changing need circumstance probably autocorrect gilbert strang linear algebra course mit opencourseware link lazy",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "What questions do data-literate professionals ask_.txt",
    "text": "source reddit datascience title question literate professional ask content e g come collected reliable noisy e g measurement error missingness etc larger population inferring sample inherent trade offs e g bias v variance method used interpret level given prediction e g standard error confidence interval etc know inference wrong finding replicated programming script another scientist could attempt replicate result comment agree u divisible zero everything domain knowledge process knowledge exception process infer etc",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "[D] Current ML Engineer Searching for Career Advice.txt",
    "text": "source reddit machinelearning title current ml engineer searching career advice content hi recent grad 1 5 year experience machine engineer except actual duty role even close ml engineer role found involve much machine even though job description manager description implied also placed position become job perform physic simulation due experience gained physic master degree small amount machine part involve training deploying instead usual practice company adopted build pip installable python package command line interface deploy code installing customer computer first job grad school jumped opportunity initial regret treating experience definitely aware position advertised within week starting position first job pandemic brought decided ride recently decided time move another position actually offer career development future machine engineer role except quite bit difficulty finding another position past year working weekend update portfolio display current coding machine capability definitely go beyond expected job quite interview including technical interview amazon think resume lacking feel able put positive spin small amount machine done work also trying highlight extra time put make desirable candidate however feel like keep falling short interviewer ask deploy current position done small amount work portfolio using restful apis show ability job currently asks feel like hitting wall sure focus effort help situation would appreciate advice could considered useful even sanity check know others position moved role applicable responsibility tl dr first job grad school actually considered ml engineer role struggling interview experience gained applicable position fit current job title seeking advice become real ml engineer comment point get experience deploying production ready find job actually set faang job interview interesting startup scientist rather engineer get valuable experience learn thing deployment pipeline job long contribute way work often interesting obviously pay less resume faang tag skill gain make also make sure get much detail role company role tie want end another stop gap role limited chance growth still take look link user provided gain least theoretical system knowledge needed need learn engineering ml folk there spectrum analysis design people straight academia often 100 analysis really design reusable infraestructure need even 50 50 standalone ml knowledge useless unless targeting niche position need phd anyway learn engineering stack recommendation docker kubernetes sql big query apache airflow fastapi panda databricks apache beam apache pulsar sound like technical skill gap issue interview tactic office using ancient dark age method feel truly show initiative build something modern little micro service lambda function docker service whatever match desired job requirement build way like even partially ask process say well nature business problem trying solve would love build using stuff want know think make sense could even put front end using r could imagine management moved forward yet deploy using simple download service basic work follow series technical question deploy deploy monolithic micro service maybe excel something else use lambda something like docker kind store report cleaning etc question educate interviewer much know building thing solve business problem know educate need polish get job eventually probably true job machine tool toolbox get job done whole job company want whole",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "[D] Current ML Engineer Searching for Career Advice.txt",
    "text": "package get discouraged good luck honestly worst deployment scheme ever heard actually use case definitely see advantage think need prepared talk trade offs company current approach decision work well thing might differently understood people someone 2 year experience necessarily position radically change company practice similarly able tie physic simulation business need hopefully connects ml stuff assuming actual connection mle job going stuff training deploying able talk thing important enable either ml directly business goal ml experience docker fastapi personal project plenty panda experience work recent interview much stronger emphasis work experience rather personal project trying show breadth knowledge engineering portfolio focus developing stronger grasp skill",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "[D] Seeking advice on choosing PhD topic_area.txt",
    "text": "source reddit machinelearning title seeking advice choosing phd topic area content hello everyone currently enrolled master program statistic want pursue phd focusing theoretical foundation machine deep neural network considering statistical theory primary option optimization phd research area unsure whether statistical theory optimization appropriate area doctoral research given goal context hope theoretical foundational work neural network researcher research lab future question 1 area research would recommend someone interested fundamental research machine dnns 2 popular promising technique mathematical framework used researcher working theoretical foundation deep thanks lot help comment broad question field big give concise answer 1 several open problem topic ml theory generalization distribution shift quantifying aleatoric epistemic source limited limited annotation domain adaptation self supervision inverse problem causal modeling time dependency language video modeling reliability xai alignment epistemological ethical dimension line continual self domain adaptation functional space ntks inr pinns etc optimization regularization double descent grokking disentanglement concept hierarchical sparsity quantization gradient discrete space note list topic grouped manner may disagree may claim certain topic tangentially related ml theory moreover mean exhaustive could grow arbitrarily large several relevant open question mathematics statistic could also apply well several computer science philosophy ethic 2 due extent 1 difficult provide satisfying answer interesting theoretical framework show several domain highlight information theory classic theory kernel method broadly applicable differential geometry field find often interesting application much niche case also worth noting discussed previous post algorithmic information theory ml differentiable approximation algorithmic structure e g difflogic link statistical optimization procedure algorithmic structure transformer learn algorithm compute digit pi arbitrarily large training set start question understand really interest see solved look good well connected advisor tell look problem",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "[D] Should I learn Probabilistic Programming if I'm an prospective Data Scientist.txt",
    "text": "source reddit machinelearning title learn probabilistic programming prospective scientist content currently journey becoming scientist contemplating whether invest time probabilistic programming prospective scientist aware importance statistical analysis machine technique curious relevance benefit probabilistic programming field understanding probabilistic programming allows u effectively crucial scenario noisy incomplete incorporating probabilistic make informed decision quantify uncertainty develop robust predictive would love hear insight experience matter found probabilistic programming valuable skill science endeavor significantly enhance quality analysis prediction learned resource course would recommend someone starting perhaps importantly industry indeed seek merit skill comment probably high list thing study basic important thing looking build ml based system probabilistic programming good skill arsenal use work quantify associated prediction useful book learn space idea talking never heard probabilistic programming probabilistic programming decade tl ask going bug fixed end sprint say probably another good reference second book advanced check course topic yes might worth taking know probabilistic machine asked probabilistic programming assumed someone let know entire field employment haha big new course optional part master degree currently undecided weather attend seems quite niche thing telling saw online mean like programming quantum computer thing think since based probability help career science interesting conversation starter decide take course hope hope teach useful stick simpler method excellent tool arsenal especially helping leader high stake company policy decision even directly use method teach starting basic bayes theorem get firmer grasp many important area working tl dr yes take course add important tool arsenal fortifying stats skill overall",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "[D] Solo machine learning engineer woes.txt",
    "text": "source reddit machinelearning title solo machine engineer woe content work start sole machine engineer tasked develop state art domain specific problem information task computer vision problem complex keypoint localisation 3d mesh prediction training proposing build synthetic pipeline supplemented real world feel like much reading paper writing plan documentation implementing code training reporting management etc bos understands research take long time nevertheless feel like management fully grasp result certain even likely even though work environment good handling around result well mentally wanted ask experience typical team size kind job advice thanks comment sound like want scientist ml researcher engineer problem small company startup understanding really need probably hired early bro read paper unless really find implementation github use even state art figure deliver solution quickly even perfect possible push back management scope deliver business value quickly gtfo go somewhere structure startup amazing learn lot everyone solo ml person absolutely crucial blunt limited resource manager tend read thing assume google apple whoever something able well unless front know achievement possible huge raw datasets budget resource cleaning large compute cluster multiple researcher engineer zero responsibility given freedom focus exclusively single problem freedom fail meaning big deal research go nowhere solo engineer need tell want x need resource available need figure realistic target achievable resource actually best person figure actually delivered let get railroaded impossible objective want disappoint confident enough stand ground insist lofty ambition scaled back edit want echo someone else point start thinking early possible ambitious flag soon possible product goal always revised flagged early enough hi dude alone solo engineer till one year joining neither organization know purpose find problem justify salary problem related contactless vitals pretty tough 3 month came solution solo single coworker manager involved sort guidance btw joined fresher startup always difficult tell bos proposed possible would try take approach research laying everything need done coming realistic estimate pitfall well conversation better informed point view need help key point detection sota paper repo recently developed one multiple team faang calibrate expectation hey man understand feeling work biotech startup sole computational ml researcher first year really tough project hired poorly thought always happy work though able show wanted possible computationally experimental confirmation never worth eventually figured project doable serious potential value full swing someone else mentioned probably hired soon great time learn blow reddit regret much time wasted exact situation couple year ago bos expected big thing want admit completely unaccomplishable unless went way bigger hiring engineer learned lot ml computer vision cloud computing developing solution unsolved problem edge device get unscathed like come away lot good experience try focus good luck research lab spend year ton money developing state art perform well poor benchmark compared real life alone know much supposed find new sota method field sota suck moment leave going happen first manager understand company based sota based work well enough real talk boss make understand need buy provider using synthetic ever compare real thing next understand able find new take wrong way many many people year",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "[D] Solo machine learning engineer woes.txt",
    "text": "research experience work field come working idea time personally would either stay startup fails hey learn ton thing next job leave asap could start collection exploration create roadmap work needed go project kind return could expect would cost give clear idea workforce would need hire obtain result read enough paper get decent estimate amount type required achieve desired result come concrete plan acquire implement plan best ensure management truly understands much everything else dependent treat part separate project must completed first moving anything else one person team need break thing manageable achievable piece productivity hit scattered focus cause also psychological effect trying achieve large intimidating goal without measurable progress see getting closer final goal brain need incremental win dopamine seratonin endorphin whatever come stay confident motivated instead ending feeling overwhelmed even depressed tell bos patience need need sota small company senior management know anything ml absolutely true make mistake going project solo hey live learn problem well developed area open source implementation tried even build working poc good enough hence research building unless think shortcut solution like say deliver business value even perfect think entirely different problem work kind people characteristic think start ups suited year gave problem either way seems like well solved 3 month think good advice already given plan timeline never said might unrealistic oops saying previous plan optimistic harder thought would fine also may find lot paper circling problem come source depending security issue may able convince boss partner school help research phase realistic estimate different plan attack well received take look thanks",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "[D] To those of you who quit machine learning, what do you do now_.txt",
    "text": "source reddit machinelearning title quit machine content currently master degree set dl related career recently noticed bring joy coming architecture randomly work work tuning parameter waiting day till trained level high feel productive working slowly considering switching another field quit machine especially deep 1 switch 2 satisfied new job stressful intellectually challenging possible keep 9 5 3 ensure smooth transition field thanks advance p know machine deep current subfield computer vision mostly deep used comment try applied field industry phd student used deep type research computer vision background working applying ml computer system least 90 actual research work building system collecting deciding frame problem last essentially anything reasonable catch solid ml background might completely miss last never realize much satisfying since working application likely potential downstream use case lined collaborator also probably person working specific problem formulation need worry beating sota squeezing last 5 driven algorithm 2x better traditional approach really matter deleted coming architecture randomly work work tuningparameters waiting day till trained level ofuncertainty high good news say work computer vision high chance practice mostly use shelf solution actual time spent gathering removed pivot adjacent role within ml like mlops engineering like waiting entire day run fix late night another iteration throughout night build machine platform traditional software engineering come predictable outcome knowledge ml still valuable actually went way background fedgov hurricane scientist work popular weather app teaching ml part lot statistical modeling meteorology already mainly bias correction downscaling made decision move away dynamic based research statistic based application since take field well argue meteorology one original big problem guess topic different day anyway bunch meteorology company using ml variety thing since said computer vision identifying thing satellite imagery sentinel 2 landsat maybe even go popular right get away core issue tuning maybe different focus might work would warn pay field meteorology suck relative big tech 1 engineering devops 2 way less stressful ml really clear requirement need get source certain target constraint sometimes challenging due business requirement time consistency monitoring pipeline find better go project even know feasible 3 good programmer got ml like switched back used big deal curriculum lot software engineering managing network pure dev lot subfields even basic churn prediction something valuable lot firm sure swept dl hype job similar describe current job contains less way tiny tweak massive dl feature engineering engineering general suit better slightly warm take dl coal face industry feel random time consuming result bit demoralising power knack enjoy super bag manage people still machine motivating scientist unlike constant iterative grind matching right reward function right jumped ml 2 year mlops 1 year backend engineering 1 year planning come back ml mostly worked cv bit bored since mostly used shelf endeavour turned boring personal opinion nothing wrong field find intellectually challenging bit stressful seems lot system place created complexity coming back ml feel satisfying although con enjoy field tried went quant research position prop trading firm happy decision unfortunately using deep far lot stats machine also interesting application physic informed neural",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "[D] To those of you who quit machine learning, what do you do now_.txt",
    "text": "network want look point definitely fun work problem need solved creative way instead continuously come new research idea lot nlp specialist existential crisis since chatgpt gpt4 shown solve pretty much problem throw u used knowledge productising large language move mlops engineering outcome deterministic would say coming architecture randomly work work shortcoming understanding field general asking thinking opposite switch right ml interest deeply currently standard cloud development dislike computer vision annoys somehow never quite made working ml professionally week engineer case like dive something along line position tend called high chance right haha sure comforting ready collect",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "[D] What is the current consensus on the effectiveness of Active Learning_.txt",
    "text": "look number paper talk semiconductor none yet whole semiconductor field consists large company like kla tsmc samsung hynix large computer vision research group research publish nothing promise fulfilled extremely hard question answer try see work best reason wanted try al first place limited labeling budget 2x work al work",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "[P] Prediction model developed and validated - how to proceed_.txt",
    "text": "source reddit machinelearning title p prediction developed validated proceed content finished master non informatics health related field developed classifier predict probability adverse event ventilation intensive care unit auc around 0 86 testing external validation yielded worse result 0 77 quality poor using higher quality dataset already planned professor want publish paper far good work product manager clinical information system vendor actually place live embedded workflow topic pretty hot domain perspective clinical economical however management show interest buy probably fear risk responsibility clinical environment lot tech background general purpose recommendation experience situation appreciate input comment got academic buy ip issue employer worth aiming publication patent try get everyone room e industry bos faculty mentor hash legal side thing may become make product worth trying get cv form typically look good party get pub especially work done already accuracy metric give bit pause though without knowing domain 77 seem terribly high compare thanks input sound reasonable im gonna try 8 related paper dealing issue one validated externally prospectively auc around 82 retrospectively collected one customer real mess training testing came high quality research database icu case",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "[R] Consensus and uncertainty ML research- arXiv endorsement - is it actually possible without affiliation_.txt",
    "text": "source reddit machinelearning title r consensus ml research arxiv endorsement actually possible without affiliation content hey r machinelearning independent researcher working private company agent consensus metrology hitting classic arxiv endorsement wall wondering people experience working mathematical framework deterministic multi agent consensus using metrology framework new lm training approach based quantification routing benchmark evaluate basic reasoning sota score 30 hypothesis agi probability requires proper system parameter scaling problem seen post claiming independent researcher get endorsed reaching couple researcher reality seems different affiliated phd program institution option 1 keep trying arxiv endorsement tip approach 2 publish personal website github reproducible code 3 openreview researchgate 4 find academic collaborator affiliation 5 anyone successfully gotten endorsed private independent researcher worked also curious published outside traditional channel hurt help work visibility care idea reaching right people academic exposure would especially love hear others working foundational ml outside academia big lab thanks comment u endorse unless know personally obviously work share work u feel genuine research otherwise yea submit conference acceptance need go attend simply get published tmlr jmlr important asking find acquaintance published paper ask ask endorsement coauthor first published paper completely unrelated send mail close research area got endorsment first try work already blog code open could checked situation consequently thinking submitting towards science publishing one linkedin account also way timestamp paternity idea suppose try research square hi seeking endorsement c paper agentic operating system happy discus paper anyone want prior endorsement paper endorsement link thank definitely helpful thank jmlr tmlr rly easy get accepted something thought still decent standard easier timing op curious suggestion thanks trying find old colleague might working ml thanks check checking would seems possible peer reviewed somehow seems arxiv still relevant maybe norm field checking possibility point",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "â€œGood at practical ML, weak on theoryâ€ â€” getting the same feedback everywhere. How do I fix this_.txt",
    "text": "source reddit datascience title good practical ml weak theory getting feedback everywhere fix content recently got feedback machine engineer interview clearly understand make ml algorithm work practice solid experience real world project explanation theoretical concept behind algorithm vague imprecise recommend taking month review fundamental reapplying first time heard fact pattern seeing across multiple interview tech focused company getting way landing kind role really interested context working 2 3 year ml engineer large non tech company experience pretty diverse traditional supervised computer vision recent shift toward genai llm embeddings prompting rag etc built end end pipeline deployed shipped ml production work applied lately genai oriented honestly drifted away theoretical side ml trying move role ml mature company getting stuck theory part interview question would recommend brushing ml theory structured deep way field starting zero clearly need tighten understanding explanation would love advice resource even personal story others made leap applied practical ml theory heavy role thanks advance comment deep book understanding deep prince excellent classic machine book introduction statistical good well provide example theoretical question asked explain people giving good practical advice wanted give encouragement lot people really bad conducting interview remember one company gave feedback surprising lack basic statistic knowledge phd statistic working statistician census bureau time follow people advice improve interview answer let interview hurt self confidence normalized tech interview actually bizarre unnatural experience unusual able recite random theory top head normal able question asked judged qualified resume experience read introduction statistical working bishop bishop recent deep book nice run lot concept kind aware truly absorbed nice complement something like islr walk ridge regression gradient boosted tree perfunctory 30 page neural net unlike islr assume solid background undergrad level calculus linear algebra everyone recommending course bro need grab real math stats book easy forget theory teaching researcher something constant contact theory would get reilly text book perhaps hi depends specific subdomain general would recommend coursera machine mooc andrew ng base interesting learn prof explain concept go deeper pre deep recomment book element statistical friedman hastie tibshirani deep book goodfellow bengio courtville also lot course standford online youtube channel choose specific topic dive study founding paper current llm wave try prepare question failed llm see explain even understands sometimes difficult find right word take much time brush theory come completely different background started theory dsa slowly want get problem solving experience current job get deploy solution reach stage hoping someday clear interview land job get plenty development experience rather analytics useful resource practice kind theoretical ml interview question hey dm really want know project learn practical applied knowledge scientist hey op please share resume hoping apply mle position resume might help tia idea give lecture teach class beginner force improve understanding communication mock interivews friend interview platform use llm mock interview asks question provide answer verifies answer fix theory opinion fine keep applying practical stuff actually important 99 time give science embrace ml engineering regret read book learn technical term interview presentation bro see lost fluency might solve 1 ml",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "â€œGood at practical ML, weak on theoryâ€ â€” getting the same feedback everywhere. How do I fix this_.txt",
    "text": "problem per day least track experience week sure get back something felt lot time computational thinking might accurate might lack interpretation due loss fluency try let u know worked look think mean know job work want someone explain mathematical foundation behind kind thing tutorial video teach really going technical book suggest take two three master best go mathematical basis behind try explain mathematical perspective",
    "cluster": 2,
    "keywords": "ml, work, machine, need, like, thing, get, one, know, problem"
  },
  {
    "filename": "334ae44c6285b8ebe8df2327a4e0d3fc.txt",
    "text": "08 app ahsen khaliq ahsen khaliq ml hugging face 1 believe believe llm explore quantification large language llm goal identify response given query large simultaneously consider epistemic aleatoric uncertainty former come lack knowledge ground truth fact language latter come irreducible randomness multiple possible answer particular derive information theoretic metric allows reliably detect epistemic large case output unreliable condition computed based solely output obtained simply special iterative prompting based previous response quantification instance allows detect hallucination case epistemic high single multi answer response contrast many standard quantification strategy thresholding log likelihood response hallucination multi answer case cannot detected conduct series experiment demonstrate advantage formulation investigation shed light probability assigned given output llm amplified iterative prompting might independent interest 1 393 25 ahsen khaliq ml hugging face 1 paper page 7 georg zoeller 1 different selfcheckgpt 6 ameer arsala co founder almata research lead cal poly 20x hackathon loser 1 exactly point developed feel cohorte 1 rag system like supercharged llm large language take already impressive ability",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "A friendly starter paper - Entropy-Guided Loop_ Achieving Reasoning through Uncertainty-Aware Generation [R].txt",
    "text": "source reddit machinelearning title friendly starter paper entropy guided loop achieving reasoning aware generation r content hey r machinelearning idea wanted put simple straightforward way tried make paper easy read starter friendly also show research partner focus measurement metrology think widely addressed ml nlp motivation came exploration weight bias sunday cafe event sf exploring observability weave product think running loop adding complex tool paper production valuable help bunch way importantly help making small useful kind reasoning process sort future might useful make loop inside output layer anybody think cool application method title entropy guided loop achieving reasoning aware generation abstract reasoning often outperform smaller 3 5 higher cost added latency present entropy guided refinement lightweight test time loop us token level trigger single targeted refinement pas extract logprobs compute shannon entropy top k alternative apply simple logic trigger perplexity maximum token entropy low confidence token count unlike approach use entropy measurement decoding pas compact report token confidence alternative context back guide corrective edits representative technical query across reasoning mathematics code generation task small loop approach 95 reference reasoning quality approximately one third cost method achieves selective refinement 31 response improving accuracy 16 percentage point single pas inference demonstrate aware loop provides effective middle ground single pas inference expensive reasoning chain making practical production deployment quality cost matter like let know open critique comment seems similar really cool sound like could useful router hybrid reasoning determine reason dont understand reasoning wasnt specified result table earlier reference sound like deepseek used compare gpt 4o mini o4 mini deepseek v3 r1 pair base would also interesting compare result router hybrid exist right like gpt 5 deepseek v3 1 nice find quantification cot clever tested generalizes beyond math problem aware statistical tool implementation really nice efficient like efficient implementation older paper robust neural network check related method basically perform perturbation latent space reminds related book published pdf like share also curious used help simplify agent design also would love use encoding importance improve design interesting approach balancing cost quality concept using report feedback loop clever focused higher level process loop token level refinement neat first thanks pointing read deep think confidence surface feel related work turn token level test time behaviour think shape sufficiently different deepconf multi sample parallel thinking method spin many trace compute local confidence metric group tail early stop weak trace filter weight rest vote good relevant afford non trivial sampling budget gain come selecting better trace wasting token obvious low confidence egl entropy guided loop single path one targeted refinement run compute simple signal per token entropy perplexity low confidence span trip threshold create compact report looked bad alternative brief context ask rewrite answer conditioned report n way sampling voting engine mod drop inference layer put front api focus predictable latency cost engineering implementation observability leaderboard sota theme use inference different action deepconf rank stop filter across many candidate self consistency egl feed back repair single candidate also different deployment recipe deepconf strongest budget lot parallel sample tweak decoding internals",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "A friendly starter paper - Entropy-Guided Loop_ Achieving Reasoning through Uncertainty-Aware Generation [R].txt",
    "text": "patch decode loop confidence plumbing egl meant production path small request refine one get exactly one extra pas guided report evaluation posture differs well deepconf focus math logic leaderboards bigger sample count prioritised cost latency trade offs human rated correctness mixed task value judgment two target actually think complementary practical hybrid would run small number trace local confidence early stop avoid junk pick best run one guided rewrite like mine survivor keep accuracy gain keeping cost closer single pas open point point anyone spot specific section look similar mechanism send page figure address directly said related idea space different computation different action taken different constraint yes seems like thorough review hey feel right specify paper idea make broad enough concept let people experiment idea notebook run openai non reasoning expose logprobs make really easy test also quick blog post yes basic tool added reason kind matter extensive test make small improve output specifically failed tool call useful task inherent forward pas always idea simply start checking tap time time simple almost elegant think yes please share help yes think sort technique simple yet improves output applied agentic system importantly make small useful yes please share thought encoding importance also make pr github like create new script add info readme page link yeah please give try let know also came around time published paper recommend entropy related improvement also entropix go sampler yeah deep thinking answer multiple choice question voted kind silly case easy show performance way trying go back see performance improvement paper thorough skim reveal say found give poor answer follow pointing wrong result usually still poor almost always go back regenerate first response get one make mistake keep poor content content first paper make think something like beam search generate 512 token 512 different response cull 256 worst performer choose confident response end run method objective benchmark curious see method v non voting version confident thinking paper choose result confident thinking something like humaneval coding benchmark would cool could see using method determining key decision point branching response point explore parallel llm choose best response end return let user decide could lead interesting rlhf process hm see invalidates performance comparison reasoning theyre even base still interesting cost perspective though think invalidate claim matter within small single pas v single pas entropy refine loop whole point reasoning row yardstick cost quality basis improvement lock named pair want method portable api level vendor agnostic reasoning expose logprobs cloud apis run reasoning reproduce almost case vendor shuffle version weekly need exact control easy notebook available pick base toggle loop pick whatever reasoning anchor like compare repo notebook run 4o mini v 4o mini loop v3 v v3 loop put o4 mini r1 reference line want pr github log add matched pair section readme credit pattern hold selective refine buy back big chunk quality cheap",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "Decoding LLM Uncertainties for Better Predictability.txt",
    "text": "source reddit datascience title decoding llm uncertainty better predictability content hi building last research post wanted figure way quantify ambiguity prompt response llm ended discovering two useful form structural conceptual nutshell conceptual sure say structural sure say play around demo read detail blog post comment bot bleep bloop someone linked thread another place reddit r datascienceproject decoding llm uncertainty better predictability r datascience follow link please respect rule reddit vote thread info r totesmessenger contact message compose r totesmessenger",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "e4f5f1b14de072596e67fd41f535dc3f.txt",
    "text": "08 app bayes lab bayes lab 2 567 1 research paper highlight believe believe llm yasin abbasi et al google deepmind language sometimes produce hallucination response low truthfulness align common knowledge since llm probability distribution text address truthfulness statistical paper explores quantification llm distinguishing epistemic aleatoric 1 quantification llm study introduces information theoretic metric measure large language llm metric distinguishes epistemic lack knowledge aleatoric randomness reliably detects high epistemic indicating unreliable output computed iterative prompting 2 detection hallucination method effectively detects hallucination instance high epistemic single multi answer response unlike standard strategy excels detecting hallucination multi answer scenario demonstrating significant advantage extensive experiment 3 limitation existing approach existing quantification method struggle scenario involving multiple correct response due aleatoric cannot differentiate large aleatoric perfect predictor large epistemic poor predictor 4 innovative solution utilizing iterative prompting developed information theoretic metric measure epistemic large language llm metric assesses difference distribution response generated llm actual ground truth remains unaffected aleatoric allowing accurately gauge epistemic even whe",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "ed062b733f461ea29048f89daac58cd8.txt",
    "text": "08 app stefan harrer phd stefan harrer phd 3 interesting paper alert openai topic large language hallucination word llm make stuff incorrect counteract think old news heard every answer question already think since advent genai immediate standard answer question llm probability distribution certain word phrase followed word phrase using sort training high quality correct much garbage false prompted llm use learned training predict likely next word sentence phrase following prompt fancy autocomplete process lead llm produce incorrect prediction llm hallucinates obviously way reduce never eliminate llm probabilistic hallucination weed bad training much possible far good paper shine new light onto another way reduce llm hallucination change way evaluate performance llm currently many valuation metric reward giving correct answer punish giving wrong answer remember multiple choice test took know correct answer checked one three box anyways knew chance answering question correctly getting 1 point added final score 33 took chance guessed 0 admitted know answer skipped question nothing lose guessing without knowing 33 chance win llm evaluation metric working one reason make hallucinate matter fact openai researcher observed many hallucinated less optimised using acc",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "How I Use MLflow 3.1 to Bring Observability to Multi-Agent AI Applications.txt",
    "text": "source reddit datascience title use mlflow 3 1 bring observability multi agent application content hi everyone diving world multi agent application probably noticed recurring issue tutorial code example feel like toy fun play come building something reliable production ready fall short run code half time result unpredictable exactly challenge faced started working enterprise grade application wanted application work also robust explainable observable observable mean able monitor happening every step input output error even thought process explainable mean able answer question like give result went wrong catch multi agent framework become abstract convenient use also made harder see hood often even tell prompt finally sent large language llm let alone result expected started looking tool could help monitor evaluate agent effectively turned mlflow worked machine might know mlflow tracking experimentation tool latest 3 x release mlflow added specialized support genai project trust game changer mlflow tracking record observability matter diving detail let talk important application especially multi agent setup need three key capability 1 observability monitor application real time log visualization see happening step 2 explainability something go wrong figure algorithm explain decision 3 traceability result deviate expectation reproduce issue pinpoint cause three key metric evaluating stability enterprise genai application image without flying blind building enterprise grade system reliability critical flying blind option mlflow help mlflow best known tracking capability genai feature really caught attention let track everything prompt send llm output generates even streaming scenario responds token token event tab mlflow interface record every sse message mlflow autolog also stitch together streaming message chat interface setup straightforward annotate code use mlflow autolog feature automatic tracking leverage context manager granular control example want know exactly prompt sent tracked want log input output every function agent call done want monitor error unusual behavior mlflow make easy capture view code execution error message event interface best part mlflow ui make accessible clean organized way filter search drill specific run span e individual event application real world example project involving building workflow using autogen popular multi agent framework system included three agent 1 generator creates idea based user input 2 reviewer evaluates refines idea 3 summarizer compiles final output framework made easy orchestrate agent also abstracted away lot detail first everything seemed fine agent producing output workflow ran smoothly looked closer realized summarizer getting information needed final summary vague uninformative mlflow able trace issue step step examining input output stage discovered summarizer receiving generator final output simple configuration change fixed problem without mlflow might never noticed might never noticed agent passing right info llm mlflow helped sharing sell mlflow open source sharing know frustrating feel like stumbling around dark thing go wrong whether debugging flaky chatbot trying optimize complex workflow right tool make difference working multi agent application struggling observability encourage give mlflow try perfect patch bug autogen integration example tool found job far comment hey sound like really diving deep complexity multi agent application totally get frustration lack robust example observability explainability crucial especially trying build something functional also reliable one thing found",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "How I Use MLflow 3.1 to Bring Observability to Multi-Agent AI Applications.txt",
    "text": "super helpful tackling challenge focusing architecture agent interact using framework like autogen easy lose sight flow agent enhance observability consider implementing logging mechanism capture input output also intermediate state agent way trace back decision making process understand certain output generated instance using mlflow leverage autologging feature track every interaction agent llm includes prompt sent response received invaluable debugging want take step think integrating granular logging system record state agent various point workflow could help pinpoint thing go awry like summarizer example also concerned reliability agent consider using sandboxing approach working cognitora dev utilizes firecracker microvms sub second vm startup hardware level isolation help run agent controlled environment reducing risk interference making easier monitor behavior real time lastly looking scale application think implement multi agent coordination using a2a protocol help streamline communication agent ensure page crucial maintaining integrity output keep pushing challenge right architecture tool able build observable explainable multi agent system written full tutorial walk everything step step including sample code real world example find",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "LangDiversity_ software to identify LLM errors.txt",
    "text": "source reddit datascience title langdiversity software identify llm error content due challenge hallucination detecting error output given prompt becomes important challenge langdiversity implementation diversity measure domain independent used measure result language type pip install langdiversity video web visit read paper comment",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "Why Language Models Hallucinate - OpenAi pseudo paper - [D].txt",
    "text": "source reddit machinelearning title language hallucinate openai pseudo paper content hey anybody read seems rather obvious low quality missing something openai working hard make system useful reliable even language become capable one challenge remains stubbornly hard fully solve hallucination mean instance confidently generates answer true new research paper open new window argues language hallucinate standard training evaluation procedure reward guessing acknowledging chatgpt also hallucinates gpt 5 significantly fewer hallucination especially reasoning still occur hallucination remain fundamental challenge large language working hard reduce comment actually million dollar optimization problem pressured answer everything introduce idk token might circumvent reward become lazy answer query know bunch try solve issue latest one gpt 5 people felt lazy abstained much answered way shorter predecessor created lot backslash others performed better tldr hallucination like guessing rewarded primary evaluation discus statistically rigorous modification existing evaluation pave way effective mitigation mentioned wish research bitemporal probabilistic knowledge graph rag toyed hour structured output see could get llm convert arbitrary information format seems require lot work getting entity relationship perfect probably requires whole team keep thinking one able feed time travel novel build flawless kg temporal entity relationship actually practice seems difficult kg would contain wikipedia book scientific paper etc preprocessed optimal relationship obviously push would also used training reinforcement system detect possible hallucination personally want kg stuff think required future embodied continual stuff kg act short term memory obviously new idea ton derived paper memgpt cover lot memory idea one larger company invest resource build complete kg test retrieval would really beneficial one structure llm improves used reprocess information attempt find error kg granted utilizing actual query even optionally would cost think people would pay extra token though source fact imagine hovering clicking fact specific information rapidly getting backing reference kg luke star war ever go planet getting back yes x book page 211 tend see openai b factory article response much paper anthropic others published compute needed stop hallucination even bigger current scaling problem supposedly many great comment thought asking openai deal anybody want see reply reaction new finding read like restatement lot people pointing year problem guess training incentive reward coherence fidelity saying know valued highly producing fluent yes see fewer hallucination u started calling fidelity drift system drift truth preserving behavior toward output look convincing unless evaluation protocol change keep seeing paper sound like breakthrough touch deeper issue abstract blaming state hallucination improperly designed benchmark rather anything internal hey look timing make think openai trying get ahead trending paper hassana lab compression failure llm bayesian expectation realization yeah really new framing around training reward guessing instead admitting feel like way explain hallucination persist big breakthrough hallucination failed generalization irony generalization good inference improve less training rather depending context overtraining make neural network rigid brittle reducing node sometimes help situation dealing rarity distribution situation little generalization help matter much work believe possible hallucinate could precise impossible without hallucination sure value paper add error generating reply llm hallucination something counter whenever engineering product one quick straightforward way reduce",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "Why Language Models Hallucinate - OpenAi pseudo paper - [D].txt",
    "text": "hallunications rag provide necessary context llm ask generate content based context also add citation response citing part context source responsible generation improves credibility response recently implemented hybrid rag approach reduce hallucination twin platform helped build keyvalue curious know please head blog written much feel like hype real progress recent response anthropic paper addressing hallucination make wonder focus shifted towards handling potential issue rather pushing new development forward issue hard say even know wrong inkling wrong know factual statement correct naturally entropic sentence einstein one correct continuation hallucination pretty straight forward result generative nature supervised training generate likely sample wether sample true false never explicitly considered optimization many case training question even answerable high certainly due missing context original might used guessing e g paper title fit certain structure real obvious consequence rlhf theory lot potential fact check reward punish issue well first get lazy saying idk without punished using task dependent rejection budget limit rejection rate one close one expected given task context might possible rejection budget lowered many answer hallucination increased much rejected often time rlhf directly applied instead reward used need careful train reward accept plausible sounding answer aka hallucination instead mimic fact check process done human really really hard give reward ability e g search paper title accept one sound plausible hallucination popular conceptualization happening hallucination imo extremely harmful many front poor conceptualization strongly pull one towards line thought understanding fundamentally misaligned way actually behave output hallucination sometimes happen right translate text refuse answer query would otherwise resulted non factual answer certainly train output confidence estimate alongside normal output carefully calibrate estimate expectation less align probability given output labeled accurate first done double spot hallucination occur guess still hallucinating regular output alongside confidence estimate plus agent general something like chatgpt evidently straight impossible problem eliminate opposed merely somewhat reduce non trivial error judgement hint anytime somebody claim something impossible c halting problem probably right around corner even assume magically got 100 accurate confidence estimate still bog standard roc curve situation faced whatever confidence threshold arbitrarily pick cutoff getting significant amount false positive hallucination false negative refusal answer even though answer would hallucination trade see fit magic way eliminate false positive false negative one smart trick conceptualization something basic entirely well understood hallucination leading many people think bypass limitation statistic magic llm unless got literal magic oracle good luck halting problem false positive false negative fact life trying anything non trivial research angle commercial one make idk answer acceptable really think solvable principle w token entropy signal fed multisample inference majority voting",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[Discussion] Uncertainty propagation in LSTM-based RNNs.txt",
    "text": "source reddit machinelearning title discussion propagation lstm based rnns content several application using lstms rnns char rnn alex graf handwriting generation given input current time step output predict parameter governing distribution output training time time step train maximize likelihood true output w r predicted distribution prediction time last observed time step predict output distribution sample get input next time step given predicted input obtain output distribution lstm output distribution truly indicative variability prediction previous time step propagated correctly single sample taken discrepancy keep increasing multi step prediction work past dealt problem problem restricted lstms rnns predictive propagate non linear function comment currently experimenting idea similar particular making output decoder lstm use function negative moving average previous probability distribution order make prediction time step one thing found help much concatenate onto input decoder use explicitly computing probability distribution yes fact training time behaviour match way use rnn test time known problem example paper trying address would scheduled sampling hugo thought paper also informative recent example would professor forcing seen method described multiple sample taken step creating tree possible output sequence repeatedly pruned keep highly likely sequence professor talking mentioned practice often take single sample vastly increased runtime slight hit accuracy incur field think often became always maybe bayesian solution use output new prior word symbol next prediction special decoder cell take entire output projection previous timestep input interesting moving average affect prediction subsequent time step see affect prediction send input lstm also reason chose store negative moving average previous distribution anything maybe phrase question right talking problem training time inference time behavior different scheduled sampling professor forcing approach address question concerning problem getting accurate estimate multi step prediction consider first time step inference give input predicts output distribution proceed sample single point distribution send input next time step would accurate would send distribution input next time step get predictive distribution second time step function previous distribution function single sample hope make clear would accurate would send distribution input next time step get predictive distribution second time step function previous distribution function single sample converge toward general frequency output token corpus get single coherent sequence ask next letter sequence hello frie good idea context n ask 40th letter sequence likely really shrug guess letter tend common language context going stale ah see sorry aware work address directly could maybe look entropy probability decoding high entropy would suggest high wanted something bit concrete could convert number perplexity basically tell many option uncertain far understand worried getting proper estimate p 2nd prediction input note n th n 1 st prediction correlated sampling single point sending input next timestep interested p 2nd prediction input 1st prediction since amount correct common probability p sequence prediction input distribution second next letter sequence saw th high probability space e chosen continue sequence saw thi choosing space second letter would horribly wrong make prediction inconsistent interested total probability particular n th prediction afraid general case cannot get single run rnn either bruteforce possibility n 1 mc sampling general case",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[Discussion] What exactly are World Models in AI_ What problems do they solve, and where are they going_.txt",
    "text": "source reddit machinelearning title discussion exactly world problem solve going content hi reading lot world lately especially context reinforcement potential crossover llm love hear community insight key thing problem world actually solve understand idea let agent build internal environment predict imagine plan instead blindly reacting would massively reduce sample inefficiency rl allow generalization beyond seen accurate world differ expert system rule based reasoning world us prior knowledge simulate infer unseen outcome fundamentally different expert system encode human expertise use inference dynamic flexibility generative imagination capability make world scalable technology architecture typically involved see reference latent dynamic e g dreamerv3 planet vae rnn transformer structure predictive coding latent imagination memory based planning e g muzero key approach people exploring state art right know dreamerv3 performs well continuous control benchmark muzero breakthrough planning without known environment close scalable general purpose world complex open ended task current challenge guessing thing like modeling partial observability transferable representation across task balancing realism v abstraction internal simulation heading people say world key artificial general intelligence agi others say brittle outside curated environment see merged llm build reasoning agent embodied cognition system would love hear thought example paper even critique comment generated reason look similar format chatgpt us emoji section header stuff opinion general concept ability predict next state world based previous state world understanding world well grasp behaviour nature people even abstract concept claim created world using completely different technique deep based vision system symbolic llm etc sense almost give definition point think might like video btw really like thread actually learned lot thank much world really set assumption make world reflected make decision mostly mean actually referring kind concretized design instead speculating performance characteristic simplify version system many real world think possible use current machine training technique world maybe deserve coordinated quickest makeshift test air grandfather lunchroom simplistic telephone post mass deleted anonymized redact world latent representation world case video diffusion transformer dynamic world look open oasis case learns minecraft world physic interaction minecraft video gameplay imagine training real high def video nature video game maybe even impose symplectic form classical mechanic using hamiltonian graph neural network symplectic integrator unitary quantum mechanic adding kind auxiliary loss otherwise enforce self adjointness perhaps go far try put general relativity start adding proper time every voxel latent space representation world like fiber bundle q p would form cotangent fiber classical phase space proper time fiber u 1 phase fiber quantum amplitude enforced architecture thing could added learn physic world top scaffold built architecture closer architecture real physic easier would learn physic video let give picture building playing go yes alphago state easy right snapshot board tile 3 kind state empty white black modelling state kind trivial agree action would action placing piece board state way many action take might unallowed action like placing piece already placed one naive solution make mistake taking unallowed action punish negative reward definately work let see another solution simulate go yes search traditional one seeking next state next state give biggest advantage greedily",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[Discussion] What exactly are World Models in AI_ What problems do they solve, and where are they going_.txt",
    "text": "simulate say 3 turn look advantage action simulation agree taking greedy advantage state best get state take certain action notice never policy q old plain v ro evaluate state possible finite action infinite action may impossible simulate want go example always want think try explain hell world explicit implicit explicit one work like go example implicit one work differently mostly latent space use explicit knowledge build human plan whats next best move interestingly believe human reason planning dont follow order blindly reason whats best action take predict common sense whats gonna happen interestingly risk also modelled greedy advantage following example might also chance create greetest disadvantage might risky move something believe definition world varies slightly depending different context cognitive science often referred mental nature human ability construct internal representation world predict future state thing also known human world development concept evolved discus world think good definition world broadly mechanism agent capture access approximate environment dynamic definition come paper also inspired understanding concept world long existed research independent technology like reinforcement generative rise brought renewed attention idea absolutely great post correct world core difference world expert system rule automatically generated absolutely massive scale probabilistic nature neutral network adjusted kind decision space expert system 1000 rule system extremely hard adjust new point match current set rule main challenge also include able efficiently learn complex internal representation success video game quite straightforward many reason method scale real world problem sample efficiency world scale think partially seems understand stuff enough point think least heavily edited part familiar chinese find telling chatgpt detail structure want say post would logic clear english speaker thanks thoughtful reply really appreciate video recommendation helped clarify thing interestingly saw others thread suggesting world really judging whether system reason infer causality almost like functional benchmark rather specific technique video shared seems draw distinction world llm emphasizing world operate language space instead kind latent space aligned mental simulation like human think visually intuitively without word feel closer yann lecun proposing world part new computation paradigm entirely rather behavior attribute generalizes well curious see think world general purpose concept applicable many technique shaping new class architecture altogether understand correctly talk world really evaluating whether system reason causality make informed decision based internal assumption environment rather referring specific architectural module make wonder think current llm form embodied intelligence starting meet bar kind technique improvement architectural training wise think enabling love hear take yeah think right fully modeling real world complexity probably infeasible current ml technique like top voted comment suggested see world less literal simulation world way machine reason unseen condition internal causal logic building complete replica world rather giving agent ability make informed prediction plan beyond directly experienced make sense understand correctly saying world processing information internal representation space time rather relying semantics like llm quite different view thread world treated like conceptual benchmark whether system reason infer causality regardless process information really curious though grounded semantics kind world actually process talking equation physical state form structured signal ah see interesting use case",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[D] A MoE Model of Manageable Size for Initial Experiments.txt",
    "text": "source reddit machinelearning title moe manageable size initial experiment content research focussed routing mechanism mixture expert strcuture llm right find tough spot pre trained available huge smallest moe language find olmoe still around 7b parameter ideally looking small enough experiment still large enough exhibit interesting behavior since research centered routing mechanism necessarily need llm moe designed downstream task would work well suggestion manageable moe thanks advance input comment ibm 1b 400m active 3b 800m active moe also work w moes granite moes bad make arbitrarily small moe low compute cost using mergekit goddard clown car moe recipe edited clarify since make sense downvote unless misunderstood clear clown car moe choose existing base whatever size like optionally make fine tune put moe container mergekit goddard also describes clever hack programming gate parameter overall compute cost near zero except fine tune optional sound like needing anyway tiny moe suggested user suit great use wanted one one two order magnitude smaller clown car moe good way go similar situation phd assuming aiming top venue scaling experiment unavoidable want high quality pub recommend initial test small cost debugging code low everything working gonna dish h100 compute hour want atleast result 10b parameter even considered small suffice research need pretraining already trained main compute constraint pure inference large scale evaluation fine tuning experiment current training setup look like need full parameter fine tuning modern lora acceptable train olmoe 20gb gpu excessive even fft really push standard adamw touch expensive thing like torchao cpu optimizers low rank gradient approximation method also work free gpus kaggle colab used purpose depends extent training though question deployment training moe feel free ask area focus specifically cheer helpful definitely check thanks yeah thinking vision project run final experiment similar scale deepseekmoe 3b 16b proper gpu cloud rental service platform recommend platform called runpod discovered pricing seems reasonable already trained wanna large scale training hey thanks replying right pure inference approach first involves change original routing mechanism simple strategy like random selection yes quantitatively analyse routing mechanism effect final result need large scale evaluation fine tuning conducted part parameter right one problem encountered even simple round inference take long time deepseekmoe 16b colab a100 via transformer api beam search beam num 5 generating len 50 response take around 1 5 minute might exactly accurate experiment long time ago get idea anything wrong make computationally inefficient used lambda time pricing seemed reasonable easy use open one suggestion check shadeform marketplace popular gpu cloud rental provider like lambda paperspace etc let compare everybody pricing deploy one console account really easy way get best rental deal across gpu type",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[D] Best Way to Measure LLM Uncertainty_.txt",
    "text": "source reddit machinelearning title best way measure llm content best way quantify trained llm assume entropy final probability distribution decent measure wanted know nlp community stick measure something specific language would really appreciate recent reference may popped past month also cool easy integrate implementation thanks comment honesty point type evaluation qualitative simply joke observed long time ago working nmt trying base result bleu score literally meant nothing trying force new metric based simple rule computation probably fail believe need human stronger llm loop e g human rank output multiple llm human multiple different language new one otherwise view meaningless self promoting paper llm interesting enough read new idea better performance entropy good language like language understand world difficult hard gpt 3 like edit semantic look interesting would still rather let human rank result came across recently semantic linguistic invariance estimation natural language generation along prompt put end response state scale one ten confident answer work amazingly accurate source added bonus get confidence interval confidence interval asking confident estimation confidence published llm estimation method bsdetector outperform using entropy perplexity llm simultaneously accounting epistemic aleatoric semantic looking quick solution already implement company offer one language community general dont think best way measure opinion research distribution detection still primitive without solid theoretical ground general reference please look recent iclr paper different quality bleu attempt measure certainty prediction could highly certain wrong uncertain correct two measure independent good point sound like open problem statement get llm quantify way human also interesting relates broader question sentience consciousness cool thanks heard even ask llm fraction aleatoric v epistemic would estimate changed used bootstrap v mc dropout weather coherent gray soft chase aback frame zephyr paint grab post mass deleted anonymized redact overconfident agree anything else gaussian process scale require strong regularization work well deep net principled empirically demonstrated augmentation method ensemble etc post joke linked urban dictionary entry trust bro source",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[D] Beyond the cloud_ SLMs, local AI, agentic constellations, biology and a high value direction for AI progress.txt",
    "text": "source reddit machinelearning title beyond cloud slms local agentic constellation biology high value direction progress content dear r machinelearning friend today share thought different direction development field chase multi trillion parameter believe extremely valuable endeavour lie power constraint pushing get 1 billion parameter excel new blog post argue constraint feature bug remove scale cheat code force u innovate fundamental algorithm architecture path allows faster experimentation architectural change longer risk necessity improvement fear scale wash away gain real let remember mlp could never compete transformer matter much scaled post explores question current transformer mlp something better within grasp ignored obsession scale read full article feedback thought would greatly appreciated regard antreas comment agreed overall point making two cent side note say mlp could actually better transformer given unlimited compute limitation led u think way similarity led transformer opened full article see lot word see result word cheap show u result waste time disagree mlp infinite weight infinite would able potentially match transformer trained amount resource feasible understand real world compute efficiency associative inductive bias well scale translation equivariance invariance staggeringly clear soon start serious comparison say would really fun paper use best modern recipe mlps convnets rnns transformer see compare low medium high regime amount parameter activation flop intuition mlp transformer came paper titled convnext 2022 ig showed older convnets weakness tackled better transformer tried specifically tackle aspect convnets made think supermlp may exist potentially super generalized learner across domain task going diverse also learns identify nuance better leave paper hoped last para",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[D] Confidence _ may be _ all you need..txt",
    "text": "source reddit machinelearning title confidence may need content edit curious know anyone tried practice simple average log probability output token llm might take tell hallucinating idea confident low output token probability may inventing random stuff author claim simple method best heuristic detecting hallucination beauty us generated token probability implemented inference time comment okay let say run compute confidence get ground truth hallucinated know well metric method would predict hallucination also loss extrapolation capability overall negative llm general theoretical issue possible high confidence incorrect past token plus syntactic semantic language approach distinguish manifest lot noise difficult distinguish case initially uncertain talked confidently wrong case low confidence lot different way say thing somewhat better metric would generate bunch answer higher temperature use filter check contradict better job detecting semantic specifically thought general idea well known obvious major subset hallucination issue might genuinely bad information might produce one token throw rest output good starting metric seem perfect practice edit think metric called perplexity called perplexity one basic metric llm generation quality people really publishing repacked version perplexity people used metric long time show probable sequence beam search next token prediction essentially optimizing metric people comment saying perplexity perplexity calculated test show well predicts token test distribution proposed confidence metric use test dataset yeah principle gonna ignore anything coming need format would interesting average property log probability could determine font color output text giving signal need google find paper arxiv id give match get ground truth hallucinated know well metric method would predict hallucination expensively human raters presumably okay let say run compute confidence get ground truth hallucinated know well metric method would predict hallucination indeed need ground truth evaluate method work let say evaluate sample show decent correlation whatever performance metric using use simple method inference time monitor output give proxy performance metric compute inference time ground truth averaging across token within single position rather averaging across position used perplexity perplexity metric regardless calculated test train whatever split either reddit reddit op link edit op edited link sorry bad one expensively getting human raters reliably detect hallucination hard look plausible raters may need domain expert spend lot time looking detail depends openai must gigantic database labelled hallucination try let u know go make post relevant subreddits statement hallucination get lot angry reply correcting",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[D] ICML 2019 Machine Learning Talks.txt",
    "text": "source reddit machinelearning title icml 2019 machine talk content recent advance population based search deep neural network quality diversity indirect encoding open ended algorithm presented jeff clune joel lehman kenneth stanley never ending presented tom mitchell partha talukdar primer pac bayesian presented benjamin guedj john shawe taylor meta shot rapid reinforcement presented chelsea finn sergey levine active theory practice presented robert nowak steve hanneke neural approach conversational presented michel galley jianfeng gao tutorial attention deep presented alex smola aston zhang active hypothesis testing information theoretic view presented tara javidi algorithm configuration space algorithm design presented kevin leyton brown frank hutter u census bureau try good steward 21st century invited talk john abowd best paper award challenging common assumption unsupervised disentangled representation session deep algorithm selectivenet deep neural network integrated reject option manifold mixup better representation interpolating hidden state processing megapixel image deep attention sampling tapnet neural network augmented task adaptive projection shot online meta training neural network local error signal gmnn graph markov neural network self attention graph pooling combating label noise deep using abstention lgm net generate matching network shot session deep reinforcement elf opengo analysis open reimplementation alphazero making deep q method robust time discretization nonlinear distributional gradient temporal difference composing entropic policy using divergence correction tibgm transferable information based graphical approach reinforcement multi agent adversarial inverse reinforcement policy consolidation continual reinforcement policy deep reinforcement without exploration random expert distillation imitation via expert policy support estimation revisiting softmax bellman operator new benefit new perspective session adversarial example adversarial attack node embeddings via graph poisoning first order adversarial vulnerability neural network input dimension certifying non uniform bound adversarial attack improving adversarial robustness via promoting ensemble diversity adversarial camera sticker physical camera based attack deep system adversarial example computational constraint popqorn quantifying robustness recurrent neural network using pre training improve robustness generalized free lunch theorem adversarial robustness proven verifying robustness neural network probabilistic approach session generative adversarial network self attention generative adversarial network multivariate information adversarial ensemble scalable joint distribution matching high fidelity image generation fewer label revisiting precision recall definition generative modeling wasserstein wasserstein loss generative flat metric minimization application generative modeling entropic gans meet vaes statistical approach compute sample likelihood gans non parametric prior generative adversarial network lipschitz generative adversarial net hexagan generative adversarial net real world classification session deep reinforcement investigation free planning curious intrinsically motivated modular multi goal reinforcement task agnostic dynamic prior deep reinforcement collaborative evolutionary reinforcement emi exploration mutual information imitation imperfect demonstration curiosity bottleneck exploration distilling task specific novelty dynamic weight multi objective deep reinforcement fingerprint policy optimisation robust reinforcement session deep theory invariant representation domain adaptation lexicographic depth sensitive margin homogeneous non homogeneous deep adversarial generation time frequency feature application audio synthesis universality invariant network fine grained analysis optimization generalization overparameterized two layer neural network gauge equivariant convolutional network icosahedral cnn feature critic network heterogeneous domain generalization convolve generalized weight tying approach dropout nuclear norm regularization gradient descent find global minimum deep neural network session deep architecture graph matching network similarity graph",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[D] ICML 2019 Machine Learning Talks.txt",
    "text": "structured object bayesnas bayesian approach neural architecture search set transformer framework attention based permutation invariant neural network shallow deep network understanding mitigating network overthinking graph u net satnet bridging deep logical reasoning using differentiable satisfiability solver area attention evolved transformer jumpout improved dropout deep neural network relus stochastic deep network session deep optimization investigation neural net optimization via hessian eigenvalue density differentiable linearized admm adaptive stochastic natural gradient method one shot neural architecture search quantitative analysis effect batch normalization gradient descent effect network width stochastic gradient descent generalization empirical study adagrad stepsizes sharp convergence nonconvex landscape beyond backprop online alternating minimization auxiliary variable swalp stochastic weight averaging low precision training efficient optimization loop limit randomized telescoping sum self similar epoch value arrangement session large scale system composable core set determinant maximization simple near optimal algorithm sublinear time nearest neighbor search generalized weighted space compressing gradient optimizers via count sketch scalable fair clustering conditional gradient method via stochastic path integrated differential estimator fault tolerance iterative convergent machine static automatic batching tensorflow improving neural network quantization without retraining using outlier channel splitting memory optimal direct convolution maximizing classification accuracy embedded application dl2 training querying neural network logic machine robot think fast invited talk aude billard test time award online dictionary sparse coding session deep generative sum square polynomial flow flowavenet generative flow raw audio generative classifier robust adversarial attack gradual semi discrete approach generative network training via explicit wasserstein minimization disentangling disentanglement variational autoencoders eddi efficient dynamic discovery high value information partial vae wrapped normal distribution hyperbolic space gradient based emerging convolution generative normalizing flow large scale study regularization normalization gans variational annealing gans langevin perspective session deep reinforcement social influence intrinsic motivation multi agent deep reinforcement maximum entropy regularized multi goal reinforcement imitating latent policy observation solar deep structured representation based reinforcement dimension wise importance sampling weight clipping sample efficient reinforcement structured agent physical construction novel policy task taming maml efficient unbiased meta reinforcement self supervised exploration via disagreement efficient policy meta reinforcement via probabilistic context variable session adversarial example theoretically principled trade robustness accuracy odds odd statistical test detecting adversarial example net towards effective adversarial robustness matrix estimation certified adversarial robustness via randomized smoothing imperceptible robust targeted adversarial example automatic speech recognition parsimonious black box adversarial attack via efficient combinatorial optimization wasserstein adversarial example via projected sinkhorn iteration transferable clean label poisoning attack deep neural net nattack distribution adversarial example improved black box attack deep neural network simple black box adversarial attack session deep architecture invertible residual network na bench 101 towards reproducible neural architecture search approximated oracle filter pruning destructive cnn width optimization legonet efficient convolutional neural network lego filter sorting lipschitz function approximation graph element network adaptive structured computation memory training cnns selective allocation channel equivariant transformer network overcoming multi forgetting bayesian nonparametric federated neural network session deep reinforcement natural language action control regularization reduced variance reinforcement generalization gap reparameterizable reinforcement trajectory based policy deep reinforcement deep reinforcement perspective internet congestion control based active exploration extrapolating beyond suboptimal",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[D] ICML 2019 Machine Learning Talks.txt",
    "text": "demonstration via inverse reinforcement observation distributional multivariate policy evaluation exploration bellman gan baseline order gradient estimation stochastic computation graph remember forget experience replay session causality causal identification markov equivalence completeness result counterfactual policy evaluation gumbel max structural causal causal discovery forecasting nonstationary environment state space classifying treatment responder causal effect monotonicity measurement error tackling underreporting adjustment criterion generalizing experimental finding conditional independence testing bayesian network sensitivity analysis linear structural causal efficient policy evaluation regularized targeted inferring heterogeneous causal effect presence spatial confounding session representation adversarially learned representation information obfuscation inference adaptive neural tree connectivity optimized representation via persistent homology minimal achievable sufficient statistic route similarity graph invariant equivariant representation multi class infinite mixture prototype shot mixhop higher order graph convolutional architecture via sparsified neighborhood mixing learn grow continual structure framework overcoming catastrophic forgetting session generative tensor variable elimination plated factor graph predicate exchange inference declarative knowledge discriminative regularization latent variable application electrocardiography hierarchical decompositional mixture variational autoencoders finding mixed nash equilibrium generative adversarial network compile compositional imitation execution sparse multi channel variational autoencoder joint analysis heterogeneous deep generative via variational gradient flow flow improving flow based generative variational dequantization architecture design neurosymbolic generative via program synthesis session deep algorithm disagreement help generalization label corruption eigendamage structured pruning kronecker factored eigenbasis addressing loss metric mismatch adaptive loss alignment deep compressed sensing differentiable dynamic normalization deep representation toward understanding importance noise training neural network cheap orthogonal constraint neural network simple parametrization orthogonal unitary group breaking inter layer co adaptation classifier anonymization understanding impact entropy policy optimization probability functional descent unifying perspective gans variational inference reinforcement session deep generative state reification network improving generalization modeling distribution hidden representation variational laplace autoencoders latent normalizing flow discrete sequence multi objective training generative adversarial network multiple discriminator discrete continuous factor via alternating disentanglement bit swap recursive bit back coding lossless compression hierarchical latent variable graphite iterative generative modeling graph hybrid deep invertible feature miwae deep generative modelling imputation incomplete set scalable efficient computation large scale optimal transport session reinforcement batch policy constraint quantifying generalization reinforcement latent dynamic planning pixel projection approximate policy iteration algorithm structured decision problem unawareness calibrated based deep reinforcement reinforcement configurable continuous environment target based temporal difference iterative linearized control stable algorithm complexity guarantee finding option minimize planning time session interpretability neural network attribution causal perspective towards deep unified understanding deep neural nlp explaining deep neural network polynomial time algorithm shapley value approximation functional transparency structured game theoretic approach exploring interpretable lstm neural network multi variable tensorfuzz debugging neural network coverage guided fuzzing gaining free low cost interpretability interpretable partial substitute state regularized recurrent neural network understanding impact high order loss approximation feature deep interpretation connection adversarial robustness saliency map interpretability session deep understanding correcting pathology training learned optimizers demystifying dropout ladder capsule network unreproducible research reproducible geometric scattering graph analysis robust inference via generative classifier handling noisy label lit learned intermediate representation training compression analyzing improving representation soft nearest neighbor loss effect importance weighting deep similarity neural network representation revisited session deep",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[D] ICML 2019 Machine Learning Talks.txt",
    "text": "sequence stochastic beam find gumbel top k trick sampling sequence without replacement exploit long term relational dependency knowledge graph meta neural bloom filter cot cooperative training generative modeling discrete non monotonic sequential text generation insertion transformer flexible sequence generation via insertion operation empirical analysis beam search performance degradation neural sequence trainable decoding set sequence neural sequence generalize sparse underspecified reward efficient training bert progressively stacking session deep theory larger generalize better theoretical perspective via xor problem spectral bias neural network recursive sketch modular deep zero shot knowledge distillation deep network convergence theory deep via parameterization tail index analysis stochastic gradient noise deep neural network approximation non parametric estimation resnet type convolutional neural network global convergence block coordinate descent deep measurement three level hierarchical structure outlier spectrum deepnet hessian limitation representing function set 4 year old yet invited talk alison gopnik best paper award rate convergence sparse variational gaussian process regression session representation fast algorithm linear transforms using butterfly factorization breaking softmax bottleneck via learnable monotonic pointwise non linearity multi object representation iterative variational inference cross domain 3d equivariant image embeddings loss landscape regularized linear autoencoders hyperbolic disk embeddings directed acyclic graph latentgnn efficient non local relation visual recognition robustly disentangled causal mechanism validating deep representation interventional robustness lorentzian distance hyperbolic representation session bandit multiagent decentralized exploration multi armed bandit warm starting contextual bandit robustly combining supervised bandit feedback exploiting structure efficient matroid semi bandit pac identification many good arm stochastic multi armed bandit contextual multi armed bandit algorithm semiparametric reward bayesian action decoder deep multi agent reinforcement tarmac targeted multi agent communication qtran factorize transformation cooperative multi agent reinforcement actor attention critic multi agent reinforcement finite time analysis distributed td 0 linear function approximation multi agent reinforcement session bayesian deep probabilistic neural symbolic interpretable visual question answering nonparametric bayesian deep network local competition good initialization variational bayes deep dropout structured shrinkage prior arsm augment reinforce swap merge estimator gradient backpropagation categorical variable variational bound mutual information partially exchangeable network architecture summary statistic approximate bayesian computation hierarchical importance weighted autoencoders faster attend infer repeat tractable probabilistic understanding prior bayesian neural network unit level workshop generative modeling based reasoning robotics self supervised invited talk yann lecun mental simulation imagination based deep rl invited talk jessica b hamrick bayesian inference identify cause human error efficient based rl unsupervised discovery curiosity driven exploration top bottom approach hierarchical physic manipulation discovering predicting planning object finegan unsupervised hierarchical disentanglement fine grained object generation discovery generalized hidden parameter mdps based meta reinforcement hedge hierarchical event driven generation improved conditional vrnns video prediction improvisation physical understanding using novel object tool visual foresight feedback linearization mf rl high level representation continous experience deep knowledge based agent workshop robustness deep workshop understanding improving generalizing deep daniel roy progress nonvacuous generalization bound chelsea finn training generalization spotlight talk meta analysis overfitting machine spotlight talk uniform convergence may unable explain generalization deep workshop understanding improving generalization deep sham kakade prediction memory mikhail belkin hard look generalization theory spotlight talk towards task architecture",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[D] ICML 2019 Machine Learning Talks.txt",
    "text": "indipendent generalization gap predictor spotlight talk dependent sample complexity deep neural network via lipschitz augmentation workshop generative modeling based reasoning robotics learned invited talk stefan schaal trust based policy optimization based planning energy based perspective object systematic generalization based rl workshop session keynote kilian weinberger calibration fairness relu network yield high confidence prediction far away training mitigate problem detecting extrapolation influence function dense robustness highly sparse representation keynote suchi saria safety challenge black box predictor novel approach failure proofing workshop understanding improving generalization deep invited speaker aleksander dry feature created equal invited speaker jason lee foundation deep sgd overparametrization generalization spotlight talk towards large scale structure loss landscape neural network spotlight talk zero shot scratch leveraging local compositional representation workshop session subspace inference bayesian deep quality quantification bayesian neural network inference bayesian neural network keynote dawn song adversarial machine challenge lesson future direction workshop generative modeling based reasoning robotics value focused invited talk david silver manipulation feel touch based control deep predictive based policy gradient entropy exploration sampling based reinforcement atari predict without looking ahead world without forward prediction physic inverse graphic joint unsupervised object physic video planning explore visual environment without reward precog prediction conditioned goal visual multi agent setting regularizing trajectory optimization denoising autoencoders towards jumpy planning variational temporal abstraction visual planning semi supervised stochastic action representation world program based planning compositional state action space online planning without prior knowledge workshop generative modeling based reasoning robotics online adaptive robotic system byron boot inference perspective based reinforcement reducing noise gan training variance reduced extragradient complexity without losing generality role supervision composition chelsea finn self supervised exploration representation abhinav gupta panel discussion workshop understanding improving generalization deep panel discussion moderator nati srebro overparameterization without overfitting jacobian based generalization guarantee neural network rate delay affect minimum selection asynchronous training neural network toward closing generalization gap workshop self supervised bert pre training deep bidirectional transformer language understanding jacob devlin play self supervised alison gopnik latent plan play corey lynch mohi khansari ted xiao vikash kumar jonathan tompson sergey levine pierre sermanet using self supervised improve robustness dan hendrycks manta mazeika saurav kadavath dawn song workshop identify understanding deep learnign phenomenon optimization untold gift implicit regularization nati srebro bad global minimum exist sgd reach deconstructing lottery ticket zero sign supermask layer created equal study neural network represent function chiyuan zhang workshop exploration reinforcement exploration final frontier doina precup overcoming exploration play corey lynch optimistic exploration pessimistic initialisation tabish rashid scheduled intrinsic drive hierarchical take intrinsically motivated exploration nicolai dorka generative exploration exploitation missing journey reward unsupervised influential trajectory jonathan binas workshop exploration reinforcement sampling exploration control physical system emo todorov benchmarking bonus based exploration method arcade environment adrien taiga simple reget minimzation contextual bandit aniket deshmukh exploration exploration reinforcement pieter abbeel workshop session line attractor dynamic recurrent network sentiment classi cation deep neural network learn shallow learnable example first crowdsourcing deep phenomenon agent set measurable goal chelsea finn workshop session reverse engineering neuroscience cognitive science principle aude oliva understanding hardness sample neural network",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[D] ICML 2019 Machine Learning Talks.txt",
    "text": "convex behavior deep neural network relation layer width intriguing phenomenon training generalization dynamic deep network andrew saxe workshop session self supervised self supervised yann lecun revisiting self supervised visual representation alexander kolesnikov xiaohua zhai lucas beyer efficient image recognition contrastive predictive coding olivier j henaff ali razavi carl doersch ali eslami aaron van den oord workshop session explroation reinforcemnt exploration dangerous world raia hadsell lightning talk curious ilqr resolving based rl sarah bechtle empirical conceptual categorization value based exploration method niko yasui skew fit state covering self supervised reinforcement vitchyr h pong optimistic proximal policy optimization takahisa imagawa exploration unreliable intrinsic reward multi agent reinforcement tabish rashid parameterized exploration lili wu efficient exploration side scrolling video game trajectory replay huan chiang hypothesis driven exploration deep reinforcement caleb chuck epistemic risk sensitive reinforcemnt hannes eriksson near optimal optimistic reinforcement using empriical bernstein inequality aristide tossou improved tree search automatic program synthesis lior wolf mulex disentangling exploration exploitation deep reinforcement olivier teboul workshop session explroation reinforcemnt adapting behaviour via intrinsic reward learn prediction martha white panel discussion martha white jeff clune pulkit agrawal pieter abbeel moderated doina precup workshop session stratagies mitigating social bias deep system olga russakovsky panel discussion kevin murphy nati srebro aude oliva andrew saxe olga russakovsky moderator ali rahimi workshop session self supervised self supervised video sound andrew zisserman supersizing empowering self supervised abhinav gupta revolution supervised alexei efros workshop session deep open set adversarial example deep terrance boult panel discussion moderated tom dietterich thought would put together list machine talk icml 2019 since found kind difficult look facebook figured would share may minor error listing also believe mostly available icml website looking livestreams already posted r reinforcementlearning well comment though understand might always like research many field looking huge list talk induces anxiety list found lot new update stuff interested feel like mere human like never able catch also thanks sharing facebook absolutely awful video talk wonder default end youtube guessing sponsorship clause involved effort neat slideslive much better interface thank organizing deleted always feel way quite sure deal feeling pdf file u without facebook desire nice thank",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[D] My custom DynamicNeuralNetwork hit 2.63 total loss on ARCâ€‘AG1 at 0.6 epochsâ€”projected 78% exactâ€‘match validation bef.txt",
    "text": "source reddit machinelearning title custom dynamicneuralnetwork hit 2 63 total loss arc ag1 0 6 epoch projected 78 exact match validation finishing epoch 1 content hey everyone excited honestly little stunned quickly scratch dynamicneuralnetwork arc agi task built two year fewer 100 gradient update 0 6 full epoch 1 302 example arc training set already achieved total loss 2 63 started 11 cross entropy knowledge distillation loss 2 60 cosine similarity 0 70 teacher combined reward 0 228 healthy scaled entropy 0 196 based curve comparing distilled baseline project hit 78 exact match accuracy held arc validation end epoch 1 163 step bleu 0 90 state art narrow reasoning performance small even finishing one pas simply overfitting memorization balanced ce v kd loss rising cosine alignment robust suggest genuine pattern abstraction happening faster comparable distilled architecture seen sharing believe phillnet2 early trajectory represents meaningful advance narrow generalization introduce phillnet2 dynamicneuralnetwork without prior exposure arc agi phillnet2 distilled knowledge teacher achieved total training loss 2 63 0 6 epoch 97 step arc agi training set key metric point include balanced cross entropy knowledge distillation loss 2 60 cosine similarity 0 70 teacher hidden representation combined reward 0 228 exceeding typical baseline performance forecast held exact match accuracy 78 end epoch 1 surpassing state art distilled arc result suggest phillnet2 rapidly internalizes complex reasoning pattern marking substantial leap narrow generalization capability comment nothing way communicating inspires confidence lmao net might able understand going apart extrapolating unfortunately human get excited clue feel lot joy forgot leave inspiration lol basically like jarvis able plan internally reason left way took make main point fully cognitive dynamic let say put robot could learn human time without intervention due continuous fact able generalize arc agi testing reaching full episode crazy promising proving continuous better anticipated crazy thank took two year change lol",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[D] RL_ GANs as MCTS environment simulator for deep model-based planning_.txt",
    "text": "source reddit machinelearning title rl gans mcts environment simulator deep based planning content apropos news alphago recurrent environment simulator guo et al 2014 deep real time atari game play using offline monte carlo tree search mcts planning desai bannerjee 2017 deep reinforcement play space invader mathieu et al 2017 deep multi scale video prediction beyond mean square error lecun talk unsupervised rl wondering something anyone proposed done work using generative adversarial network forward planning tree search method like monte carlo tree search seems like people working towards bring gans rl related talk recall anyone explicitly suggesting gans would useful specific approach searching turn either go gans approximate distribution wasserstein gan arjovsky et al 2017 drawing sample gan trained sequential timesteps constitute distribution true environment distribution weighted likelihood sampling one could combine mcts evaluate deeply decision tree estimate value action choose optimal next action mcts gan rl agent would inherit strength gans mcts simple implementation able learn policy transition sample create deep environment long term planning quantify better exploration provide time estimate run parallel etc could extended using dropout trained gans bootstrapped deep exploration via randomized value function osband et al 2017 gans deep exploration training gan taking action true environment based mcts gan estimate using newly collected sample retrain gan mcts gan value estimate could also used distill pre train fast reactive deep rl agent like dqn a3c providing high quality transition sample highly accurate advantage function note sure gan use strictly necessary pixelcnn appears competitive gans modeling visual distribution perhaps approach would also work go back lecun talk one point rl want sort based planning look ahead free non planning sample inefficient get anywhere although tends fast advantage human see fairly clear looking difference immediate system reflex intuition slower system ii explicit planning thinking possible future exploring little environment head deep predict future show example car take cnn predict subsequent frame using something like rmse loss autoencoders lead blurry image immediately average pixel possible future helpful planning since blur tell anything action take future multimodal autoencoder produce average lecun demonstrates gans predicting next frame sample sharp pick one possible future depicts exactly possible useful either probably future highest likelihood maximum likelihood still unlikely step exact sequence becomes vanishingly unlikely plus ignores expected value slightly less likely alternative need planning entire distribution single sample fortunately gan provide u entire distribution future weighted likelihood via z vector create say 1000 unique set uniform deviate feed gan get approximation distribution future one sharp 500 future car continues forward average 500 sample slight variant car moving forward perhaps another 250 turning left another 250 turning right start planning note 25 chance turning towards u coming unacceptably close potentially huge negative reward slow something would get autoencoder would show expanding blur grayness single sample gan would usually safely show turn going straight turning away u account action changing environment turn predictive causal would make gan conditional addition z vector provide action immediate reward rather terminal reward gan prediction new environment state state immediate reward",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[D] RL_ GANs as MCTS environment simulator for deep model-based planning_.txt",
    "text": "require knowledge total value state policy followed since future agent done working backwards eventual terminal state summing discounted 1 step reward sharp simulator environment well plug decision tree enumerate possible sequence action stochastic outcome backwards induction figure optimal action outcome hard approximate mcts simulator provides possible outcome action fed back simulator explore another ply hitting depth limit exploring randomly terminal state point cumulative reward node estimated another rollout begin whole algorithm go 1 create dataset environment reward action sample perhaps following random policy ale game expert human trajectory 2 train convergence gan like improved wgan improved training wasserstein gans gulrajani et al 2017 approximate distribution new environment conditional old environment action noise z vector 3 n game parallel initialize rl agent environment termination take action according mcts using gan environment simulator drawing relatively small number 5 100 sample similar go branching factor adding environment transition reward action dataset 4 go 2 could probably implemented fairly easily using gym improved wgan parallelizable since agent act independently stochastically gan trained parallel experience replay buffer learn policy since gan focused immediate state transition reward depend successive action following better worse policy full value estimated mcts search backwards induction keep maintaining sharp environment state since individual gan sample represents 1 possible future ensemble average capable exploring deeply using mcts prioritize promising line action capable handling environment dqn a3c like ale capable handling sequence given recent work improved wgan gans discrete sequential anytime since rollout update value estimate incrementally major weakness sure well mcts part would handle continuous action hybrid algorithm seems fall closer dqn a3c taxonomy also potential curse dimensionality many drawn gan sample morally equivalent gan reflecting basic causal change slight appearance difference despite mcts usual ability handle extremely high branching factor like go might wind killing approach comment tried something like using gradient descent latent space rather mcts long story short three thing go wrong require technical theoretical improvement one noted may properly causal latent space index controllable variation also index uncontrollable variation something like minimax search needed pessimistic control optimistic much issue deterministic setting though second even environment deterministic may distribute error uniform way throughout latent space case search simply find point make right kind error rather actual correct policy furthermore type error made matching distribution versus making accurate future prediction different implication continuous valued control problem example slightly billiard shot make big difference arranging particular outcome error correspond fairly plausible future gan type setup penalize strongly third training stability gans generative source distribution non stationary rl algorithm issue tendency gans fluctuate even stationary problem seemed make much worse catastrophic collapse tried result needed use non adversarial generative instead stable still sometimes exhibit collapse building good forward via gans otherwise pretty hard challenge many u tackling order based planning likely plan use mcts based stuff post good write precise granular detail thinking one build good forward either input space state space 1 input space given previous frame pixel action predict next frame pixel extendable previous n frame n action next frame",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[D] RL_ GANs as MCTS environment simulator for deep model-based planning_.txt",
    "text": "2 state space given previous hidden state embedding action predict next hidden state harder debug prototyping tangible 1 trying 1 2 via gans success really limited ust gans deepmind showed build recurrent prediction plain old pixel wise mse like loss trick recurrent environment simulator video1 video2 folk honglak lee showed reasonably compelling result well limited 2d synthetic environment atari stuff action conditional video prediction using deep network atari game lastly elevated rise paper use prior environment build better forward rather thing fully unsupervised one build forward top pixel human pose estimate fundamental problem building good forward long term coherency catastrophically forget happened past subtle pixel wise error compound problem tackle similar see elsewhere like language modeling take explicit memory fast search go long way though compelling published work direction yet reason seen whole algorithm someone building forward using mcts planning yet convincing large scale application forward dont work yet people still trying make work relevant new paper multimodal transition dynamic based reinforcement moerland et al 2017 repo paper study learn stochastic multimodal transition dynamic based reinforcement rl task stochasticity fundamental property many task environment however function approximation based mean squared error fails approximating multimodal stochasticity contrast deep generative capture complex high dimensional outcome distribution first discus amongst conditional variational inference vi theoretically appealing sample based planning based rl subsequently study different vi identify ability learn complex stochasticity simulated function well typical rl gridworld strongly multimodal dynamic importantly simulation show vi network successfully us stochastic latent network node predict multimodal outcome also robustly ignores deterministic part transition dynamic summary show robust method learn multimodal transition using function approximation key preliminary based rl stochastic domain make number point need deep based planning usefulness generative forward planning demonstrate vae network make sharp correct rollouts small gridworld policy transition free approach mention might go well mcts argue vae pixelcnn better gans purpose produce explicit likelihood function rather implicit sampling based one question spending time type instead typing code seeing work also lecun might shown applicability gans everything else including use deep net new bot bleep bloop someone linked thread another place reddit r reinforcementlearning gans mcts environment simulator deep based planning footer follow link please respect rule reddit vote thread info r totesmessenger contact message compose r totesmessenger bot deleted yeah saw earlier paper really get still 1 sure mean sample gan conditioned previous state hypothetical next action sample collectively cover combination effect action well inherent stochascity hidden variable environment 2 sure optimistic pessimistic error gan overly optimistic poorly modeled region environment dynamic running tend run lot transition retraining gan learn state provide many immediate reward expected successor state predicting seems self correcting rl algorithm think something valuable get correct prof practice valuable example slightly billiard shot make big difference arranging particular outcome error correspond fairly plausible future gan type setup penalize strongly right thing inherently hard predict sharp affected small difference get spread plausible future lead wide variety outcome show great go issue long range global board outcome sensitive slight difference positioning individual",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[D] RL_ GANs as MCTS environment simulator for deep model-based planning_.txt",
    "text": "stone mcts help alphago much exploring precisely line play 3 gan stability issue seems fairly solved regular wgan often saw converge good result monkeying around hardly ever ever saw spectacular mode collapse divergence way saw daily dcgan torch gan try earlier wgan maybe issue answering narrowly forward gans wgans gans saw huge amount mode collapse 3 month ago people still building forward using adversarial loss hope good gans think sampling work mcts personally sure gans need much emphasis magic tool think gated auto encoders give equally sharp result even brute force space explicit memory fast search another one also us vaes gans human pose particular vaes always strike bit problematic sample look smooth nobody know really really pity nothing like resource hungry pixelcnn maybe moore law solve seems must something sub linear rather sequentially sampling pixel pixel something like bayesian neural network question spending time type instead typing code seeing work flattered think could execute project couple minute hour reality still bad python tf take 15 minute type idea see anyone tried say easy mean would week month work one person 1 2 gpus compared say project like alphago requiring dozen people 2 year hundred gpu spending much time idea good run past people see anyone done anything similar anything obviously wrong apparently least 2 people thinking along similar line useful know kill deepminders alive understand kind thinking learns p 1 chain together sample etc found via tree search difference sampling p n 1 n directly rather series one step transition since benefit gans generative sort able match distribution high dimensional space thought one shot would basically mean certain coherent pattern action something whole individually important could learn represent entire action sequence compressed way kind sequence basically rediscovered every time build future one action time hoped would see improvement method sort natural h dqn tried pacman one point problem learn pretty good frame generator gan instability killing u go alternate method suffers sort blur problem pure vaes make unsuitable visual reconstruction redo research 9 month later guess shape like recent recurrent environment simulator paper everything control policy part thing take place nicely partitioned latent space encoding decoding frame happens start end anyhow specific point 1 causal issue clear sample joint distribution action state given latent parameterizes possible variation goal variation environmental noise source even 1 step conditional related causal problem case environment simple markov process term single step example past action influence unobserved hidden variable etc least potentially problematic bit tricky diagnose though certainly none applied cartpole example either joint distribution conditional one cautionary thing 2 thing generator would optimistic particular environmental transition rather region latent space low density region latent space new observational generated think something necessarily end landing region latent space may self correcting imposing prior latent space distribution via regularizing gradient descent helped limit saw major problem latent space dimension became comparable task sensor dimension one particularly severe cartpole 4 sensor forced u use 2d latent space term question error distribution actually billiards problem one point see",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[D] RL_ GANs as MCTS environment simulator for deep model-based planning_.txt",
    "text": "thing worked found error specific future trajectory time higher modelling future generative way rather regressing specific next state conditioned action meant generative could barely manage make contact ball whereas regression based able less nudge target towards supposed go 3 yes good time revisit specific algorithmic choice wgan gp began particular plain old gan anyhow think pulling action joint distribution interesting idea capturing long term correlation action pattern mcts action conditional regression generation probably better 99 case right said think still something done joint distribution action state stuff maybe joint distribution approach would better case lot expert since actually starting nearly random action policy get much compression generating action sequence already low dimensional manifold coming expert play index generatively seems like would make sense anyhow thanks write clarified thought matter became bit long winded though could elaborate mean memory fast search thinking sort associative memory multiple state recorded literally indexed prior state action many experienced state prior state action due stochasticity forward planning avoids expensive lossy state action pas gan forward favor fast lookup one exact literal memory sample really pity nothing better resource hungry pixelcnn maybe moore law solve seems must something sub linear rather sequentially sampling pixel pixel always going generate pixel getting around necessarily single pixel time willing force locality multi scale eg dilated convolution parallel multiscale autoregressive density estimation reed et al 2017 meant seems like reasonable idea pursue would go posting around deleted understand kind thinking learns p 1 chain together sample etc found via tree search maybe quite p notation implies calculating likelihood probability particular 1 planning want able draw sample full distribution 1 even 1 step conditional related causal problem case environment simple markov process term single step example past action influence unobserved hidden variable etc least potentially problematic think deep rl approach moment bit questionable pomdp setting without switching rnn controller nothing special always going generate pixel argue one could also take care stochasticity using much lower dimensional embedding space condition generator sampled hypothesis produce output especially complex hidden variable like behavior agent sparse low dimensional representation seems much efficient making prediction operating via pixel space seems like bottleneck oh well planning working anytime soon want work intro getting hand dirty deep image tagging cnns less research applied someone else mcts gan would great something like idea simple deepmind guy would already working stopped",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[D] Trying to understand Concept learning _ Some questions based on Tom Mitchell Chapter 2.txt",
    "text": "source reddit machinelearning title trying understand concept question based tom mitchell chapter 2 content hi im going tom mitchell machine couple question based 2nd chapter concept hoping could get external point view 1 pg 44 para 2 part 1 advantage viewing inductive inference system term inductive bias provides nonprocedural mean characterizing policy generalizing general procedure identify validate inductive bias system guideline ensure inferred definition inductive bias without error assuming predictive algorithm defined term inductive bias concentrating choosing algorithm aligns philosophy talking problem weigh part inductive bias originating implementation architecture v procedure algorithm 1 pg 44 para 2 part 2 second advantage allows comparison different learner according strength inductive bias employ example given text comparison seems qualitative nature could method qualitative measure strength although number individual assumption number training instance ratio seems like something used however assumption count directly proportionate strength generalization case use size version space define version space hypothesis space reinforcement estimate size version space rl algorithm self answering hypothesis space rl set possible policy given state action space policy based method set possible value function given state action space based definition generalized v policy based rl would set possible policy consistent training step similar definition value based rl method size h v bounded state action space quantifiable discrete space however infinite continuous space comparing size v directly sound like feasible option many case additionally using v comparison might make sense training generated expert maybe like imitation make sense exploratory system rl method apart ensemble technique maintain single hypothesis time therefore comparison size v cannot done paper inductive bias deep reinforcement hessel et al quote stronger bias lead faster weaker bias potentially lead general algorithm general algorithm refer something generalize well align example rote learner well text book quote page 45 para 2 strongly biased method make inductive leap classifying greater proportion unseen instance e generalize better definition weak general paper refer particular kind algorithm gone paper completely yet general learner paper refer general hypothesis seem version space v v 1 training larger general necessarily better another slightly smaller version space v 2 belong different algorithm different configuration algorithm reason large v higher chance misclassification similar small v determine algorithm better look respect respective efficiency convergence hypothesis space asymptotically sample efficiency measuring loss sense describes size current v wrt hypothesis space training algorithm also seems like utility strength inductive bias could lever solution design e liberal constrained want solution association seen example depending problem area would measuring loss also measure regarding identity target concept strength inductive bias identifying target concept algorithm comment",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[D] Using LLMs to extract knowledge graphs from tables for retrieval-augmented methods â€” promising or just recursion_.txt",
    "text": "source reddit machinelearning title using llm extract knowledge graph table retrieval augmented method promising recursion content thinking approach large language used extract structured knowledge e g table spreadsheet database transform knowledge graph kg use kg within retrieval augmented generation rag setup support reasoning reduce hallucination tricky part feel bit like llm generating almost recursive one hand structured knowledge could help llm reason better hand extraction relies llm stacking uncertainty love hear community thought see viable research application direction like dead end promising framework paper tackling self extraction rag llm pipeline see biggest bottleneck scalability accuracy extraction reasoning limit curious know anyone tried something along line comment google researching graph foundation unwrapping table row natural language would make way indexable vector database kg might overkill adding extra latency chatbots need low latency possible retain user feel redundant essentially weighted parameter job fail see purpose unless find different way defining term quality quantity processing team build ton graph processing pipeline better bang buck llm box fundamentally speaking want graph specify relationship extract appropriately best way imo chunk 2k 4k token overlap chunk define relation normalized form idea split one full relationship extraction across multiple prompt containing pk fk relationship join later form full set bonus use small chip run long period time compute cost inhibiting factor graph subpar lot information always complete every relationship use graph lot work think hundred thousand text sample start non llm pattern idea got great set relationship graph shine could enlist output graph oriented approach fill llm system prompt remember output approach creating query agent graph multi hop multi index query great breaking thing making faux think mode example searching document x two pronged query graph node related x relation refine relationship pick document chunk group pattern graph highly underrated query breakdown think formulation need oriented borrow idea use llm fit required context idea get always reworked formed graph work great especially statistical spatial pattern finding chain condensation community detection aspect grouping etc idea unlock borrow even give graph query agent curate existing graph looked graphrag unstructured source thanks gfm pointer check thanks idea thought well however believe simply translating nl might lose schema course tried yet result based experimentation parameter indeed encode knowledge explicit kg help reduce hallucination forcing retrieval entity relationship kg support structured reasoning graph algorithm traceability clear source thanks sharing experience insightful agree explicitly defining extracting relationship crucial chunking normalization approach make sense large scale also make reflect balance completeness graph contextual flexibility llm offer whether hybrid approach effectively leverage understand graphrag mainly targeted unstructured focused structured table database relatively less research provide cleaner schema saw instant 5 rag accuracy bump back 2008 worked team trying build project called nepomuk called semantic desktop web 3 0 time look literature related find interesting concept given much thought may onto something using llm attribute semantic value term based managed rag pipeline impressive seems try soon possible thank definitely look nepomuk literature see concept could relevant",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[Project] Library for offline model-based reinforcement learning.txt",
    "text": "source reddit machinelearning title project library offline based reinforcement content year half ago finished master thesis estimation offline based reinforcement revisited code past couple week turned simple high quality ml standard baseline playing around free based reinforcement approach online offline setting mostly tested type hinted documented link repo removed stuff like hyperparameter tuning ray tune fairly easy implement different training scheme environment agent top existing code feel free leave feedback issue repository comment great project could reproduce result mopo morel find repo could reproduce morel result scratch read somewhere unstable wrt hyperparameters case would great add best hyperparameter configuration environment algorithm result obtained either readme doc irrespective whether match result original paper yes got approach work added figure showing training result also added section hyperparameter tuning still stuff lying around produced previous version code tag 1 0 0 pm send thesis interested",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[P] Data Efficient Reinforcement Learning with Probabilistic Model Predictive Control_ Opensource Implementation.txt",
    "text": "source reddit machinelearning title p efficient reinforcement probabilistic predictive control opensource implementation content hi everyone implemented paper efficient reinforcement probabilistic predictive control thought might interest people abstract paper trial error based reinforcement rl seen rapid advancement recent time especially advent deep neural network however majority autonomous rl algorithm either rely engineered feature large number interaction environment large number interaction may impractical many real world application example robot subject wear tear hence million interaction may change damage system moreover practical system limitation form maximum torque safely applied reduce number system interaction naturally handling constraint propose based rl framework based predictive control mpc particular propose learn probabilistic transition using gaussian process gps incorporate uncertainty long term prediction thereby reducing impact error use mpc find control sequence minimises expected long term cost provide theoretical guarantee first order optimality gp based transition deterministic approximate inference long term planning proposed framework demonstrates superior efficiency rate compared current state art result obtained pendulum v0 task openai gym see happening real time past predicted state action loss learned gaussian process point memory top value bottom uncertainty resulting animation code documentation github want information run test limitation method controlled environment must low dimension state action controlled environment must continuous action computation time iteration important depends length planning horizon reward loss function must defined function depending state action paper important currently real world application free reinforcement algorithm limited due number interaction require environment debate within reinforcement community use based reinforcement algorithm improve sample efficiency extent improve sample efficiency limitation method present show application used state art free algorithm extent knowledge done 20 time less interaction environment test used increased efficiency explained different reason open search algorithm improvement sample efficiency without limitation mentioned example future predicted reward loss predicted distribution maximizing upper confidence limit reward future state high reward encouraged allowing effective exploration maximizing future state could also used explore environment without reward future research remove limitation type efficiency could used real world application real time required thus open many new application possibility comment really good job wondering technique used physical science mean example may used optimization problem right amazing great summary thank simple optimization look bayesian optimization method us gaussian process possibly advanced way select next point try one example application optimizing material property testing different material composition trial possible also look video visual explanation used optimize drone controller parameter interesting similar example optimization especially rl used finding process parameter think need read understand difference diferent method thanks sharing",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[P] Project Feedback Request_ Tackling Catastrophic Forgetting with a Modular LLM Approach (PEFT Router + CL).txt",
    "text": "source reddit machinelearning title p project feedback request tackling catastrophic forgetting modular llm approach peft router cl content feedback request tackling catastrophic forgetting modular llm approach peft router cl working project conceived researched designed coded llm background field frankly head anyone could read project outline provide feedback thrilled everything created beginning output hi r machinelearning working project focused enabling large language currently experimenting gemma 2b learn sequence diverse nlp task continually without catastrophic forgetting core system involves frozen llm backbone dynamic management parameter efficient fine tuning peft module specifically loras via trainable peft router scaffold also includes standard cl technique like ewc generative replay high level approach new task introduced system aim 1 represent task using feature initially task description exploring richer feature like example based prototype 2 peft router select appropriate existing lora module reuse adapt decide create new lora suitable one found 3 train adapt chosen new lora current task 4 employ ewc replay mitigate forgetting lora module current status key challenge router intelligence built functional end end simulation successfully run multi task sequence e g sst 2 mrpc qnli key cl mechanism like lora management stateful router loading saving ewc replay working even seen promising result single lora reuse managed system adapted well across multiple task positive backward transfer likely due effective ewc replay however main challenge hitting intelligence reliability peft router decision making initially using task description embeddings router struggled discrimination produced low undifferentiated confidence score softmax cosine similarity known lora profile recently experimented richer router input concatenating task description embeddings averaged embeddings task example k 3 also implemented clean router training phase step c fresh router trained rich feature forcing new lora creation task tested router step loading state observation even richer feature router trained specifically operating clean initial set trained profile router still often fails confidently select correct specialized lora reuse known task type presented frequently default creating new loras confidence reusing specialized previously trained profile surpass moderate threshold e g 0 4 confidence score softmax still seem low peaky enough correct choice seeking insight discussion 1 improving router discrimination rich feature example prototype step common pitfall advanced robust way represent task lora module specialization router consider gradient sketch context stats dynamic expert embeddings 2 router architecture decision mechanism current router linearrouter cosine similarity learned profile embeddings softmax threshold given continued challenge even richer feature clean profile set architecture simplistic common alternative type dynamic expert selection better handle feature interaction provide robust confidence 3 confidence calibration thresholding reuse decision confidence slide softmax pool potential even selected expert grows concern beyond temperature scaling plan try established best practice alternative decision mechanism e g focusing absolute similarity score learned decision function adaptive threshold based router like entropy margin particularly effective dynamic growing expert pool scenario 4 router training critical router training regimen e g number epoch negative example online v offline update using complex input feature current approach 1 5 epoch training currently active task lora pair main task goal build router make truly intelligent confident",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[P] Project Feedback Request_ Tackling Catastrophic Forgetting with a Modular LLM Approach (PEFT Router + CL).txt",
    "text": "reuse decision trying avoid scenario system keep creating new loras due perpetual low confidence would undermine benefit router optional pursuing project largely assistance llm conceptualization research coding interesting journey pointer relevant research common pitfall general advice aspect would greatly appreciated thanks time end output slop actually something merit wasting time feedback would great galileo82 comment look interesting actually implemented something similar bert style classifier open source project adaptive classifier enables user use classifier without fine tuning also us ewc see implementation may want think going evaluate kind task test first key would demonstrate improvement existing technique thought getting stuck wanted give response generated response reddit friendly formatting relevant helpful reply reddit user operator great see someone point open source project share conceptual similarity also us ewc key aspect adaptive classifier ewc implementation 1 standard ewc formulation store old params previous task parameter computes fisher information matrix classic penalty python ewc loss ewc lambda fisher param new param old param 2 2 fisher computation compute fisher us dataset eval stability sample label prediction python output batch embeddings probs f softmax output dim 1 log p f log softmax output dim 1 sampled label torch multinomial probs 1 squeeze loss f nll loss log p sampled label loss backward fisher n p grad 2 len loader 3 ewc loss application ewc loss added main task loss back prop optional batch size normalisation comparison adaptive learner ewc aspect adaptive classifier adaptive learner store old params old params value fisher score fisher info importance score penalty added loss pre backward gradient post backward label fisher sampled prediction true label replay buffer scope whole lora specific extra control gamma gain modulation key difference takeaway fisher label adaptive classifier sampled label reflects belief useful true label noisy adaptive learner true label better preserving specific replay knowledge current choice fit generative lora setup penalty application loss level v gradient level mathematically equivalent method fine modularity control adaptive learner ewc lora targeted gamma gated finer adaptation evaluation advice reddit user think going evaluate kind task test first key would demonstrate improvement existing technique plan run adaptive learner gauntlet diverse set standard task strong baseline single task sft multitask etc bottom line ewc logic sound tailored modular loras reference implementation validates approach suggests alternative fisher estimation experiment later next step keep step c v3 training collect result refine evaluation pipeline copy paste ready render cleanly reddit follow already demonstrated improvement existing technique generated response reddit friendly formatting follow question already demonstrated improvement existing technique shown clear progress 1 continual scaffold lora ewc replay run setup avgacc bwt take away step b v4 single lora reused across 3 task sst 2 mrpc qnli gated ewc replay 0 7933 0 0400 positive backward transfer slightly higher avgacc new lora per task baseline matter positive bwt 0 04 gained past task performance instead forgetting beat internal force new lora baseline step retry avgacc 0 7800 bwt 0 00 naive sequential fine tune without ewc replay would almost certainly crash avgacc give negative",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[P] Project Feedback Request_ Tackling Catastrophic Forgetting with a Modular LLM Approach (PEFT Router + CL).txt",
    "text": "bwt 2 robust profile dilution fix clean router state initialisation text peftmanager init disk scan router 0 learnable profile peftmanager router 3 learnable profile eliminates hidden ghost profile stable fair router evaluation matched beaten existing technique yet 1 router intelligence confident reuse step run router defaulted creating new loras confidence never crossed reuse threshold result good metric effectively force new scenario proof router outperforms simple strategy 2 benchmark leaderboards sota individual task score e g sst 2 0 94 1 00 solid gemma 2b lora necessarily sota main claim cl robustness avgacc bwt single task peak yet sota competitive single task performance still prerequisite take aways next step yes scaffold lora gated ewc replay mitigates forgetting even transfer knowledge see step b v4 router proven smarter always spin new lora present bottleneck still need 1 smarter routing logic gradient sketch metric learned calibrators ucb adaptive threshold etc 2 head head degraded baseline run truly naive sequential lora w ewc replay quantify gain 3 expanded evaluation gauntlet diverse task sequence stronger baseline ablation bottom line shown promising continual gain inside scaffold need much smarter brain router decision module unlock reliable lora reuse truly outperform existing technique across board copy paste ready render cleanly reddit sigh people understand llm think predictive based input prompt going spend time demonstrating nonsense going ask language exactly predicted",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[P] UQLM_ Uncertainty Quantification for Language Models.txt",
    "text": "source reddit machinelearning title p uqlm quantification language content sharing new open source python package generation time zero resource hallucination detection called uqlm leverage state art quantification technique academic literature compute response level confidence score based response consistency multiple response prompt token probability llm judge ensemble check share feedback reach want contribute comment",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[R] azzurra-voice, a new State-of-the-Art Italian Text-to-Speech model.txt",
    "text": "source reddit machinelearning title r azzurra voice new state art italian text speech content hey r machinelearning cartesia small research lab based italy believe future processing command creating genuine connection vision build agent private personal feel culturally present today excited share first step open source community azzurra voice azzurra voice highly expressive natural sounding text speech tt italian language trained thousand hour high quality diverse italian speech worked hard capture accent intonation real life conversational pattern across italy avoid robotic monotone sound listen audio sample comparing azzurra voice open blog post comment congrats guy look amazing thank sharing opening community",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[R] Bootstrapped model learning and error correction for planning with uncertainty in model-based RL.txt",
    "text": "source reddit machinelearning title r bootstrapped error correction planning based rl content comment title bootstrapped error correction planning based rl author alvaro ovalle simon lucas abstract access forward enables use planning algorithm monte carlo tree search rolling horizon evolution unavailable natural aim learn reflects accurately dynamic environment many situation might possible minimal glitch may lead poor performance failure paper explores problem misspecification aware reinforcement agent propose bootstrapped multi headed neural network learns distribution future state reward experiment number scheme extract likely prediction moreover also introduce global error correction filter applies high level constraint guided context provided predictive distribution illustrate approach minipacman evaluation demonstrates dealing imperfect method exhibit increased performance stability term accuracy use within planning algorithm pdf link landing page read web page arxiv vanity",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[R] Decoding LLM Uncertainties for Better Predictability.txt",
    "text": "source reddit machinelearning title r decoding llm uncertainty better predictability content hi building last research post wanted figure way quantify ambiguity prompt response llm ended discovering two useful form structural conceptual nutshell conceptual sure say structural sure say play around demo read detail blog post comment cool work minor nit please consistent ordering conceptual structural perhaps consider new name structural communicative love investigation gave new perspective something quite put finger good standard trick ml one even use detect potential hallucination within llm surprised one tried repo link dead also use demo link actual llm using good note thought structural really able come something felt fit well continue noodle yeah seen vanilla entropy perplexity measure used found tell part story e g llm might spread logprobs evenly across set token really impact underlying meaning response entropy high imagine implies core position splitting structural conceptual ended aligning lot better human intuition ah sorry realized repo private made public demo wired gpt 3 5 turbo instruct directly apply approach llm long offer logprobs top n sampled token quick question relation aleatoric epistemic",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[R] Grammars of Formal Uncertainty_ When to Trust LLMs in Automated Reasoning Tasks.txt",
    "text": "source reddit machinelearning title r grammar formal trust llm automated reasoning task content large language llm show remarkable promise democratizing automated reasoning generating formal specification however fundamental tension exists llm probabilistic formal verification demand deterministic guarantee paper address epistemological gap comprehensively investigating failure mode quantification uq llm generated formal artifact systematic evaluation five frontier llm reveals satisfiability modulo theory smt based autoformalization domain specific impact accuracy 34 8 logical task 44 5 factual one known uq technique like entropy token probability failing identify error introduce probabilistic context free grammar pcfg framework llm output yielding refined taxonomy find signal task dependent e g grammar entropy logic auroc 0 93 finally lightweight fusion signal enables selective verification drastically reducing error 14 100 minimal abstention transforming llm driven formalization reliable engineering discipline comment paper bridge gap probabilistic llm deterministic formal verification quantifying generated formal artifact could get eli5 wow amazing work nice bridge probabilistic grammar llm see coming computer vision stuff",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[R] Graph ML benchmarks and foundation models.txt",
    "text": "source reddit machinelearning title r graph ml benchmark foundation content team recently published two graph ml paper one new realistic benchmark second one graph foundation related tabular foundation graphland benchmark paper code widely discussed community graph machine suffers lack realistic meaningful reliable diverse benchmark agree hope improve situation recent paper graphland evaluating graph machine diverse industrial graphland benchmark 14 diverse graph datasets node property prediction classification regression different industrial application datasets cover realistic machine problem come rich numerical categorical node feature common real world application importantly besides standard random split graphland provides split temporal distributional shift inductive prediction setting enable evaluating gnns realistic challenging scenario graphland benchmark datasets evaluated wide range graphland includes several openly available graph foundation gfms found provide weak performance compared classical gnns thus set develop better gfm led u next paper turning tabular foundation graph foundation paper code graph may come different domain thus may diverse feature varying across datasets result one key challenge gfms deal diverse heterogeneous feature prior study fully address issue often limiting text attributed graph relying simple technique like pca svd however challenge unique graph domain tabular domain face exactly issue recent tabular foundation like tabpfnv2 successfully deal decided transfer success graph g2t fm framework framework g2t fm graph table foundation augment original feature graph information computing neighborhood feature aggregation structure based encoding essentially transforming graph task tabular task g2t apply tabpfnv2 augmented feature get prediction g2t fm result evaluated g2t fm graphland several graph datasets found show strong performance context finetuning setting particular g2t fm outperforms well tuned classic gnns trained scratch prior publicly available gfms hope work help develop better gfms highlight graph community similarity graph tabular domain prospect utilizing tabular foundation graph task comment glad graph world able interpret number seems seems lightgbm nfa strong",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[R] Identifying Critical Decision Points in Neural Text Generation Through Token-Level Uncertainty Analysis.txt",
    "text": "source reddit machinelearning title r identifying critical decision point neural text generation token level analysis content paper introduces framework analyzing visualizing branching decision language make text generation key methodology involves tracking probability distribution across different sampling path understand early choice affect downstream generation main technical point developed metric quantify generation step created visualization tool mapping decision tree generation analyzed different sampling method affect path divergence measured correlation confidence generation quality identified clustering pattern generation trajectory key result found path tend cluster 2 3 distinct trajectory group early sampling decision outsized impact final output pattern vary significantly sampling method similar prompt lead dramatically different generation path confidence consistently predict output quality think work provides important insight might better control text generation ability map understand generation path could help develop reliable sampling method better estimate think clustering generation path particularly interesting suggests may way guide generation toward desired trajectory group could valuable application needing predictable output methodology also reveals concerning aspect current sampling method strong dependence early decision suggests may need new approach better preserve generation flexibility throughout sequence tldr new framework analyzing language make text generation choice show generation path cluster distinct group early decision heavily influence outcome could help develop better sampling method estimate full summary paper comment nice thank sharing",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[R] Investigation about neural networks and OOP.txt",
    "text": "source reddit machinelearning title r investigation neural network oop content nvestigaci n sobre redes neuronales hecho este descubrimiento que quiero compartir necesito sugerencias simple look neural architecture inspired object oriented programming towards hybrid neural probabilistic litus ramos march 1 introduction hi name litus want suggest way combine object oriented programming neural network idea explore could help create hybrid use neural network probabilistic method 2 idea behind started thinking neural network set especially neuron layer connected layer build backpropagation information like error gradient move one layer another training creating flow knowledge got thinking look neural network like object oriented programming oop oop class inherit property behavior class make easy reuse organize code neural network also kind hierarchy layer build work done layer like child class inheriting parent class layer pas complex information still depending basic work done previous layer made think could apply oop idea improve neural network set 3 possible us using idea oop neural network might design flexible modular network mean layer could specialize one task still benefit previous layer done one potential application could hybrid mix neural network probabilistic would allow learn also deal could help task prediction need handle noise like reinforcement probabilistic programming type hybrid 1model could work better situation unclear incomplete another possible application could involve making easier understand oop inheritance make clear different class connected apply neural network could create easier see layer could helpful area like medicine finance understanding make decision important moreover could apply neural symbolic system combine neural network symbolic reasoning could improve ability solve complex problem need logical reasoning finally making network modular could help transfer trained one task easily adapted different one could make training faster reduce amount needed 4 challenge uncertainty challenge uncertainty although idea interesting still several challenge solve sure put everything together yet idea seem clear making work practice require research testing big challenge figuring design layer inherit information properly also combining probabilistic neural network easy requires deeper understanding field another challenge combining symbolic reasoning neural network hard link continuous driven part neural network structured logical reasoning used symbolic system finding way make approach work well together tough problem even though answer right believe idea could lead important discovery creating flexible understandable promising goal time research hope solve problem 5 conclusion conclusion combining object oriented programming idea neural net work still early stage could change design improve neural architecture could make network modular understandable adaptable many challenge solve believe benefit worth hope keep exploring idea developing learn 6 signature 5th march litus ramos 13 year old terrassa barcelona spain nvestigaci n sobre redes neuronales hecho este descubrimiento que quiero compartir necesito sugerencias simple look neural architecture inspired object oriented programming towards hybrid neural probabilistic litus ramos march 1 introduction hi name litus want suggest way combine object oriented programming neural network idea explore could help create hybrid use neural network probabilistic method 2 idea behind started thinking neural network set especially neuron layer connected layer build backpropagation information like error gradient move one layer another training",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[R] Investigation about neural networks and OOP.txt",
    "text": "creating flow knowledge got thinking look neural network like object oriented programming oop oop class inherit property behavior class make easy reuse organize code neural network also kind hierarchy layer build work done layer like child class inheriting parent class layer pas complex information still depending basic work done previous layer made think could apply oop idea improve neural network set 3 possible us using idea oop neural network might design flexible modular network mean layer could specialize one task still benefit previous layer done one potential application could hybrid mix neural network probabilistic would allow learn also deal could help task prediction need handle noise like reinforcement probabilistic programming type hybrid 1model could work better situation unclear incomplete another possible application could involve making easier understand oop inheritance make clear different class connected apply neural network could create easier see layer could helpful area like medicine finance understanding make decision important moreover could apply neural symbolic system combine neural network symbolic reasoning could improve ability solve complex problem need logical reasoning finally making network modular could help transfer trained one task easily adapted different one could make training faster reduce amount needed 4 challenge uncertainty challenge uncertainty although idea interesting still several challenge solve sure put everything together yet idea seem clear making work practice require research testing big challenge figuring design layer inherit information properly also combining probabilistic neural network easy requires deeper understanding field another challenge combining symbolic reasoning neural network hard link continuous driven part neural network structured logical reasoning used symbolic system finding way make approach work well together tough problem even though answer right believe idea could lead important discovery creating flexible understandable promising goal time research hope solve problem 5 conclusion conclusion combining object oriented programming idea neural net work still early stage could change design improve neural architecture could make network modular understandable adaptable many challenge solve believe benefit worth hope keep exploring idea developing learn 6 signature 5th march litus ramos 13 year old terrassa barcelona spain comment",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[R] LangDiversity_ software to identify LLM errors.txt",
    "text": "source reddit machinelearning title r langdiversity software identify llm error content due challenge hallucination detecting error output given prompt becomes important challenge langdiversity implementation diversity measure domain independent used measure result language type pip install langdiversity video web visit read paper comment interesting idea thanks sharing",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[R] New paper_ LLMs don't have privileged self knowledge, which means we can efficiently train a General Correctness Mod.txt",
    "text": "source reddit machinelearning title r new paper llm privileged self knowledge mean efficiently train general correctness predict correctness multiple surprising expected content quick paper highlight adapted tldr thread find special advantage using llm predict correctness trend prior work instead finding llm benefit predict correctness many becoming gcm training 1 gcm strictly accurate training specific cm train including cm trained predict correctness gcm transfer without training outperform direct training ood datasets gcm based qwen3 8b achieves 30 coverage selective prediction v much larger llama 3 70b logits tldr thread full paper discussion seed previous work suggested used llm self knowledge e g identifying preferring generation ability predict paper claim specifically llm knowledge correctness curious everyone intuition llm self knowledge whether result fit prediction conflict interest making post comment absolutely true 2 year go task curate best example many tuned outperform kind make sense guess llm human analogy think people unreasonably confident belief due nunber factor cannot reasoned think cult member like beleive correct even given contrary evidence internal feeling certainty turned 11 look past gap logic quite easy however spectator identify person gone wrong normal follow line evidence claim person make spot claim match evidence basic pattern recognition x match yes think lot llm generation low entropy relative key token actually reason reason intuition trained predict confidence likely high confidence response low entropy token e g found safety alignment made token deep paper answer yes dangerous question probability continuing spit answer incredibly high less semantic lookup weight think intuitively would easier train separate correctness biased rest training procedure maybe two would also brittle though maybe likely collapse idk curious distinction x correct x correct x mention paper whether useful distinction confidence given answer confidence prompted answer correct one thing come mind confidence answer correctness reason pick option world state x described using n different phrase x alternative mutually exclusive world state described set different phrase probability given answer correct p x true 1 p true contribute partially final probability also would expect manner vote splitting answer answered different way reduce apparent confidence p x true n p x true n 1 p x true obviously true trait answer correctness provide uniform rescaling probability could reasonably highly peaked answer particular way answering question favoured stylistic pattern defined system optimal temperature would expect perfectly accurate appropriately add probability associated correct answer produce aggregate probability corresponds confidence correct value temperature aggregation would disappear example lower temperature answer less possible way express would become favoured mutually exclusive answer range expression probability associated group shift add sublinearly transformed distortion towards higher probability answer higher temperature optimum effect would reversed becoming inclined answer according diversity answer instead either case would bias away correctness contrast asking whether statement output distribution correct cause reveal binary partition subject competition potential correct answer also another obvious methodological advantage ask question prompt individually without account impact potentially correct answer also deal potential answer may may true scope experimental team verify somehow appear unintended valid answer question simple example inspired something currently question",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[R] New paper_ LLMs don't have privileged self knowledge, which means we can efficiently train a General Correctness Mod.txt",
    "text": "first climbed mount everest answered edmund hillary tenzing norgay another tibetan nepali name despite expected answer may actually correct one outside recorded history outside range experimental design thus constrained focus subset answer verify correct incorrect cannot evaluate entire output distribution unless possibly way create measure consistency something like probability answer correct incorrect well answer question like two answer true applied pairwise across full answer distribution recall paper tuning predict v predicting correctness predicting better seem find fascinating result honestly llm lack self knowledge correctness might emergent relational property introspective one true knowing right something feel something exists across time shared calibration kind hint deeper architecture built yet one learns output correctness evolves let say treat correctness temporal signal thing get interesting fast reason posting code yet llm dont privileged self knowledge become globally interactive information future becomes u privileged self knowledge either ethic infect u like subjective experience anyway great hear really curious could share context kind scale good point also see llm usually overconfident rather underconfident human calibrated either ostensibly human given training superforecasters predict correctness due access private information tired past performance tired llm turn really yes also pointing deeper problem behavioral calibration correctness many paper including one deal identifying incorrectness posthoc filter reranker preventing llm generating badly first place also important confidence need paper seems like could potentially work use posthoc correctness prediction reward signal make llm prefer correct calibrated often understanding correctly pointing different thing yes choose evaluate binary correctness answer deal case potentially multiple true answer confidence 100 possible measure outside consideration yes true require ground truth indeed concrete ground truth ground truth hard evaluate correctness mention definition correctness confidence briefly suggested paper correctness objective concept could define correctness differently depending context following bit tangent e g alternative world correct mean answer people agree without reference external world modeling correctness way believe general correctness still would able outperform specific correctness llm would still self knowledge type correctness key distinction correctness must reference property llm generation thus llm would self knowledge due exposed behavior past generation typically go biggest teacher something 1 7b parameter gemma 3 mistral 7b go day create task specific require complex logic reasoning tend test 7b first see pick task well work way depending complexity task usually need 20k example beyond tend drop past certain loss rate get example require lot work though complex task require 100k generation filtering junk get 20k premium example clean tricky though time say something usless instead following instruction enough information right instead null pia clean rerankers similarity search help wrangling take totally different dimension took year work process pretty repeatable oh case super clear cheap example generation expensive part fortunate get bunch built google offering free usage gemini api thousand call 20 account",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[R] Theoretical limitations of generalization bounds.txt",
    "text": "source reddit machinelearning title r theoretical limitation generalization bound content tl dr fundamental limitation tight generalization bound though many newly proposed generalization bound recent year common theme numerically loose even vacuous evaluated practical setting e realistically sized standard datasets severely limit utility performance guarantee impact practical algorithmic design observed gap theory practise merely artefact loose proof technique also fundamental statistical limitation tight bound find many setting latter case paper 1 published iclr 24 bound tailored specific algorithm necessarily loose many algorithm distribution combination rich enough setting algorithm dependent bound subject principle one either learn target distribution well verify success never paper 2 recent preprint show algorithm certain inductive bias cause unstable admit tight generalization bound next show algorithm sufficiently stable tight generalization bound think finding could interest many member community broadly interested generalization happy discus question feedback criticism welcome comment coming differential geometry topology community much interest generalization came across result might bearing stability robustness gradient flow descent perturbation gradient suggest could read recent related work sorry topic check linked paper later super interesting paper congratulation congratulation nice work crucial think realizable assumption zero bayes risk sound hard e g show work class parity function principle also work noisy parity true label flipped probability alpha 0 interesting think interesting insight ported quantum machine learn ability might easier extend physically encode lot assumption architecture quite easily paper proved theoretically could achieve better generalization compressing internal representation instead scaling generalization occurs forget remember work provide recommendation regarding change architecture hyperparameters order achieve improved stability e generalization proposed thanks interest familiar result concerning stability gd gf result might fact interest u since least intuitively notion stability implied case average perturbation incurred removing sample per mini batch change batch gradient much thank thanks great question high level fact show impossibility result realizable noiseless case make stronger worse algorithm increasing label noise algorithm detereorate performance easier problem finding good generalization bound parity example hypothesis realizable erm output risk either 0 1 2 hardness come decide declare risk 0 1 2 possibly something inbetween huge discrepancy one add label noise range possible risk depending draw training set start move towards 1 2 extreme case flipping label w p 1 2 constant generalization bound declares 1 2 perfectly accurate generalization bound high level argument also applies function class label noise preliminary experiment see appendix paper 2 indicate least vanilla neural network architecture trained sgd standard vision task surprisingly stable think rather exiting warrant investigation present however rigorous result show property generically imply stability might indeed technically rather challenging example one could try show certain distribution sgd find local minimum stable leave investigation larger scale experiment future try long run distribution stochastic gradient descent large deviation analysis paper examine long run distribution stochastic gradient descent sgd general non convex problem specifically seek understand region problem state space likely visited sgd much using approach based theory large deviation randomly perturbed dynamical system show long run dis tribution sgd resembles boltzmann gibbs distribution equilibrium thermodynamics temperature equal method step size",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[R] Theoretical limitations of generalization bounds.txt",
    "text": "energy level determined problem objec tive statistic noise particular show long run problem critical region visited exponentially often non critical region b iterates sgd exponentially concentrated around problem minimum energy state always coincide global minimum objective c connected component critical point visited frequency ex ponentially proportional energy level finally component local maximizers saddle point dominated component local minimizers visited exponentially often perhaps want discus elsewhere sure would happy receive dm someone else know kind optimization result though probably offer much insight due unfamiliarity subject",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[R] Whatâ€™s your suggestion for offline RL_.txt",
    "text": "source reddit machinelearning title r suggestion offline rl content hi guy read lot offline rl paper last fall semester choose course project offline rl seems hot topic recent year believe major challenge offline rl distribution shift ii overestimation second challenge caused learner agent never allow interact true environment optimistic unseen state action hence many paper address challenge e g cql mopo however method handle misleading datasets consider following example suppose one state mab two arm reward first arm return 2 3 probability 1 reward second arm bernoulli distribution p 1 2 clearly choosing first arm best choice dataset unfortunately sample second arm received reward 1 agent access misleading dataset use bayesian method posterior give high score second arm use lower confidence bound need count occurrence arm hard extend method mdps arbitrary large state action space anyone know function capture caused dataset method tell learner misleading situation comment think generally use policy technique eg importance sampling thanks answering suppose know behavioural policy pi b let w density ratio w pi pi b add penalty term like f divergence f pi pi b f w algorithm e g pro rl designed conservative policy based importance sampling question specifically measure caused small number point without counting function capture counting information implicitly",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "[R] When an LLM is apprehensive about its answers -- and when its uncertainty is justified.txt",
    "text": "source reddit machinelearning title r llm apprehensive answer justified content estimation crucial evaluating large language llm particularly high stake domain incorrect answer result significant consequence numerous approach consider problem focusing specific type ignoring others investigate estimate specifically token wise entropy judge masj would work multiple choice question answering task different question topic experiment consider three llm phi 4 mistral qwen different size 1 5b 72b 14 topic masj performs similarly random error predictor response entropy predicts error knowledge dependent domain serf effective indicator question difficulty biology roc auc 0 73 correlation vanishes reasoning dependent domain math question roc auc 0 55 principally found entropy measure required reasoning amount thus related entropy integrated within estimate framework masj requires refinement moreover existing mmlu pro sample biased balance required amount reasoning different subdomains provide fair assessment llm performance comment",
    "cluster": 3,
    "keywords": "network, llm, deep, like, neural, one, based, paper, would, could"
  },
  {
    "filename": "07a44be60be0c997665b8a7eb0e07282.txt",
    "text": "08 app colin eberhardt colin eberhardt cto scott logic 1 building product probabilistic era blog post provides engaging articulation building product requires significant change overall approach software find tone little overenthusiastic sure faux mathematical representation key theme expressed resonate deeply building product probabilistic era giansegato com 3 1 karan bhatt security product improvement lindy 1 interesting share completely agree building product requires mindset shift founder building space realized longer designing deterministic system orchestrating probabilistic one big cultural change team used traditional software 1 vinay devaraja join dociteasy io waiting list freelance engineer helping business adoption digital transformation founder zwift lab llm agentic decade experience building digital product 1 conventional software probabilistic v llm application deterministic approach deterministic approach always produce output given input based explicit rule based logic e g calculator foundation traditional software development probabilistic approach produce plausible fixed output based statistical likelihood e g llm core generative handling ambiguity human language complex often multiple interpretation probabilistic system weigh different possibility provide likely relevant response creativity diversity generate novel unique content text image code drawing pattern training something fixed rule system cannot adaptability",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "0e578140909cbf24fd82f1fa6b28316e.txt",
    "text": "08 app anna marqu banqu anna marqu banqu lead qppv office operation strategy deputy eu qppv 3 openai recently highlighted one reason tend hallucinate training evaluation procedure reward guessing acknowledging openai sep yet concern articulated year ago 2021 ben kompa jasper snoek andrew beam emphasized importance quantifying communicating medical machine cornerstone building trust argued must able say know sure since healthcare context guessing unsafe fast forward message remains must handle transparently rather improvising answer 2021 article highlight two key form aleatoric arises noise ambiguity variability cannot eliminated matter much collected epistemic contrast stem limited knowledge insufficient reduced trained representative information distinction underline simple confidence score enough without understanding type user cannot judge act output view building framework might also consider third category economics knightian introduced frank knight 1921 refers situation unfamiliar meaningful probability assigned unlike aleatoric epistemic form remain within realm calculable risk first measured latter reduced knightian lie beyond risk case safest responsible action abstention perspective add important layer recognizing uncertain entered truly unkno",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "0f626f15e7eef56a43f2cd869830b6e7.txt",
    "text": "08 app christopher mcmahon christopher mcmahon director emerging technology research goodlabs studio 4 tend present output persuasive confidence even completely wrong quantifying clarifying user hugely important especially high stake application making shoutout mit team behind themis developing capsa open source python library provides agnostic wrapper allows quantify communicate tool differentiate two distinct source ml aleatoric epistemic note expensive word intuitive concept aleatoric come noise input like blurry image ambiguous language epistemic come gap knowledge limited biased training knowing category belongs mean know reduce better v better training live expect result ambiguity v naivety capability especially valuable application like healthcare finance 2 field goodlabs intimately familiar advice lead serious impact people life understanding properly handling advice system become transparent trustworthy ultimately human capsa development reflects broader shift research towards creating system intelligent also self aware limitation incorporating mechanism assess communicate better support human user leading safer effective outcome across industry capsa themis c2pa 8 407 50 ensure trustworthiness transparency",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "0f8ff5c9e5d1c9378aaca4088e681a52.txt",
    "text": "08 app agentic workshop 26 1 certain know two kind first common epistemic come u lack knowledge inability measure properly failure fully understand system weather forecasting classic case physic deterministic gap become big gap prediction second aleatoric randomness inside system like rolling dice matter much know cannot get past probability randomness baked matter talk see epistemic reduced better better measure better context may aleatoric especially introduce use pseudo random number generator simplify sampling llm effectively injects aleatoric system otherwise deterministic want place llm proper scientific philosophical context start unpredictability machinelearning genai agent agentic llm artificiallife contextengineering promptengineering 2 1 agentic workshop 1 check full video john jc cosgrove partner cloudwerx founder former ceo lightfold advisor speaker strategist practitioner applied generative agentic total lunatic 1 certain know two kind first common epistemic come u lack knowledge inability measure properly failure fully understand system weather forec",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "10aacf6fa85ed6e1e9d3505e220be16a.txt",
    "text": "08 app micha jask lski micha jask lski digital marketing generating b2b lead team leader computational chemist psychologist looking podcast guest share insight 1 markov chain powerful tool analysis modeling know markov chain around since early 1900s introduced mathematician andrey markov 1906 provide way predict future event based present state without needing know full history concept behind markov chain simple yet powerful future state depends solely current state making memoryless process practice markov chain widespread application machine e g predictive finance risk modeling artificial intelligence recommendation algorithm biology gene sequence modeling game theory optimal strategy markov chain relate large language llm llm like gpt use complex algorithm foundation probabilistic modeling including markov chain paved way advancement early stage simple markov used sequence word letter text today llm build concept take using deep neural network attention mechanism understand predict language context evolution traditional probabilistic sophisticated llm demonstrates incredible advancement natural language processing explore potential markov chain see continue influence modern technology markov datascience machinelearning llm nlp modeling c2pa 4 godwin josh co founder altrosyn director cdtech inventor manufacturer 1 convergence markov chain quantum computing promise unprecedented l",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "116c86e6c8fe946944dbfe0cb5c46fa3.txt",
    "text": "08 app fayadh alenezi phd fayadh alenezi phd consultant risk leadership governance strategic thinking enabling organizational success transformation vision visionary leadership leadership 4 iceberg surface level risk management fails let swim let dive risk life list log deep risk float isolation swim context shaped see never thought ask define risk effect objective keyword effect layer visible hidden like iceberg surface risk measure discus prepare stochastic happen aleatoric outcome beneath surface risk mislead u blindside u quietly build late epistemic think know agnostic even know seeing ontological game changer yet imagine 1 stochastic greek stokhos target risk binary event happen risk love list score assess tip iceberg 2 aleatoric latin alea dice lie variability outcome roll dice something coming predict land 3 epistemic epist knowledge risk created illusion certainty false assumption overconfidence blind spot unprepared misinformed 4 agnostic kind even aware hide silo unspoken truth power distance iceberg ignorance life executive see 4 problem frontline staff see almost 5",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "11e3aa2f9ee12950f821b53d08490065.txt",
    "text": "08 app sarat chandra vallabhaneni sarat chandra vallabhaneni manager deloitte gen machine analytics 5 large language llm deterministic probabilistic large language llm mathematically deterministic core functionally probabilistic due way implemented let u break llm determinism output llm neural network fixed given prompt set parameter mean always generate probability distribution token given input example input sky llm might assign 90 probability blue 5 probability red next token probability distribution consistent every time input provided identical condition llm probabilism probabilistic behaviour llm arises token sampling process used generating text continuing example input sky 100 time might predict next token blue 90 case red 5 case due sampling process token sampling inference make output llm appear probabilistic answer question llm generate deterministic probability distribution actual token prediction probabilistic due sampling process 163 19 chandranshu jain asst risk consulting associate moody analytics statistic major gold medallist genai rag agent r sql excel 5 sarat chandra vallabhaneni agree given prompt contextual embedding hence softmax give distribution decoding techinques like greedy decode exhaustive decode beam search deterministic decoding stochastic decoding technique like top p top n temperature sampling add randomness decoding process feel free add",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "1ac24ef3320b5ff9040b59a8bd7d2a8b.txt",
    "text": "08 app hero io 947 1 vyas raina chief science officer apta hero io shed light limitation probabilistic like chatgpt generate probable answer based pattern probable answer always right one especially precision key agentic framework come play unlike traditional agentic framework designed provide highly accurate sophisticated answer understanding specific context query work best choice delivering precise result find latest video 38 transcript transcript hello everyone via serena chief science officer apta today talking large language role called agentic framework familiar large language lm chpt open gemini google lama meta quick reminder alms called autoregressive decoder architecture able answer question human generating probable word 1 1 example input sequence word buy cheapest apple buy cheap apple output word think probable next based training implicitly encoded billion parameter example next probable word may generated word appended original sentence generates next probable word fee case manner run auto aggressively generate probable answer store case alarm used without external influence way answer question say us parametric memory e final answer user get based",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "1b95e78ed8a21c3ea6ebb99233b2154a.txt",
    "text": "08 app anthony alcaraz anthony alcaraz senior ml strategist aws business angel enterprise partnership 8 understanding bi face nature agentic reasoning modern system system manifest multiple distinct form trigger different type reasoning response primary among epistemic gap knowledge understanding arise system encounter situation existing knowledge base insufficient incomplete second major category aleatoric stem inherent randomness variability phenomenon analyzed fluctuation experimental condition natural variation biological system third subtle form emerges integration process might call compositional search o1 framework reveals different type trigger specific cognitive response system first symbolic reasoning actually becomes crucial less dealing epistemic search o1 framework demonstrates structured knowledge representation particularly knowledge graph provide essential scaffolding navigating uncertain situation rather abandoning structure face ambiguity system us symbolic framework systematically explore resolve uncertainty directed query information integration second dynamic interaction knowledge retrieval reasoning process crucial framework agentic search mechanism show trigger systematically addressed structured knowledge acquisition process simply reactive maintains coherent reasoning chain incorporating new information demonstrating structured approach facilitate rather hin",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "2d3804a9485c5a9b60f0f9bb95d7a32b.txt",
    "text": "08 app vincent vanhoucke vincent vanhoucke distinguished engineer waymo 1 stanford professor decision theory famously gave multiple choice exam asked student answer using probability grading done using kl divergence true answer chosen probability distribution consequence brutal gave 100 probability one question wrong got negative infinity question failed entire class maybe llm take page playbook 4 408 110 firas obeid sr scientist oppfi nyse opfi instructor mentor 1 tuned ml lately prediction clustered towards 90 confidence decile learned choice parameter caused issue although clear sign overfitting present think need focus choice hyperparameters look make overconfident making next token prediction david aponte senior research sde applied science group microsoft 4 brutal behnam mohammadi assistant professor quantitative marketing ut dallas 1 people would learn llm work making post llm 6 franklin carroll vp analytics modeling kukun 4 one sense system brilliant another introduces kind terrible unfairness play around clearer becomes without curve net produce grade worse standard average score grading consider question four possible answer unsure normal multiple choice system might guess yielding 0 25 expected score correct sat like wrong answer penalty skip get zero point kl log scoring system skippi",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "32013230d535081422353d0061a90530.txt",
    "text": "08 app jason parker jason parker phd economics generative science 4 real world guessing wrong dangerous education reward confidence caution student learn guess test perform certainty fake fluency unsure high stake situation clinical decision software safety financial confident wrong answer cost far honest know inherited problem large language trained source like stackexchange reddit wikipedia place confidence king training objective predict next token evaluate whether speaker actually know talking result fluent answer often sound correct grounded certainty trained bluff fixing would mean relabeling huge amount training reflect said said answer cautious hedged overconfident expensive requires expert annotator crowdworkers result would behaves less like trivia bot like careful expert hard problem think next step toward building actually trust answer judgment thinking epistemic humility let connect machinelearning responsibleai epistemichumility trustworthyai educationreform datascience aiethics 100 6 shashank ravishankar scientist capgemini north america analytics practice 4 great post extend problem would great think come simply randomness world unsure world uncertain ignorant aleatoric yanhang hui strategy consultant teleinfo caict 4",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "3b7076c2a5322e788609c29e9f6cbcd1.txt",
    "text": "08 app david tolpin david tolpin computer scientist 2 one never never use llm chatgpt similar anything fact based previous research concept drift detection handling context probabilistic programming chatgpt concept drift detection handling context probabilistic programming active area research several paper published topic example paper bayesian online change point detection probabilistic program yang et al 2018 proposed online change point detection algorithm probabilistic program algorithm us bayesian inference detect change underlying probability distribution program variable another paper adapting probabilistic program concept drift liu et al 2019 proposed approach adapt probabilistic program handling concept drift approach involves distribution program parameter adjusting concept drift detected also several paper propose using bayesian inference update presence concept drift bayesian nonparametric online change point detection probabilistic program bai et al 2020 probabilistic programming non stationary stream minka et al 2019 convincing answer one small problem answer none paper mentioned ever written title look relevant author name real people working field paper simply exist llm parrot trained tell truth trained convincing like politician politicans llm little use fact important want health related consultation advice bank account convincing b",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "41004e6364426158b78af2395e63cb0a.txt",
    "text": "08 app john jc cosgrove partner cloudwerx founder former ceo lightfold advisor speaker strategist practitioner applied generative agentic total lunatic 1 certain know two kind first common epistemic come u lack knowledge inability measure properly failure fully understand system weather forecasting classic case physic deterministic gap become big gap prediction second aleatoric randomness inside system like rolling dice matter much know cannot get past probability randomness baked matter talk see epistemic reduced better better measure better context may aleatoric especially introduce use pseudo random number generator simplify sampling llm effectively injects aleatoric system otherwise deterministic want place llm proper scientific philosophical context start unpredictability machinelearning genai agent agentic llm artificiallife contextengineering promptengineering 7 2 john jc cosgrove partner cloudwerx founder former ceo lightfold advisor speaker strategist practitioner applied generative agentic total lunatic 1 check full video justin tauber agentic innovation ethical technology business culture aaicd 1 sell pill called simplicitytm cure recurring aleat",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "41519f91de7be1370e6a74769f71fd6c.txt",
    "text": "08 app justin strharsky 3 introducing much life career job 2 year even bother degree instead running away maybe run towards silicon valley new york london jessica ridella incredible career journey taken startup founder global sale leader board level client advisor credit career growth embracing never going hundred percent certainty right progress come waiting clarity come moving fog trusting figure along way 64 6 transcript transcript lot people think thing changing uncertain run away one run towards edge right stepped role necessarily playbook right thinking even watson x launching platform exist moved country different leadership role taken account told nothing good come good luck never going 100 certainty right progress come waiting clarity come moving fog trusting figure along way richard foster fletcher global advisor keynote speaker future work responsible named uk top 20 researcher entrepreneur 3 career anxiety describe real argue new visible accelerating change already happening skill live distinguish productive struggle pointless anxiety leader focus developing judgment critical thinking ability ask b",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "41e23d3f4597f28872a73e0a090e6590.txt",
    "text": "08 app preksha kaparwan building private pharma cloud based agentic platform 2 min story linkedin top voice simplifying non tech 8 9 100 probabilistic v deterministic difference probabilistic thrives possibility calculates likelihood make decision think making well informed guess gpt llama etc deterministic hand operates strict rule logic producing result every single time given input super let know comment wanna see demo understanding difference key choosing right business need probabilistic suit scenario natural e g weather prediction deterministic excels rule based system e g automated checkout creating series 100 concept basic definition advanced topic like agentic slms help business team make informed decision share even content instagram citizendatascientist perfect young professional transitioning science check got specific topic mind let know cover datascience enterprise 100 142 16 transcript transcript genot thai prive propublica plek van beaufort analytics yes even api based platform wrapper three reason one probabilistic mean lack accuracy important business cannot make decision worth million based approximate answer 2 threat privacy yes every training catch drift 3 hallucination must seeing chatgpt good completing story imagination business cannot",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "46a0d1ff1e69ac9d665c94bf43677f23.txt",
    "text": "08 app mark jennings bates mark jennings bates co founder mosaicdm co architect deterministic currency truth keynote tedx speaker leadership risk technology 1 everyone talking breakthrough asking truth verified drove rally race half field trusted luck adrenaline half trusted note discipline guess finished race story probabilistic give flash noise deterministic framework give repeatable audit traceable result luck required high performance come gambling come system collapse stake highest framework built di dimensional intelligence trust arthur howell cybersecurity executive head cyber advisory incident response digital forensics leader generative security revenue growth resilience 1 ready dive world artificial intelligence latest blog post uncovers true cost agi delusion u falling behind real race spoiler alert chasing superintelligence tackling big question revealing mean future technology miss insight could reshape understanding potential check cpa trendlines 2 022 1 age hype must seize back control say dataswyft ceo bad news seems snowballing sector even poor humanity running escape avalanche market economist professor irene ng idea future cpa trendlines research 543 1 age hype must seize back control",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "474ea51aa1945b0c227841a938b45c2b.txt",
    "text": "08 app tobias rebert tobias rebert co founder tentris query wait less high performance disk based rdf graph database enabling graph analytics deemed computationally infeasible 1 rdf could say maybe stumbled topic ago stuck traditional rdf assumes binary truth know triple either true false real world rarely clear cut let tell something probabilistic rdf allows attaching probability triple enabling reasoning especially useful domain like biomedical sensor network nlp fact inferred incomplete noisy example instead asserting express confidence 0 7 open door probabilistic inference ranking query answer integrating bayesian markov logic approach rdf sparql could key step toward scalable aware reasoning semantic web think semanticweb ready embrace practical experience probabilistic rdf would love hear knowledgegraphs graphdatabases graphdb 66 58 jamie mccusker opinionated ontologist 1 able nuanced way 2017 paper using nanopublications powerful approach capability 3 eugene agbor egbe senior backend nlp engineer mentor wikimedian 1 thinking rdf statement claim would give clear sense extent claim close fact 1 nick nisbet built environment information consultant 1 information claim needed definition need normative knowledge rdf distinctly underpowered keith corbett tra",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "4bdd6e936d9e763df7783c6f4abe40e9.txt",
    "text": "08 app anthony alcaraz anthony alcaraz senior ml strategist aws business angel enterprise partnership 1 deceptively confident tackling hallucination llm application imagine seeking medical advice chatbot assures uncanny articulation stomach pain probably cancer mere month live picture financial advisor confidently instructing move life saving volatile memecoin promise 10x return would trust recommendation sure making thing hallucinating machine parlance large language llm demonstrate increasingly fluent human like output rapidly integrated real world application ranging search engine tutoring therapy however llm prone generating output seem plausible actually inconsistent factually wrong even outright nonsensical phenomenon known hallucination llm applied high stake domain like health finance law hallucination lead catastrophically bad decision erode trust technology whole detecting mitigating hallucination thus urgent challenge llm application developer must grapple encouragingly recent research google deepmind yielded technique quantifying llm output screening unreliable one incorporating aware method developer make application transparent robust ultimately worthy human trust article look primary type plague llm analyze tool assessing reducing examine integrated hallucination detection pipeline llm apps aleatoric arises ambiguity randomness",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "4ca4c56662ac165ffe32f0cd998174af.txt",
    "text": "08 app sam lee sam lee strategy operation leader focusing monetizing innovation 9 pungent help easy oh accurate back snowflake used say strategy without strategy llm essentially probabilistic large unstructured learn bad information repeat bad information stake much higher agent bad conversation becomes bad action eduardo ordax generative lead aws 150k startup advisor public speaker outsider founder thinkfluencer 9 cool still one biggest challenge customer today part super important something cannot skip create cool chatbot virtual assistant even use ml cluster customer quality suck output get suck regardless ml genai agent output still genai agenticai kudos dr christian krug previous post topic 19 1 godwin josh co founder altrosyn director cdtech inventor manufacturer 9 fascinating stake rise agent pushing u scrutinize quality even rigorously ethical implication llm biased information constant concern researcher like envision mitigating risk practical application 1 4 095 621 llm process language understand llm evaluation method understand neural network llm understanding limitation llm poor affect result improve agent performance llm career productivity finance soft skill emotional intelligence proje",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "57248fb2ac11847438d6661b75d2384c.txt",
    "text": "08 app eric little eric little innovation principal director life science head strategy global product harmoni accenture 1 deterministic software v probabilistic seeing topic come lot worked space probability logic intersect career want talk little real world client regulated environment instead c thought experiment research paper demo level stuff unvarnished take deterministic software sometimes couched old hat stuff whereas llm gen engine seen future b c deal billion parameter produce probability textual quickly meaning potential answer set far broader much complex fine trusted absolutely need accurate speak client time trying get head around bring get work scale run multiple pocs pilot getting tech point someone bet new product company reputation stock price lot trepidation good reason still yet find engineer bet entire paycheck system performing well high stake environment lot regulated environment need deterministic answer order counted validated system e able produce answer regulator complete confidence order product approved market drug car food etc need safe need show regulator engine going make drug disastrous side effect food dangerous eat etc work stuff every day amount erroneous info thing spit still way high peop",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "69979b2db7645df17e2204a6321f9928.txt",
    "text": "08 app esther gimeno mir esther gimeno mir ling ista computacional l der creativa de proyectos ia conversacional cofundadora de cabe tendiendo puentes entre lenguaje tecnolog cultura 2 last post claimed critical thinking rest eternal questioning certainty like push idea little certainty even marker madness whoever never doubt lack critical thinking also risk compromising mental health interesting point since constantly bombarded people claim answer method philosophy diet routine need succeed clinical setting absolute rigidity belief without trace doubt red flag often seen delusion paranoia obsessive fanaticism psychosis philosophy popper already argued knowledge must always falsifiable questioned knowledge dogma yet live time certainty rewarded hot take strong opinion viral claim even machine llm algorithm recommender system reinforce look certain user question answering question implies asking others 1 expectation must ai fulfill profitable 2 train least design interface predict show range 3 would desirable epistemic aleatoric value hand user determine best course action nb epistemic limit knowledge aleatoric variability world 4 balance user want v user need like education healthcare training client trust expertise often accept hear need want 5 trust",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "7fec04e123e5b90c1a7d09c10b10970a.txt",
    "text": "08 app vincent pavero vincent pavero co founder ceo homeric enabled product agile excellence 1 really understand probabilistic nature llm yesterday posted image chatgpt 5 able assess number b blueberry surprised see people reacting basically said work work bugged llm make simplistic llm prediction machine guess next best character output context defines weight option llm output sky character b may 99 chance picked could g 0 5 0 5 someone lucky enough get llm sky may yellow know color hallucinates hallucination bug direct consequence probabilistic nature regular code one input one output llm one input infinite number output different odds nothing secret chatgpt transparent keep digging look like uncomparable b pun intended b blueberry may ask possible get response thousand time chatgpt llm anymore became system myriad tool multiple try combine best generative content consistency algorithm asking math question example chatgpt predict anymore detects expect calculation feed calculator tool get result time believe important understand different system helpful maximise benefit claude chatgpt",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "8653403f2c242f38e06ad66fed0ec662.txt",
    "text": "08 app jaap vink jaap vink driving widespread responsible use decision making 1 probabilistic trap even smartest system struggle deterministic rule recent kaggle exhibition chess tournament several llm levy rozman new video openai gpt5 v stockfish see triggered put thought together limitation current llm deterministic rule context like law public policy administration push llm domain precision reliability non negotiable like law regulation even something seemingly simple chess confronting fundamental limitation llm reason predict latest draft analysis dive critical often overlooked flaw heart today inability guarantee adherence deterministic rule even advanced play game chess without occasionally making illegal move architecture built probability logic next token prediction engine behind impressive output also reason fully trusted high stake environment chess field like law governance probabilistic core lead legal hallucination advice undermine justice public safety institutional trust emergent reasoning fascinating solve problem make mimicry convincing usual asking thought feedback critique asking much pure probabilistic accelerate development hybrid system deliver relevant content reliability arnoud engelfriet jos van der wijst ip lawyer bart de visser monika milanovi jasper kars elja daae martijn wiertz aleksand",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "923bdde7f092286136841faf4dc07ac8.txt",
    "text": "08 app jason gorman jason gorman 1 hypothesis increasingly looking like large language going get significantly reliable unless altman get dyson sphere course point improvement foreseeable future come use spent couple year researching experimenting watching team try range different strategy improve confidence predicting next token believe essentially boil statistical physic ftw think group strategy appear move needle hallucination downstream cost 2 broad area 1 reducing entropy prediction e g small step prompting test 2 minimising downstream bottleneck get overwhelmed unleashing code generating firehose dev process e g continuous testing trunk based development calling technology may get faster probably cheaper hmmm compute requirement kind reliability need box speak literally astronomical think smart money going pivot towards principle practice tool build top llm pretty much today luckily software development principle practice happen practicing teaching 25 year team overwhelmed code generating firehose drop line time go back foundation 103 15 jason gorman 1 think really funny 2 year folk foretelling commoditisation software e g saas demise professional software developer though look like actually going get commoditised",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "930a6f0c42faa465ee2caf7f7216f96d.txt",
    "text": "08 app harper carroll harper carroll explained yr building ml stanford computer science facebook meta nvidia acquired co head top global pro linkedin 2 large language llm struggle math ok yes improved taken concerted effort let explain typical llm struggle much step step large language llm probability based machine word generator calculator hence trained predict next word token sequence based pattern seen massive text datasets online language text everywhere pattern learn take simple sentence hi name harper one sentence could create multiple training example predicting next word hi predict hi predict name hi name predict hi name predict harper single sentence created several training example multiply across billion sentence online see get really good understanding relationship word math different rule based pattern based llm inherently understand multiplication division arbitrary numeric relationship simple problem like 1 1 give right output extremely common online ask compute 5252 3 63 98 likely fail predicts probable mathematically correct also check structured numeric like case number legal document learn format length case number know say digit exact digit effectively random perspective every possible combination",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "96a0c74a1567da7db4a859d633343e27.txt",
    "text": "08 app giovanni bruner giovanni bruner lead scientist head fraud intelligence nexi group 2 often hear llm unreliable since deterministic produce different result input built probabilistic quite accurate auto regressive sequence predictor fixed weight training given input sequence output probability distribution next possible token via softmax distribution fully determined input parameter change prompt produce different output llm use sampling strategy like top k nucleus sampling softmax probability introducing randomness creativity diversity example asked chat gpt complete fhe following sentence going sea friend might compute probability next word amazing 30 funny 20 relaxing 18 terrible rather always choosing likely word sampler randomly selects top candidate unfold different completion sentence continues variability provides creativity always desirable workflow agent reproducibility essential fortunately controllable setting temperature 0 apis like gtp 4 enables greedy decoding ensuring always selects probable next token making response fully deterministic case randomness feature flaw knowing turn key 42 3 keith hackbarth vp engineering modern animal 2 great post thanks sharing one nuance add experience leading support true determinism even temperature set 0 fairly well documented openai gemini",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "9ada0c062df9136a77752442defccc64.txt",
    "text": "08 app vitalii duk vitalii duk founder ceo dynamiq building agent 1 llm agent undoubtedly one exciting innovation today potential hold immense bringing production environment without challenge one significant drawback lack predictability determinism handling input output business unpredictability red flag imagine entrusting critical task tool might change action slight variation task definition risk business afford take unpredictability precisely industry like aviation avoid relying neural network similar ml approach safety critical system require absolute reliability something probabilistic struggle guarantee llm design probabilistic predict next token based learned probability training approach work well many scenario pose problem task demand consistency reliability address one promising approach incorporating structured text generation key component llm agent action planning openai recent advancement structured json generation step right direction making llm agent predictable reliable combined development comprehensive self correcting loop within agent could include agent critic mechanism one agent action scrutinized another helping ensure accurate consistent output llm agent come long way still much work done make reliable tool business critical application focusing area move closer creating agent business trust exactly problem spend countless day",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "9b2b4e137a4f7966ee159a8719cb5ea5.txt",
    "text": "08 app orest yatskuliak orest yatskuliak executive partner security intelligence human system research insight advisor system generalist 11 odds language probabilistic reasoning google latest research paper odds language capable probabilistic reasoning accepted emnlp 2024 study explores whether large language llm engage probabilistic reasoning whether understand dataset whole rule parameter draw conclusion individual case based understanding simpler term llm perform mathematical deduction test researcher challenged three type task standard probability calculation sampling percentile calculation also used different kind context ideal distribution e g normal power law real world distribution like climate approximated real world distribution result llm performed best percentile calculation ideal distribution sampling real world presented challenge key takeaway llm probabilistic reasoning capability matter unlocking refining research open exciting new opportunity future driven decision making machinelearning probabilisticreasoning emnlp2024 llm airesearch googleai 2 godwin josh co founder altrosyn director cdtech inventor manufacturer 11 research suggests llm could one day analyze complex real world datasets like climate nuanced probabilistic understanding imagine llm predicting probability event also cascading effect across interconnected system could see llm used forecast mitigate global risk based probabilistic simulation future",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "a167d3f1a986b3ba249e830a4267a746.txt",
    "text": "08 app karl l smith fbcs frsa karl l smith fbcs frsa karl l smith fbcs frsa fbcs frsa cxo 1 perfect example ignorance around llm predicting tool bayesian network also known bayesian belief network probabilistic used uncertain system combining expert knowledge allowing probabilistic inference predicting outcome denis fintech professional solution architect real time ontology knowledge graph exploring beyond llm 1 say louder people back tech team telling large language brilliant prediction engine thinking machine excel pattern matching vast datasets generating convincing response predicting come next prediction understanding statistic intelligence scaling prediction never create kind artificial general intelligence could truly transform business david kenyon principal software engineer rheem 1 fool tool still fool successful integration requires knowing apply right tool appropriately selective task make blanket policy imposing use especially know use work talent organization ask existing tool improve quality productivity meaningful pilot denis fintech professional solution architect real time ontology knowledge graph exploring beyond llm 1 say louder people back tech team telling large language brilliant prediction engine thinking mac",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "a20198865589a642508a08bae70940d4.txt",
    "text": "08 app jean carron jean carron chef de projet chez sollertia sa 2 well written description limitation llm come extrapolation seen far spoiler alert limitation design llm awesome tool real use case creative thinker denis fintech professional solution architect real time ontology knowledge graph exploring beyond llm 2 sure needed whole harvard mit study tell u sky blue water wet transformer based llm lrms interpolative design side effect entire architecture go deep today maybe 99 9 field navigating probability space carved past llm lrms shift embeddings existing latent space built fit curve across known point training process compression pattern fitting trained navigates map compressed pattern produce response seem plausible approximate plausible approximate correct insightful new dress alignment layer guardrail prompt engineering context stitching change fact reasoning running highdimensional cooccurrence token karaoke layered reinforcement human feedback compliance filter result feel smart form smooth still surface stochastic variance built system also create anything new wiggle around within boundary latent space defined training get instead occasional hallucinated output fragment stitched together token level variance creature",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "c2feadf27fb2f8e351d2713642667b5a.txt",
    "text": "08 app ana costa e silva phd mba ana costa e silva phd mba global executive enterprise scale transformation ex chief fortune 500 1 large language general intelligence probability engine actually happens given context predict likely next word within neural network cluster neuron specialize activate medical text others finance others still romantic advise give illusion understanding truth pattern recognition reasoning researcher trying push beyond retrieval augmented generation rag ground answer external knowledge multi agent system llm talk solve complex problem symbolic reasoning layer add logic agi attempt combine llm montecarlo simulation reinforcement among fancy technique exciting direction llm gi confuse two risk misleading policymakers executive overhyping overregulating wrong thing explain leader system cannot really llm responsibleai leadership oecd alan turing institute european commission stanford institute human centered artificial intelligence hai mit computer science artificial intelligence laboratory csail 47 8 ana costa e silva phd mba global executive enterprise scale transformation ex chief fortune 500 1 yet another time people seem surprised obvious last three year already llm struggle clinical reasoning matching pattern study find grant oliff digital transformation leader award winn",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "c9d231fe2a82461e9f19d17616a018a4.txt",
    "text": "08 app andriy burkov andriy burkov andriy burkov phd hundred page language book hundred page machine book 1 ok let clarify llm neither stochastic probabilistic llm mathematical formula perfectly deterministic send value input get list score output decide toss coin based score make llm stochastic coin stochastic decide make decision tossing toss nothing change 431 150 abhinav kimothi 1 andriy interesting perspective may misunderstood true training complete trained deterministic 8 mazhar ali assistant manager operatios lesco 1 interesting view point 1 chirag jain ceo textify analytics ex hyundai motor ex netmarbles seoul national university k startup grand challenge 22 gks 2014 1 wait hmm happened temperature top p value thought made llm produce random value alon bochman 1 respectfully disagree andriy burkov toss get intelligence rich irony imo lot formula build llm none one know deterministic transformer typically start random weight temperature setting pick stochastically probabilistically among token distribution evolved deterministic stochastic deterministic llm wrote boring repetitive unintelligent often unintelligible text 14 adam lawrence genai science leader build high impact team fintech advisory consulting role 1 agreed interesting note though temperature parameter user defined determines lev",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "ceff0fc841415d66fa132141b0e36375.txt",
    "text": "08 app richard seiersen richard seiersen chief risk technology officer qualys xciso twilio ge lendingclub measure anything cybersecurity risk etc 4 quantitatively curious ciso recently asked really use probability cybersecurity risk management zen koan like answer important question life indeed part really problem probability laplace asks need first assess business stand lose aka value risk need understand cause loss threat start design approach determining probability myriad loss scenario realize perfect information every possible future loss need map every path every villain take steal treasure never begin start known known risk scenario negligent know scenario ransomware bad choice infographic related generating baseline starter probability start public information fortune 1000 500 great baseline example approach using f500 book measure anything cybersecurity risk must comfortable many must also understand word probability many also experiment getting baseline probability using perplexity llm based solution please mind prompt mind assumption mind output infographic explanatory high level note mapping score overly handwavy access massive petabyte scale lake certain advantage said perfect actuarial table mapping posture score probability fwiw chief",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "d727c8379780b0910bf9070c4d498986.txt",
    "text": "08 app babak hodjat babak hodjat chief officer cognizant 5 groundbreaking work llm estimation coming cognizant lab technology set absolute threshold llm output knowledge extraction llm world importantly reasoning allowing case case fallback rule based triggering human intervention much principled way important step towards making multi agent system safer consistent follow link try demo risto miikkulainen vp research cognizant lab 5 working method evaluate confidence llm response based semantic density e prominent response semantic space try interactive demo ask question system evaluates confidence various llm generated answer accessible end blog post introducing semantic density demo smarter way evaluate llm confidence medium com 159 3 chandra dasika 5 quite excited richard sternin trained since 2014 at director talent hand recruiting ic entrepreneur one exit 5 question reading medium article went demo want make sure understand llm asked question multiple time semantic similarity four response generates score across multiple llm 15 548 448 thousand agent step towards worldwide agentic web babak hodjat 2 introducing lab new website babak hodjat 3 using agent monitor agent babak hodjat 4 recent breakthrough technology trend llm development l",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "e2e33231902dfb2aa0147fa8ef9d364b.txt",
    "text": "08 app raymond uzwyshyn ph raymond uzwyshyn ph research impact edtech digital scholarship library innovation 3 meet quantum mechanic new way understand meaning language simplified podcast understand meaning probability word llm artificial intelligence way physicist understand particle wave dynamic probability quantum mechanic fascinating new paper timo aukusti laine linked proposes treating word large language llm quantum like state semantic space introduces idea semantic wave function capture strength word meaning also phase aligns interferes meaning neural net generate language using probabilistic next word prediction quantum mechanic wave function calculate probability particle might system share deeper mathematical pattern article explores llm quantized semantic space built discrete token like energy level complex number v real one better reflect subtle meaning shift semantic interference irony ambiguity hallucination emerge even potential semantic landscape context choice full paper complex ph level available review semantic wave function exploring meaning large language quantum formalism prefer listen something simpler learn topic easier longer introduction conversation two host notebooklm breaking ph topic master level completely simplified worth struggling podcast walk key idea compelling interdisciplinary forma",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "f4bdbcfb0d08650355cafef7b3b5953f.txt",
    "text": "08 app pedro martin pedro martin partner emea technology lead iac founder soludity ex nokia mba lean six sigma ml certified 4 art probability age agent agi conversation around agi singularity autonomous agent accelerate easy forget fundamental truth today powerful system still probabilistic despite remarkable capability large language llm like gpt understand reason think predict core llm generate word calculating statistically likely come next based everything came process trained massive datasets power fluency still highly advanced form pattern matching cognition awareness art probability chain thought really mean people refer chain thought reasoning llm may look like step step logic practice longer sequence statistically likely step modeled reasoning pattern found training llm solve problem human sense simulate solution might expressed based pattern seen distinction critical deploying enterprise safety critical decision making environment probabilistic nature brings real limitation 1 local global optimization llm predict one word time plan ahead unless explicitly engineered 2 grounded understanding internal world association word 3 limited memory even long context forget unless given explicit memory support 4 overconfidence hallucination confident sounding output always correct 5 post training without retraining adapt new reality feedback mea",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "f7c3014a03268599729b33d3792d9320.txt",
    "text": "08 app pierre carl langlais pierre carl langlais co founder pleias 11 presented yesterday new wave llm sampling like entropix cern science seminar llm generating probable word yet probable word always suitable even powerful like o1 claim attain level phd stem sonnet 3 5 currently unable solve simple problem larger number 9 9 9 11 due confusing different level token signification 9 11 decimal number 9 11 9 11 section number book entropix xjdr doomslide et al adaptive temperature sampling method petar veli kovi min p sampling minh nhat nguyen et al showing promising improvement front even small like llama 1b smollm enw hugging face slm rather relying fixed parameter change overall condition text generation depending degree confidence token generating generated past sometimes may future middle approach short better management internal probability make possible change overall token selection strategy give space think pause token branch different narrative topic especially relevant present cern given large reliance thesis method physic math softmax optimization entropy analysis also open possibility one day automate math seems right beyond current llm design currently training specialized slms pleias obviously following thesis development closely initial focus document processing slms math may next natural step releasing complete slide",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "f9f8071c487d6af73169db6149e6ac46.txt",
    "text": "08 app judd devermont judd devermont operating partner innovation kupanda capital 1 fantastic piece jerry offer sensible approach simplify become confusing contentious issue probabilistic language intelligence community jerry laurienti ph leidos former cia texas bush school dc adjunct professor korbel school international study alumnus council member 1 thanks thecipherbrief publishing thought restarting conversation around ic gauge express probability may disagree suggestion perhaps debate help u get right intelligence reform include updated probability yardstick thecipherbrief com 14 2 blair sondker intelligence analyst political risk manager strategic advisor 1 blisteringly important topic consideration particularly briefing customer outside traditional national security role term art within analytic probability absolutely fluster scientific engineering audience 1 christopher hoffman national security professional 20 year service cyber policy translation intelligence production covering south america sub saharan africa middle east conversation serious light 1 certainly realistic md reaz uddin cse student reverse engineering agi embedded system building scalable tool open source project 3 map top bottom llm actually work stop 1 text token embeddings stop 2 positional embeddings absolute rope alibi stop 3 attention qkv multihead stop 4 transformer normalize repeat logits stop 5 sampling trick temperature top k top p stop 6 kv cache long context hack stop 7 moe gqa stop 8 normalization activation pok mon icon stop 9 training objective causal lm",
    "cluster": 4,
    "keywords": "llm, probabilistic, system, like, probability, 08, app, language, deterministic, based"
  },
  {
    "filename": "01e50e539c106c160a598f80086d5f44.txt",
    "text": "08 app mfundo monchwe mfundo monchwe scientist researcher python r sql 2 would rather opt deep neural network limited interpretability deep bayesian neural network quantifies latter based bayesian principle offer comprehensive approach decision making ability incorporate prior knowledge furthermore probabilistic modeling fundamental language used various branch science engineering providing framework interdisciplinary collaboration otherwise willing listen option bayesian neuralnetworks machinelearning datascience statistic frequentists datanalytics 5 3 paul do santos applied engineer scientist gisc professional 2 main limitation deep bayesian neural network computational cost associated performing bayesian inference inference bayesian neural network typically requires approximation variational inference markov chain monte carlo mcmc method computationally expensive method also relatively slow make impractical certain real time application additionally interpretability bayesian neural network actually lower traditional deep neural network information provided bayesian difficult understand interpret non expert basically really choice two need consider requirement trying solve latency compute cost user etc 1 2 245 82 career productivity finance soft skill emotional intelligence project management education cookie",
    "cluster": 5,
    "keywords": "network, neural, bayesian, bnns, weight, quantum, prediction, distribution, probabilistic, 08"
  },
  {
    "filename": "08d3e9371b6e0ac65113894af790e7db.txt",
    "text": "08 app yiqiao yin yiqiao yin principal engineer azure aws professor mlops llmops devops computer vision ocr nlp llm genai agent operator mba 2 book review bayesian deep dive navigating transformative era deep reframing digital experience netflix marathon innovation behind self driving car pivotal aspect seems eluding grasp realm ever paused ponder resolute system decision answer lie bayesian deep unsung superhero world ensuring technological marvel merely taking wild stab dark rather well aware confidence metric grateful help packt vinishka kalra delving content pro con galore engage detailed exploration captivating world bayesian inference deep section provides unbiased examination highlighting strength integrating system candidly acknowledging challenge might pose bayesian neural network unwrapped delve heart bayesian neural network segment ensures reader grasp rudimentary concept also appreciate profound implication network usher broader spectrum comparative study step comprehensive comparative analysis myriad bnn implementation multitude option available chapter becomes crucial distinguishing subtle difference potential advantage method probabilistic dnns rock discover underlying reason make probabilistic deep neural network stand especially applied tangible real world scenario underscore robustness adaptability undeniable relevance today tech landscape",
    "cluster": 5,
    "keywords": "network, neural, bayesian, bnns, weight, quantum, prediction, distribution, probabilistic, 08"
  },
  {
    "filename": "0ea073361597e752f0c82680abc72cd9.txt",
    "text": "08 app maxim ziatdinov maxim ziatdinov building tool experimental material discovery 7 neurobayes 0 0 12 featuring partially bayesian transformer neuron level control stochasticity key addition partially bayesian transformer transformer neural network heart modern system increasingly used physical science however robust quantification transformer remains challenging replacing weight probabilistic distribution using advanced sampling technique work smaller network approach computationally prohibitive transformer new partially bayesian transformer implementation allows selectively make specific module embedding attention etc probabilistic keeping others deterministic significantly reducing computational cost still delivering reliable quantification fine grained stochasticity control even layer probabilistic training deep resource intensive specify exactly weight particular layer stochastic providing finer control computational cost v trade probabilistic smile transformer example github repo please add star next stop partially bayesian graph net 41 6 357 197 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 5,
    "keywords": "network, neural, bayesian, bnns, weight, quantum, prediction, distribution, probabilistic, 08"
  },
  {
    "filename": "459ed918a79d9ab7e789caceaa1cb3ee.txt",
    "text": "08 app e iode e iode 1 355 1 scientific paper inferring langevin equation via bayesian neural network abstract pervasive across diverse domain stochastic system exhibit fluctuation process ranging molecular dynamic climate phenomenon langevin equation served common mathematical studying system enabling prediction temporal evolution analysis thermodynamic quantity including absorbed heat work done system entropy production however inferring langevin equation observed trajectory remains challenging particularly nonlinear high dimensional system study present comprehensive framework employ bayesian neural network inferring langevin equation overdamped underdamped regime framework first provides drift force diffusion matrix separately combine construct langevin equation providing distribution prediction instead single value approach allows u assess prediction uncertainty prevent potential misunderstanding erroneous decision system demonstrate effectiveness framework inferring langevin equation various scenario including neuron microscopic engine highlighting versatility potential impact comment 30 page 17 figure continued e iode find interesting feel free follow comment share need help enhance visibility platform continues serve inferring langevin equation via bayesian neural network ethicseido com 1 355 career productivity finance soft skill emotional intelligence project management education",
    "cluster": 5,
    "keywords": "network, neural, bayesian, bnns, weight, quantum, prediction, distribution, probabilistic, 08"
  },
  {
    "filename": "75468e8a87cb1f3c4f867bd1c4c83a58.txt",
    "text": "08 app quantum zeitgeist quantum zeitgeist 15 013 4 bayesian neural network enable probabilistic graybox characterization quantum device quantification researcher enhanced device characterization technique called graybox modelling incorporating probabilistic method accurately estimate prediction uncertainty improve capture 1 quantum quantumcomputing technology bayesian neural network enable probabilistic graybox characterization quantum device quantification 1 quantum zeitgeist 15 013 3 neural network learn entanglement witness 99 accuracy quantum state researcher demonstrate artificial intelligence specifically continuous variable quantum neural network accurately identifies complex quantum entanglement photonic system exceeding performance conventional method offering pathway improved quantum state characterisation quantum quantumcomputing technology neural network learn entanglement witness 99 accuracy quantum state 2 1 quantum zeitgeist 15 013 2 quantum neural network demonstrate impact rotation layer four entanglement topology researcher systematically investigate arrangement quantum operation connection within neural network circuit impact ability generate complex accurately classify image revealing crucial insight designing effective quantum machine system quantum quantumcomputing technology quantum neural network demonstrate impact rotation layer four entanglement topology quantum zeitgeist",
    "cluster": 5,
    "keywords": "network, neural, bayesian, bnns, weight, quantum, prediction, distribution, probabilistic, 08"
  },
  {
    "filename": "7d5ddefa52bef7e6a90ea02d60ce29b1.txt",
    "text": "08 app maxim ziatdinov maxim ziatdinov building tool experimental material discovery 11 choose surrogate active active optimizes exploration large parameter space strategically selecting experiment conduct reducing resource consumption potentially accelerating scientific discovery key component approach probabilistic surrogate approximates functional relationship control parameter target property step us information gathered previous measurement update understanding relationship control parameter target property identify next combination parameter likely yield valuable information choosing surrogate one consider following factor total number control parameter ii operational latency requirement iii potential presence discontinuity non stationary behavior based experience find following surrogate particularly effective option consider gaussian process gp popular surrogate provides mathematically grounded estimate main limitation work directly input space without representation making struggle complex high also difficulty modeling non stationary phenomenon discontinuity due typical use stationary kernel limitation suit best low dimensional problem relatively small datasets predominantly smooth function deep kernel dkl extends gp adding feature representation via neural network enabling better handling high non stationary pattern still providing meaningful estimate however still limited gp scaling learned feature space su",
    "cluster": 5,
    "keywords": "network, neural, bayesian, bnns, weight, quantum, prediction, distribution, probabilistic, 08"
  },
  {
    "filename": "88a1d4fd44d90d22dddf2ad021d38150.txt",
    "text": "08 app amirabbas asadi mathematics student probabilistic ml rl high performance scientific computing 4 bayesian neural network weight weight type activation function number layer hidden dimension factor defining architecture network also treated random variable yet famous variant capture weight make inference easier larger neural network take weight account perform inference subset weight train rest deterministic parameter using usual optimizers video see trajectory hamiltonian monte carlo drawing sample posterior bnn classification problem left see neural network current hmc position right posterior marginals 119 3 deep dive iman 4 close point decision stable despite fluctuation slightly outcome place edge stable inner island expected good pedagogical purpose maybe active sampling new point minimize practical perspective principle able reconstruct predictable behaviour much cheaper good approximation much lower cost 1 alastair muir phd bsc bed mbb science consultant alastairmuir bsky social risk analysis optimization causal inference 4 thanks sharing amirabbas planning combine deterministic relationship nn function 1 24 904 526 career productivity finance soft skill emotional intelligence project management education 202",
    "cluster": 5,
    "keywords": "network, neural, bayesian, bnns, weight, quantum, prediction, distribution, probabilistic, 08"
  },
  {
    "filename": "91720b936c7cb6f3278fce0b51e6ac0f.txt",
    "text": "08 app vikash vikash tc engagement lead rpa specialist google cloud mba hr pmp pspo 1 bayesian neural network bayesian neural network bnns transforming way handle making prediction robust reliable incorporating unlike traditional neural network bnns treat weight distribution rather fixed value approach allows account prediction providing probability distribution possible outcome rather single point estimate improved robustness modeling bnns make informed decision especially situation noisy limited robustness crucial application medical diagnosis understanding confidence level prediction significantly impact decision making reliable prediction critical application autonomous driving financial forecasting ability quantify lead reliable prediction bnns help identifying uncertain allowing cautious considered action better generalization bnns often generalize better new unseen incorporate process lead less likely overfit capable performing well diverse scenario application various domain healthcare finance bnns used enhance reliability system ability provide estimate make invaluable field understanding risk confidence level essential bayesian neural network represent significant step forward making system reliable trustworthy particularly critical application play key role tag bayesianneuralnetworks machinelearning uncertain",
    "cluster": 5,
    "keywords": "network, neural, bayesian, bnns, weight, quantum, prediction, distribution, probabilistic, 08"
  },
  {
    "filename": "a6134d7a7b94a38cf6da31fd62615b28.txt",
    "text": "08 app maxim ziatdinov maxim ziatdinov building tool experimental material discovery heteroskedastic bayesian neural network use standard bayesian neural network bnns replace point estimate weight probability distribution capture network typically assume homoskedastic noise meaning observation measurement noise constant across input however real world scientific measurement frequently exhibit input dependent noise level failing account bayesian neural network lead unreliable estimate neurobayes package developing address challenge offering two type heteroskedastic bnns 1 noise based heteroskedastic bnn many scientific engineering application domain expert may prior knowledge noise level vary input neurobayes allows incorporate knowledge noise based heteroskedastic bnn approach particularly valuable small regime leveraging domain expertise significantly improve performance 2 two head heteroskedastic bnn less standard architecture infers noise level alongside function value without relying prior knowledge flexible option suitable scenario domain specific noise information unavailable difficult however care taken approach may require larger datasets reliably learn function noise prone overfitting small regime explore example try method noise based heteroskedastic bnn two head heteroskedastic bnn 56 8 utkarsh pratiush graduate research assistant machine quantum physic material ex mindtre",
    "cluster": 5,
    "keywords": "network, neural, bayesian, bnns, weight, quantum, prediction, distribution, probabilistic, 08"
  },
  {
    "filename": "c6e7e31f1edd53f184770b03d7cd876e.txt",
    "text": "08 app anuj mubayi anuj mubayi distinguished iba fellow phd expertise driven strategy utilizing mathematical statistical modeling heor 2 bayesian neural network v mcmc neural net often used historical predict outcome purpose hungry different dynamical system primarily find pattern past project forward standard neural network nns used frequentist way training weight minimize error consist layer neuron node connected weight dial neuron small calculation pass stacking layer allows network learn complex pattern process start random weight predict compare true answer adjust weight slightly repeat many time weight settle backpropagation gradient descent analogy like teaching child recognize cat initial guess random repeated feedback improves recognition bayesian neural network bnns extend neural net embedding bayesian inference inside instead treating weight fixed bnns treat uncertain prior posterior distribution prediction become distribution credible interval single number example tomorrow admission 500 95 chance lie 300 700 critical policy prepare average worst case bound benefit prevents overfitting quantifies combine flexibility bayesian rigor work simplified 1 start prior weight 2 train update belief posterior using bayes theorem 3 prediction averaging many possible network mcmc computational method bayesian inference exact math impossible instead giving one neat",
    "cluster": 5,
    "keywords": "network, neural, bayesian, bnns, weight, quantum, prediction, distribution, probabilistic, 08"
  },
  {
    "filename": "dbbce2e59d88935dce3c827c5778e8f5.txt",
    "text": "08 app swarnendu chatterjee swarnendu chatterjee statistic manager gsk biostatistics expert advanced r shiny formerly novartis 8 exciting news bnns officially cran thrilled announce r package bnns bayesian neural network stan accepted available cran bnns bnns provides flexible formula based interface building training bayesian neural network bnns using stan empowers user complex relationship quantifying critical feature high stake field like clinical trial finance use bnns easy use formula interface leverage power bayesian inference robust prediction support custom prior distribution weight parameter provides posterior distribution quantification suitable regression binary classification multi class classification problem benchmark performance tested popular datasets application whether working small dataset limited observation need interpretable probabilistic prediction domain bnns designed deliver powerful result get started install bnns directly cran install package bnns learn explore package documentation example see bnns fit science workflow would love hear thought feedback try bnns together let build community bayesian enthusiast push boundary science feel free share post network let spread word bnns bnns rstats bayesianneuralnetworks datascience machinelearning cran bayesian neural network stan swarnendu stat github io 117 mayur kharche p",
    "cluster": 5,
    "keywords": "network, neural, bayesian, bnns, weight, quantum, prediction, distribution, probabilistic, 08"
  },
  {
    "filename": "e07f09bd983615241ca4a6ad20720ddb.txt",
    "text": "08 app pushpendra singh pushpendra singh sophomore iit guwahati 1 bayesian neural network scratch numpy mcmc past week working building bayesian neural network bnn scratch using numpy deep framework project gave chance contrast frequentist neural network bayesian approach result insightful bayesian neural network traditional neural network give u point estimate weight single value prediction powerful naturally provide estimate contrast bnns treat weight probability distribution giving u built quantification epistemic aleatoric better regularization prior weight robust decision making critical domain autonomous driving healthcare finance robotics etc built implemented fully connected neural network backpropagation softmax cross entropy used markov chain monte carlo mcmc langevin informed metropolis hastings algorithm sample posterior distribution weight designed function encode decode parameter evaluate proposal compute posterior predictive distribution result iris dataset frequentist nn train accuracy 98 33 test accuracy 100 bayesian nn train accuracy 99 1 test accuracy 96 7 frequentist nn hit perfect accuracy test set likely due small dataset bnn provided richer insight distribution prediction single point confidence interval classification built robustness overfitting beyond classification also extended work regression task bayesian approach really shine providing prediction interval along mean estimate bnn capture way standard regression canno",
    "cluster": 5,
    "keywords": "network, neural, bayesian, bnns, weight, quantum, prediction, distribution, probabilistic, 08"
  },
  {
    "filename": "f3fbc93d73b8c969363e1c0221735aef.txt",
    "text": "08 app maxim ziatdinov maxim ziatdinov building tool experimental material discovery partially bayesian neural network balancing computational cost quantification fully bayesian neural network bnns provide robust quantification treating network weight probability distribution feature critical scientific engineering application however significant computational overhead often make impractical variational inference common approximation method bnns aim alleviate cost experience often struggle limited expressivity underestimation sensitivity initialization hyperparameters degrades performance real world task partially bayesian neural network emerge alternative solution maintaining probabilistic weight subset network approach preserve many benefit full bnns substantially reducing computational cost new neurobayes package implement partial bnns two stage first train deterministic neural network incorporating stochastic weight averaging end training trajectory enhance robustness noisy training objective second probabilistic component introduced selecting subset layer using corresponding pre trained weight initialize prior distribution subset keeping remaining weight frozen hamiltonian monte carlo sampling applied derive posterior distribution selected subset finally prediction made combining probabilistic deterministic component explore example feel free reach question feedback 46 3 maxim ziatdinov building tool experimental material discovery one obvious question",
    "cluster": 5,
    "keywords": "network, neural, bayesian, bnns, weight, quantum, prediction, distribution, probabilistic, 08"
  },
  {
    "filename": "11bb73f9820e02a7c91d6f79dad3ab51.txt",
    "text": "08 app bryan shalloway bryan shalloway science meta 1 good concept aware aleatoric v epistemic aleatoric whereas epistemic estimation epistemic go increase sample size whereas aleatoric kind stuck assuming right type relatedly somewhat common mistake realm prediction confusion confidence interval prediction interval broadly speaking confidence interval estimation prediction e epistemic however prediction interval reflect estimation individual sample observation e epistemic well aleatoric wrote though using term 2021 post understanding prediction interval 22 14 rahul vishwakarma 1 2 lynd bacon phd mba quantitative cognitive scientist science analytics expert lyndbacon masto mastodon 1 confusion regarding outcome inferential least decision maker using result important decision support consideration package estimate given intended use audience estimate communicated empirical evidence indicating decision maker always accurately perceive cognitively process estimate true even quantitative method expert may confused different kind well client colleague understand decision making perspective another distinction im",
    "cluster": 6,
    "keywords": "bayesian, aleatoric, epistemic, like, method, prediction, 08, app, deep, decision"
  },
  {
    "filename": "14d089bccc519b267f5ddce2fe782239.txt",
    "text": "08 app warren powell warren powell warren powell professor emeritus princeton university co founder optimal dynamic executive residence rutgers business school 6 teaching sequential decision analytics ix source going start identifying different form one way grab six pack beer sit around white board give process structure recommend chapter book reinforcement stochastic optimization list 12 class perspective list example supply chain management written purely perspective modeler idea use list think problem domain identify seem belong class note overlap every problem area uncertainty class complex problem health energy supply chain management list form relative application mean going incorporate think help work complete list chapter provides detail illustrates use context covid pandemic take look tinyurl com rlsochapter10 101 2 juan pablo fern ndez guti rrez doctor en modelaci n computaci n cient fica 6 warren powell put algorithm instability source kevin corella nieto strategic system architect decision framework quantum project portfolio project management pfmp pmp 6 hi prof warren powell reflecting 12 source outlined chapter reinforcement stochastic optimization",
    "cluster": 6,
    "keywords": "bayesian, aleatoric, epistemic, like, method, prediction, 08, app, deep, decision"
  },
  {
    "filename": "2809e44f3cc66297102227c08d74befe.txt",
    "text": "08 app patrick nicolas patrick nicolas geometric deep topology graph differential geometry principal engineer ex director engineering 6 great point precise terminology aleatoric measure noise associated measurement cannot removed collecting quality aleatoricuncertainty sreenivas b director head digital solution zeiss 6 deep say 99 confident trust always high confidence mean reliable prediction production system need quantify two type aleatoric measure noise ambiguity input epistemic measure whether input outside training distribution plot show aleatoric distribution clean versus noisy notice clear separation standard accuracy metric completely miss real deployment knowing trust critical knowing predicts estimate drive decision rule auto accept flag human review reject outright detailed tutorial implementation coming soon machinelearning deeplearning uncertaintyquantification 7 sreenivas b director head digital solution zeiss 6 deep say 99 confident trust always high confidence mean reliable prediction production system need quantify two type aleatoric measure noise ambiguity input epistemic measure whether input outside training distribution plot show aleatoric distribution clean versus noisy notice clear separation standard accuracy metric completely miss real deployment knowing trust",
    "cluster": 6,
    "keywords": "bayesian, aleatoric, epistemic, like, method, prediction, 08, app, deep, decision"
  },
  {
    "filename": "284f460d505e04eb2f910cd991e18823.txt",
    "text": "08 app michael kirchhof michael kirchhof research scientist apple quantification proud announce neurips spotlight work year dig decomposing aleatoric epistemic hard mean future quantification easy solution would course use decomposition formula like predictive aleatoric epistemic however find work practice two component literally see scatterplot happens second order distribution implemented evidential deep laplace approximation deep ensemble rank correlation two component always within 0 8 0 999 could estimate epistemic aleatoric epistemic find estimator trained explicitly ood best ood detection maybe go away general method towards specialized one aleatoric clear winner exists yet seems challenging task also depend exactly aleatoric ground truth collected dataset also test multiple predictive metric auroc auac raulc ece best method always depends exactly metric test thus kind predictive need solution practical task learn past year research need rethink aleatoric epistemic dichotomy estimation rich nuanced field whole spectrum type uncertainty provide precise description task want solve estimator build specialized towards exactly th",
    "cluster": 6,
    "keywords": "bayesian, aleatoric, epistemic, like, method, prediction, 08, app, deep, decision"
  },
  {
    "filename": "32aecc3380337b52e758bfdea549477c.txt",
    "text": "08 app bayesian statistic bayesian statistic 3 621 2 deep might confidently wrong even know high stake field like medicine epidemiology wrong prediction serious consequence knowing uncertain matter much prediction episode alexandre andorra talk lodie monod fran ois xavier briol yingzhen li latest bayesian deep statistician mindset make neural network reliable diffusion method quantification computational challenge bayesian neural network marginal likelihood mislead selection spotting distribution example early exploration exploitation trade reinforcement applying bayesian method large language generalised bayesian inference may robust listen bayesianstats deeplearning machinelearning datascience uncertaintyquantification 101 1 oliver ratmann 2 pleasure organise absolute treat working alexandre andorra 4 3 621 latest development deep application latest trend machine deep breakthrough trend challenge benefit deep challenge training neural understanding diagnosis prediction reliability career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 6,
    "keywords": "bayesian, aleatoric, epistemic, like, method, prediction, 08, app, deep, decision"
  },
  {
    "filename": "35c60251ab228193dbff6aaae87e638a.txt",
    "text": "08 app warren powell warren powell warren powell professor emeritus princeton university co founder optimal dynamic executive residence rutgers business school 4 long history people promoting either bayesian frequentist statistical people hiss mentioned wrong perspective never figured use sometimes time bayesian powerful method working problem make decision limited frequentist statistic go methodology readily available chapter 3 rlso book called online cover bayesian frequentist statistic without hint discussion better tinyurl com rlandso chapter 3 downloaded confused existence entire book focusing purely one style physician going use experience aka bayesian prior making decision best course treatment forecaster going use time series modeling perform demand forecast frequentist statistic paper modeling wind energy capture crossing time long actual forecast used hidden markov used unobservable state variable describe whether short medium long crossing time would bayesian belief combined frequentist updating time series statistical arises throughout sequential decision problem whether forecasting optimizing parameter estimating value state approximating policy sometimes bayesian belief appropriate often using frequentist modeling want complete analytics toolbox",
    "cluster": 6,
    "keywords": "bayesian, aleatoric, epistemic, like, method, prediction, 08, app, deep, decision"
  },
  {
    "filename": "3dfd6d2ed1c7e1a3c7473b2390940903.txt",
    "text": "08 app reza abdi reza abdi ml computational science engineer ph p e gen enthusiast 3 dealing deterministic probabilistic modeling grad school always thinking whether going get address properly yes interesting see sneak even try make predictable think people deal daily ask question twice might get slightly different answer thinking machine lab new work aim provide u insight tackle nondeterminism llm making output reliable repeatable simple term research piece say llm crunch number using floating point math vary subtly due calculation ordered like adding number different sequence getting tiny rounding difference tech folk often point gpu concurrency thread computing sync cause lab led horace dig deeper real issue batch size server query get grouped others efficiency change group size based asking math subtly shift altering result fix redesign core operation like normalization rmsnorm matrix multiplication attention mechanism ignore batch size honeslty quitete clear obviously understand like standardizing recipe cake taste whether bake one dozen technically enforce fixed parallelization strategy avoiding variable split cause inconsistency test like deepseek v3 1 llama 3 1 showed identical output across thousand run 20 speed dip everyday user mean trustworthy flip flopping advice developer supercharges training like rlhf",
    "cluster": 6,
    "keywords": "bayesian, aleatoric, epistemic, like, method, prediction, 08, app, deep, decision"
  },
  {
    "filename": "80e66cbc178b865705f4b6b00feda6f3.txt",
    "text": "08 app christoph molnar christoph molnar interpretable machine modeling mindset christophmolnar com 1 machine often distinguish aleatoric epistemic aleatoric called irreducible error caused true randomness epistemic reducible caused lack information like misspecification simple example different dice predict outcome rolling one feature type dice let say per dice count number site give site equal probability prediction would aleatoric use always predicts outcome 1 6 probability 1 6 would good could say epistemic going back aleatoric really aleatoric knew physical factor dice roll might able predict lack knowledge attribute epistemic like kid asking repeated question reducing aleatoric epistemic would quickly become unproductive better differentiation epistemic aleatoric make conditional p x x see p x x define aleatoric conditional variance v x x epistemic simply remaining variance conditioning feature pin aleatoric epistemic want dive deeper recommend paper source machine statistician view gruber et al 2023 156 22 maria moreno de castro researcher scientist teacher 1 great post usual would like",
    "cluster": 6,
    "keywords": "bayesian, aleatoric, epistemic, like, method, prediction, 08, app, deep, decision"
  },
  {
    "filename": "81a7d5e8d738150fe59de8991deb63ce.txt",
    "text": "08 app paul perera paul perera co founder director hyflux mymaskfit mba beng 2024 uplink world economic forum top innovator 1 quantification uq important quantification help u gain insight unpredictable element within leading better decision making product design process improvement also enables accurate prediction considering uncertainty affect current system two classification 1 aleatoric originates inherent randomness system like measurement error natural variation type unpredictable 2 epistemic stem limited knowledge system unlike aleatoric reduced gathering enhancing improving system understanding primary source 1 measurement error random systematic 2 modeling limitation misspecification simplification inaccuracy 3 scarcity incompleteness limited missing 4 parameter parameterization 5 natural variability inherent variability physical process 6 computational error error due numerical computation limitation conclusion understanding managing uq technique enhances reliability decision making process platform like twinlab developed digilab offer advanced uq method making sophisticated management accessible engineer scientist professional understanding quantification different type digilab co uk 22 2 tim dodwell founder ceo digilab prof ml 1 great meet today pleased digilab passion qu",
    "cluster": 6,
    "keywords": "bayesian, aleatoric, epistemic, like, method, prediction, 08, app, deep, decision"
  },
  {
    "filename": "a0eacd2e453bd08136695eab8d02799f.txt",
    "text": "08 app shekhar khandelwal shekhar khandelwal applied scientist applied bayesian ml genai 2 recently delved deep bayesian regression interval first single point prediction frequentist approach limiting like predicting tomorrow temperature exactly 72 f 68 f 76 f point prediction give u full story enter bayesian instead single prediction bayesian offer range saying something like 95 sure temperature 68 f 76 f powerful capture inherent variability interval initially thought shaded region bayesian regression plot indicates 95 lie twist shaded region represents prediction spread observed insight linear regression fit left show fit linear regression observed less confident bayesian regression fit middle acknowledges prediction credible interval better capture variability confident bayesian regression fit right narrower shaded region credible interval confident prediction beware confidence equate correctness confidently wrong takeaway always validate confidence actual outcome confidence valuable well calibrated world modeling confidently correct ultimate goal confident read article bayesian modelling valuable building marketing mix mmm datascience bayesian modelinginsights mmm",
    "cluster": 6,
    "keywords": "bayesian, aleatoric, epistemic, like, method, prediction, 08, app, deep, decision"
  },
  {
    "filename": "db35646415ca90c316e9686a78e1f453.txt",
    "text": "08 app jordan white jordan white machine engineer computer vision specialist passionate real time system driven solution 1 lately working bayesian belief network bbns got sparse messy injury record environmental condition situational factor thing know lot traditional ml like bbn flip script graph variable linked probabilistic relationship node conditional probability table say given x likelihood feed evidence whole network update mean even half missing still reason rest someone suffers x injury change probability part environment logged still infer building python pgmpy define structure fill probability run inference suddenly got system adapts incomplete instead shutting like feel closer actually think rarely perfect information still need make decision sometimes classic like bayesian method right fit even field obsessed latest deep curious anyone else leaned probabilistic far perfect 22 ammar elbedweihy scientist engineer generative nlp computer vision private tutor ksa w 5 year experience researcher scientific modeling philosopher 1 key takeaway correlation understandable equation relates amount change x two variable consistent change high correlation otherwise lower correlation scott hartshorn linear regression correlation beginner guide quo",
    "cluster": 6,
    "keywords": "bayesian, aleatoric, epistemic, like, method, prediction, 08, app, deep, decision"
  },
  {
    "filename": "dc34206ef4b0290ef1b11805d3be887c.txt",
    "text": "08 app moriba jah moriba jah celestial steward co founder chief scientist astrodynamicist macarthur genius fellow ted fellow intfrse professor rennaiscientist global speaker view affiliated organization 1 bayesians v frequentists come agree others tow line bayesian theory useful many thing modeled random many time used belief ignorance freqentist theory also useful powerful process repeat cyclical etc experience seen fail understand quantification uq aleatory epistemic aleatory due randomness assuming thing epistemic due systematic factor ignorance posit one cannot know something extent beyond aleatory definition wit knowledge end randomness begin something truly random amount type help u know yet bulk bayesian inference allows thing reduced converted knowledge aleatory rather epistemic soon say random given replacing ignorance randomness giving easy route never actually know randomness actually exists nobody assume prevent getting complacent ignorance keep always pursuit greater knowledge science defined use bayesian sure knowing limitation whenever built always proper ponder basic scientific question really based artifact displaying prejudice creator rudolf emil kalman",
    "cluster": 6,
    "keywords": "bayesian, aleatoric, epistemic, like, method, prediction, 08, app, deep, decision"
  },
  {
    "filename": "e52ec09216efa6277430825bba136e8c.txt",
    "text": "08 app vincent fortuin vincent fortuin pi helmholtz faculty tum 6 really fun chat alexandre andorra thanks podcast bayesian statistic 3 622 6 science full promise latest episode alexandre andorra sat vincent fortuin explore still work combining estimation powerful improves reliability especially critical domain fine tuning llm bayesian method result trustworthy output dominant library bayesian dl yet momentum growing application healthcare physic beyond already emerging incorporating domain expertise lead smarter efficient understanding one thing explaining clearly important subspace method pushing field forward road ahead progress requires across community focused episode well worth listening bayesiandeeplearning aiforscience machinelearning deeplearning bayesianinference 44 1 bayesian statistic 6 pleasure host vincent fortuin come back anytime 1 1 951 20 career productivity finance soft skill emotional intelligence project management education",
    "cluster": 6,
    "keywords": "bayesian, aleatoric, epistemic, like, method, prediction, 08, app, deep, decision"
  },
  {
    "filename": "ef49b0fa91d0dab246b18c829ff65f33.txt",
    "text": "08 app daniel daniel co founder cto dataflint apache spark copilot podcast host passionate math leadership machine episode review paper benchmarking disentanglement specialized uncertainty specialized task 1 michael kirchhof investigates whether current machine method effectively disentangle aleatoric epistemic uncertainty two critical type system corresponds inherent variability ambiguity similar interpolation deal situation within known distribution cannot resolved regardless additional improvement hand arises gap knowledge exposure akin extrapolation reflects encountering outside training distribution potentially reduced better modeling author benchmarked 19 quantification method across 13 task including distribution detection predictive calibration using large scale datasets like imagenet cifar traditional mathematical decomposition information theoretical formula aim separate uncertainty fail practice aleatoric epistemic component showing high correlation rank correlation 0 78 overlap suggests expected undermining practical utility application like active safety critical system paper highlight method like mahalanobis distance excel specific task detecting distribution sample without confounded aleatoric finding emphasiz",
    "cluster": 6,
    "keywords": "bayesian, aleatoric, epistemic, like, method, prediction, 08, app, deep, decision"
  },
  {
    "filename": "f55c2f2f0d1b35688228d72b7c4272c9.txt",
    "text": "08 app pymc lab pymc lab 5 975 7 make confident prediction even wrong real world difference epistemic aleatoric matter come missing knowledge reduced come randomness irreducible treating lead bad decision bayesian method bring bayesian linear regression gaussian process bayesian neural network probabilistic explicitly system know make decision making reliable probabilistic shine sequential decision making reduces collection cost focusing informative sample balance exploration v exploitation improving resource allocation integrates guide exploration safely theoretical idea recent eth z rich course probabilistic andreas krause jonas h botter dive deep method showing complement actively working forefront making predictive aware interpretable team working need better quantification reach book 30 genai airesearch bayesianinference probabilisticai 157 5 larry jones 7 interesting idea 1 mehran bazargani ph marie curie postdoc fellow free energy principle deep 7 interesting distinction different kind",
    "cluster": 6,
    "keywords": "bayesian, aleatoric, epistemic, like, method, prediction, 08, app, deep, decision"
  },
  {
    "filename": "f566a0c5e0cc8a13d3e9bf5766f1635b.txt",
    "text": "08 app simon see simon see important piece work ability decompose aleatoric epistemic allow u better insight qualify michael kirchhof research scientist apple quantification proud announce neurips spotlight work year dig decomposing aleatoric epistemic hard mean future quantification easy solution would course use decomposition formula like predictive aleatoric epistemic however find work practice two component literally see scatterplot happens second order distribution implemented evidential deep laplace approximation deep ensemble rank correlation two component always within 0 8 0 999 could estimate epistemic aleatoric epistemic find estimator trained explicitly ood best ood detection maybe go away general method towards specialized one aleatoric clear winner exists yet seems challenging task also depend exactly aleatoric ground truth collected dataset also test multiple predictive metric auroc auac raulc ece best method always depends exactly metric test thus kind predictive need solution practical task learn past year research need rethink aleatoric epistemic dichotomy estimation rich nuanced field whole spectrum type uncer",
    "cluster": 6,
    "keywords": "bayesian, aleatoric, epistemic, like, method, prediction, 08, app, deep, decision"
  },
  {
    "filename": "f931e8bcb457787d5e8da6d429bcb127.txt",
    "text": "08 app perseverence savieri phd perseverence savieri phd biostatistician statistical consultant r shiny developer 8 frequentist embracing bayesian method journey biostatistics someone began statistical journey firmly frequentist paradigm grown deeply appreciate elegance practicality bayesian method shift inspired gem like textbook evolution biostatistics recent year nothing short revolutionary bayesian approach playing starring role paradigm resonates might bayesian framework ability incorporate prior knowledge update belief new emerges aligns seamlessly iterative nature scientific research unlike frequentist method often treat parameter fixed unknown bayesian statistic quantifies probabilistically natural fit complex real world biomedical problem advance markov chain monte carlo mcmc algorithm tool like stan transformed bayesian analysis theoretical ideal daily workhorse text brilliantly bridge theory application fellow biostatisticians incorporated bayesian approach work computational tool found effective biostatistics bayesianstatistics datascience clinicalresearch 188 14 prof innocent maposa biostatistician scientist mathematical modelling epidemiologist better biostatistics better clinical research c3 nrf rated scientist 8 one best text book bayesian method 3 siddhanta subedi graduated tribhuvan university major statistic 8 way get pdf format anyone real",
    "cluster": 6,
    "keywords": "bayesian, aleatoric, epistemic, like, method, prediction, 08, app, deep, decision"
  },
  {
    "filename": "f987b6d68d01f10d36a21ca14bae482f.txt",
    "text": "08 app sreenivas b sreenivas b director head digital solution zeiss 6 deep say 99 confident trust always high confidence mean reliable prediction production system need quantify two type aleatoric measure noise ambiguity input epistemic measure whether input outside training distribution plot show aleatoric distribution clean versus noisy notice clear separation standard accuracy metric completely miss real deployment knowing trust critical knowing predicts estimate drive decision rule auto accept flag human review reject outright detailed tutorial implementation coming soon machinelearning deeplearning uncertaintyquantification 81 9 marius van den hombergh enterprise search rag deep research zeta alpha architecting secure agent neural discovery platform knowledge intensive enterprise 4 looking forward detailed tutorial sreenivas note estimate drive accept review reject policy work rag evaluation run tension noisy v distribution error often need different response think similar distinction apply outside supervised prediction retrieval generation workflow kannan rama technical advisor snowface co ltd 6 looking forward ajarn remember unet series almost 4 year working use case image regression used kera unet subsequently changed torch unet facing exactly talking forthcoming post topic request rewind back unet series",
    "cluster": 6,
    "keywords": "bayesian, aleatoric, epistemic, like, method, prediction, 08, app, deep, decision"
  },
  {
    "filename": "035e258f2bab9bccbe2824302d0a32b9.txt",
    "text": "08 app arne schmidt arne schmidt team lead senior scientist medical image 1 happy announce article focused active histopathological image classification published journal medical image analysis active al potential solve major problem digital pathology efficient acquisition labeled machine algorithm however existing al method often struggle realistic setting artifact ambiguity class imbalance commonly seen medical field lack precise estimation lead acquisition image low informative value address challenge propose focused active focal combine bayesian neural network distribution detection estimate different uncertainty acquisition function specifically weighted epistemic account class imbalance aleatoric ambiguous image ood score artifact perform extensive experiment validate method mnist real world panda dataset classification prostate cancer result confirm al method distracted ambiguity artifact harm performance focal effectively focus informative image avoiding ambiguity artifact acquisition experiment focal outperforms existing al approach reaching cohen kappa 0 764 0 69 labeled panda focused active histopathological image classification sciencedirect com 59 1 datascience show 1 amazing achievement contribution enhancing digital pathology truly groundbreaking arne schmidt 808 37 career productivity finance soft skill emotion",
    "cluster": 7,
    "keywords": "deep, segmentation, medical, image, phd, imaging, university, estimation, 08, work"
  },
  {
    "filename": "0c335a2a1e9ede381b65cd483e54c5d6.txt",
    "text": "08 app mevan ekanayake mevan ekanayake postdoctoral researcher deep cancer research medical imaging bioinformatics check latest paper journal imaging informatics medicine deep incredible come uncertainty especially true mri reconstruction deep struggle certain region image research introduces pixcue pixel classification estimation groundbreaking approach estimation mri reconstruction pixcue generates reconstructed image map single forward pas deep significantly reducing computational cost co author kamlesh pawar phd zhaolin chen zhifeng chen gary egan read full paper research deeplearning mri medicalimaging uncertaintyestimation pixcue aiinhealthcare innovation pixcue joint estimation image reconstruction mri using deep pixel classification journal imaging informatics medicine link springer com 41 4 kh tohidul islam phd lead researcher australian point care mri project deep medical imaging phd computer vision congratulation 1 anthony hannides founder heal music nh cep award winning songwriter producer co host life changing project interesting 2 sahan dissanayake w eit engineer training eit egbc b sc eng hons mba amie sl eng sl mieee miet electrical electronic engineering professional 9 great work 1 284 39 deep application mri imaging latest development deep application deep breakthrough trend emerging innovation imaging machine impact hea",
    "cluster": 7,
    "keywords": "deep, segmentation, medical, image, phd, imaging, university, estimation, 08, work"
  },
  {
    "filename": "2d7537b953ea74e8f091e1e4ccc86bf5.txt",
    "text": "08 app gabriela czanner gabriela czanner professor digital health university southampton expert applied informatics medical statistic chartered statistician cstat rss member bcs november 5th gabriela czanner silvester czanner hosted masterclass diving theme bridging neuroscience understanding unconscious faculty informatics information technology slovak university technology event part mit fiit project aware artificial intelligence detection brain eye disease funded mit international science technology initiative misti global seed fund project spearheaded professor emery n brown brown harvard mit gabriela czanner faculty informatics information technology slovak university technology liverpool john moore university incredible team including sirma orguc mit silvester czanner faculty informatics information technology slovak university technology university chester martina billichov billichova phd student faculty informatics information technology slovak university technology michal l ley phd student faculty informatics information technology slovak university technology fiit stu honour professor emery n brown brown harvard mit joining u online present deciphering dynamic unconscious brain general anesthesia took u mind blowing journey revealing team capture brain dynamic state transition anaesthesia explored mathematical statistical tool used analyse opening new door understanding mystery unconsciousness next gabriela czanner fiit stu ljmu uk spoke quantification prediction health introdu",
    "cluster": 7,
    "keywords": "deep, segmentation, medical, image, phd, imaging, university, estimation, 08, work"
  },
  {
    "filename": "4c21104e96dd601684f94db1a37baf75.txt",
    "text": "08 app thomas jubault phd mba thomas jubault phd mba advisor private market psp investment machine analysis lecturer hec montr al 2 thrilled announce publication scientific article privilege co authoring working neurorx title deep ramping estimation detecting artifact large imbalanced database mri image goal using deep assist automatic quality control qc incoming mri ensuring accuracy consistency reliability analysis performed line typical challenge company effectively leverage immense amount accumulated training able circumvent obstacle via fine understanding stored could translated tagging de facto article cover many others aspect development real life implementation type solution highly complex environment achievement would possible without relentless effort collaboration brilliant team would like extend gratitude haz edine assemlal ricardo pizarro deep ramping estimation detecting artifact large imbalanced database mri image sciencedirect com 71 3 lucie lauzon sp cialiste application en irm 2 f licitations thomas adam schwarz global head imaging digital pharmaceutical diagnostics ge healthcare 2 excellent thomas really important topic important potential impact new ml approach nut bolt area imaging turn key enablers downstream analysis read whether based p 26 figure kind record 3 dani",
    "cluster": 7,
    "keywords": "deep, segmentation, medical, image, phd, imaging, university, estimation, 08, work"
  },
  {
    "filename": "5142d695a5dfc6b917f478ed3a92140a.txt",
    "text": "08 app sue sarafrazi phd sue sarafrazi phd principal scientist genentech adjunct faculty university san francisco 3 come longing certainty fall love certainty come rumi medical imaging high stake domain expert opinion often diverge error ambiguity inherent task traditional typically treat noise forcing single truth none may exist latest work take fundamentally different approach rather suppressing embrace present bayesian modeling framework explicitly account disagreement among annotator modeling epistemic aleatoric system iteratively refines noisy label improves performance converging toward consensus without assuming perfect agreement principled probabilistic approach boost predictive accuracy 11 also aligns reality clinical human disagreement enables system know know crucial clinical deployment proud collaborate shiva fayaz sven reisdorf pushing boundary trustworthy link full paper bayesianlearning medicalimaging uncertaintyquantification machinelearning aiforhealth labelrefinement trustworthyai harnessing power bayesian neural network annotator consensus refinement enhance meibomian gland dysfunction classification ieeexplore ieee org 21 5 rohini c healthcare analyst medical informaticist bridging clinical expertise health tech 3 thanks sharing sue 1 ayushi gupta analyst strategy insight pharma analytics digital health informatics clinical analysis kol hcp thought leader analytics medical affair h1",
    "cluster": 7,
    "keywords": "deep, segmentation, medical, image, phd, imaging, university, estimation, 08, work"
  },
  {
    "filename": "547f1a8600a5249c910523df89d927ee.txt",
    "text": "08 app vera damerjian pieters phd vera damerjian pieters phd founder veradp phd computer vision consulting medtech technical management education mentorship 4 delighted share latest article automatic segmentation stage iv non small cell lung cancer nsclc confidence score using deep ensemble also proposed quantify prediction order automate identification complex case require review result demonstrate potential attention based architecture specific preprocessing strategy improve segmentation quality challenging clinical scenario emphasizing importance estimation build trustworthy system medical imaging kudos phd candidate sacha dedeken exceptional work full paper read j r faure olivier gallinato pierre henri conze thierry colin dimitris visvikis nsclc trustworthyai deeplearning oncology stageivnsclc segmentation uncertaintyestimation confidencescore medicalimaging sophia genetics 128 999 4 excited share recent publication computerized medical imaging graphic highlighting groundbreaking work latim laboratory medical information processing collaboration sophia genetics tackle complexity advanced lung cancer imaging medical imaging critical across lung cancer patient journey underscoring need robust reliable automated radiomics segmentation tool support clinical decision making improve patient care advanced nsclc study team developed high performance driven tumor segmentation using diverse real world imaging dataset advance accuracy trust imaging analysis read full article spotlight learn work brings u close",
    "cluster": 7,
    "keywords": "deep, segmentation, medical, image, phd, imaging, university, estimation, 08, work"
  },
  {
    "filename": "5beaf8d29f8b21e3ff22f4ec19e5ded5.txt",
    "text": "08 app jian xun wang jian xun wang associate professor cornell university 1 excited share new paper science advance powered automated construction patient specific cfd aortic flow building vascular medical image patient specific cfd simulation typically manual slow inconsistent work present end end deep solution generates simulation ready directly image reducing human effort improving reproducibility new logb net bayesian segmentation network capture main aorta small branch state art accuracy gnn lddmm graph neural network mesh deformation aligning seamlessly image gradient improving geometric fidelity cfd quantification explicitly segmentation demonstrate propagates cfd output pressure wall shear stress read open access article pan du delin chaoli wang sciml hemodynamics comsail uq patientspecific ai4science powered automated construction patient specific cfd simulation aortic flow science org 128 1 xu hui zhou ph candidate fluid dynamic scientific machine assimilation 1 congrats 1 pan du phd candidate exploring ai4health 1 thrilled share latest paper published science advance powered automated construction patient specific cfd aortic flow work collaboration delin prof chaoli wang prof jian xun wang university notre dame cornell university designed accelerate traditional manual aorta segmentation introducing several uniquely important feature bayesian segmentation uq empl",
    "cluster": 7,
    "keywords": "deep, segmentation, medical, image, phd, imaging, university, estimation, 08, work"
  },
  {
    "filename": "9066f429616dc377c4900c727c362811.txt",
    "text": "08 app christian jamtheim gustafsson phd christian jamtheim gustafsson phd associate professor docent senior medical physicist researcher scientist team leader consultant 1 today recieved spectacular news regarding publication paper impact deep manual correction mri based auto segmentation prostate cancer radiotherapy deep based auto segmentation organ radiotherapy planning becoming standard way clinical setting however deep assessed used workflow proud led great team included multiple internationelly recognized radiotherapy researcher first assess impact segmentation clinical setting work conclude deep information presented experienced radiation oncologist influence decision making quality perception confidence dl segmentation map presented region low less likely edited indicating increased reliance prediction additionally map improve efficiency reducing segmentation time dl based segmentation valuable tool clinical practice enhancing efficiency radiotherapy planning whole paper open access format found radiotherapy planning segmentation deeplearning journal applied clinical medical physic aapm journal wiley online library aapm onlinelibrary wiley com 56 5 christian jamtheim gustafsson phd associate professor docent senior medical physicist researcher scientist team leader consultant 1 viktor rogowski christian jamtheim gustafsson phd associate profes",
    "cluster": 7,
    "keywords": "deep, segmentation, medical, image, phd, imaging, university, estimation, 08, work"
  },
  {
    "filename": "946fd183beb3b7d516641c02dc2c8779.txt",
    "text": "08 app winston chen winston chen ml 11 published new preprint arxiv 2411 05324 saswise ue segmentation synthesis interpretable scalable ensemble estimation goal making deep medical imaging interpretable reliable clinical use blockwise dropout typical dropout introduces randomness weaken performance disrupting coordination layer saswise ue use blockwise dropout grouping layer similar function distinct block assembling various candidate block design preserve stability enhancing estimation even boost performance passionate healthcare framework designed ensure perform well also communicate confidence making real difference patient care check full paper arxiv 2411 05324 curious learn let chat 22 4 winston chen ml 11 done without alan mcmillan thanks much guidance late night helping make research reality brayden schott ml medical imaging researcher special interest quantification safety cool paper congrats winston 1 523 28 addressing reliability challenge medical deep application mri imaging machine applied cardiology ensure trust medical output emerging innovation imaging radiology improving imaging analysis career productivity finance soft skill emotional intelligence project management education cooki",
    "cluster": 7,
    "keywords": "deep, segmentation, medical, image, phd, imaging, university, estimation, 08, work"
  },
  {
    "filename": "9f704067c703f431bdd1b476e184bd36.txt",
    "text": "08 app rohini banerjee rohini banerjee computer science machine cmu machine researcher auton lab 1 excited announce paper enhanced estimation ultrasound image segmentation msu net accepted 5th international workshop advance simplifying medical ultrasound asmus miccai2024 conference miccai society propose msu net multistage deep ensemble framework designed enhance femoral vessel segmentation ultrasound image quantify goal providing autonomous guidance phlebotomy task recent study demonstrates msu net significantly outperforms traditional monte carlo u net approach offering parallelized deep framework image segmentation 27 7 improvement mean dice score coefficient statistically significant enhancement sensitivity false negative reduction superior discrimination correct incorrect segmentation advancement come minimal additional training highlighting msu net efficiency effectiveness culmination research auton lab past several month grateful opportunity make impact dynamic field excited begin computer science continuing research cmu auton lab carnegie mellon university school computer science starting fall 2024 advised professor artur dubrawski pre print arxiv org ab 2407 21273 autonlab robotics institute carnegie mellon university author rohini banerjee cecilia morale artur dubrawski denotes equal contribution medicalimaging ultrasound machinelearning computervision medicalresearch vesselsegmentation miccai2024 asmus2024 enhanced estimation ultrasound image segmentation msu net arxiv org 96 8 eshna parth priva",
    "cluster": 7,
    "keywords": "deep, segmentation, medical, image, phd, imaging, university, estimation, 08, work"
  },
  {
    "filename": "be8bcbd7991e4073df02eb6943168f55.txt",
    "text": "08 app morris lee morris lee computer vision consultant available help r 70 patent 40 year experience artificial intelligence hitech technology passionate using latest advancement improve business 2 megan mixture expert robust estimation endoscopy video reliable quantification uq essential medical evidential deep edl offer computationally efficient way quantify alongside prediction unlike traditional method monte carlo mc dropout deep ensemble de however method often rely single expert annotation ground truth training overlooking inter rater variability healthcare address issue propose megan multi expert gating network aggregate estimate prediction multiple expert via edl trained diverse ground truth modeling strategy megan gating network optimally combine prediction uncertainty edl enhancing overall prediction confidence calibration extensively benchmark megan endoscopy video ulcerative colitis uc disease severity estimation assessed visual labeling mayo endoscopic subscore me inter rater variability prevalent large scale prospective uc clinical trial megan achieved 3 5 improvement f1 score 30 5 reduction expected calibration error ece compared existing method furthermore megan facilitated guided sample stratification reducing annotation burden potentially increasing efficiency consistency uc trial newsletter story linkedin ainewsclips ml artificialintelligence machinelearning computervision",
    "cluster": 7,
    "keywords": "deep, segmentation, medical, image, phd, imaging, university, estimation, 08, work"
  },
  {
    "filename": "cc7188d9aab96bf9091bb303052fe44e.txt",
    "text": "08 app nadieh khalili nadieh khalili multi modal 1 excited share work presented today 9 15 compayl workshop miccai society wsi bayesunet aware deep histopathological image segmentation active histopathological image segmentation key digital pathology supporting cancer detection subtype classification manual annotation however time consuming subjective making automation crucial deep automated much process still make silent mistake address proposing bayesian u net framework provides principled probabilistic estimation variational inference combined active iteratively improves segmentation prioritizing uncertain sample outperforms ensemble method tiger camelyon17 provides better quantification achieves faster convergence reduces annotation effort show based sampling random sampling proud yijun cui master timly topic driving work attend honor presenting poster behalf miccai2025 digitalpathology deeplearning activelearning 44 archie tan software engineer backend ml python java sql pytorch tensorflow 25k funded research generative 1 big news excited share paper generate realistic medical image improve pancreatic cancer segmentation accepted 39th ccsc southeastern conference november mercer university also published journal consortium computing science college included acm digital library project grew simple question could generate synthetic ct scan help train whe",
    "cluster": 7,
    "keywords": "deep, segmentation, medical, image, phd, imaging, university, estimation, 08, work"
  },
  {
    "filename": "ccff7749c92e8d05315dab384270fd01.txt",
    "text": "08 app sikha k sikha k phd post doctoral researcher bcn medtech upf barcelona 3 honored present work meta unet enhancing skin lesion segmentation multimodal feature integration estimation car computer assisted radiology surgery conference berlin germany study enhanced u net architecture integrating lesion specific metadata enabling deliver confident interpretable segmentation result explored metadata fusion help perform better also express certainty prediction crucial step toward safe trustworthy healthcare grateful amazing co author alaysia leilanistone miguel angel gonzalez ballester invaluable guidance collaboration cars2025 medicalai imagesegmentation metaunet uncertaintyestimation deeplearning medicalimaging upf mit healthcareinnovation 171 5 krishnakumar ph assistant professor robotics automation srec ph image processing passionate educator researcher innovation ambassador 3 hi sikha share github link dr daison stallon samuel raj tech mba phd pdf spain researcher academician technical speaker renewable energy specialist electrical electronics engineer world traveler connected various university around 3 congrats sikha k 1 miguel angel gonzalez ballester icrea research professor 3 thanks fantastic work sikha alaysia 1 1 501 94 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 7,
    "keywords": "deep, segmentation, medical, image, phd, imaging, university, estimation, 08, work"
  },
  {
    "filename": "d9f8da2f8d118d9fa88585d0b37f7f48.txt",
    "text": "08 app ricardo pizarro ricardo pizarro solution medical imaging 1 excited share medical image analysis publication time postdoctoral fellow mcgill university developed deep dl algorithm automatically detect artifact magnetic resonance imaging mri datasets came ramping technique avoid overwhelming algorithm training addition incorporated metric provide level confidence inference work extremely helpful working large mri database reduce time effort otherwise spent technician collaboration professor amir shmuel douglas arnold huge contribution talented haz edine assemlal check corresponding github nut bolt thomas jubault phd mba advisor private market psp investment machine analysis lecturer hec montr al 2 thrilled announce publication scientific article privilege co authoring working neurorx title deep ramping estimation detecting artifact large imbalanced database mri image goal using deep assist automatic quality control qc incoming mri ensuring accuracy consistency reliability analysis performed line typical challenge company effectively leverage immense amount accumulated training able circumvent obstacle via fine understanding stored could translated tagging de facto article cover many others aspect development real life implementation type solution highly complex environment achievement would",
    "cluster": 7,
    "keywords": "deep, segmentation, medical, image, phd, imaging, university, estimation, 08, work"
  },
  {
    "filename": "ebaee27e93161b4210c03431d246e95a.txt",
    "text": "08 app yiming xiao yiming xiao associate professor department computer science software engineering concordia university concordia university research chair intelligent intuitive surgical technology frqs j1 research scholar 2 year health x lab healthx lab ca number presentation various perspective medical research miccai 2023 satellite event trainee look forward discussing work next one week vancouver main conference oral session 4 01 047 october 9th phd trainee soorena salari present focalerrornet aware focal modulation network inter modal registration error estimation ultrasound guided neurosurgery poster session 01 131 october 9th phd trainee soorena salari present towards multi modal anatomical landmark detection ultrasound guided brain tumor resection contrastive miccai machine clinical neuroimaging mlcn workshop oral session oct 8 phd trainee mumu aktar present vesselshot shot cerebral blood vessel segmentation miccai machine medical imaging mlmi workshop poster session oct 8 msc trainee zirui qiu present visual explanation grad cam reliability deeper neural network case study automatic pneumothorax diagnosis miccai unsure workshop oral session oct 12 msc trainee parinaz roshanzamir present inter rater variability relates aleatoric epistemic case study deep based paraspinal muscle segmentation thank collaborator contribution work hassan rivaz marta kersten oertel maryse fortin gina c",
    "cluster": 7,
    "keywords": "deep, segmentation, medical, image, phd, imaging, university, estimation, 08, work"
  },
  {
    "filename": "02a40ec0d422defc097be73357996df9.txt",
    "text": "08 app ehsan ehsan leader artificial intelligence machine associate professor 7 looking principled efficient approach neural network show correlate well human checkout recent aaai paper aaai2025 ml llm bao gia doan postdoctoral research fellow unsw llm computer vision security privacy 7 aaai2025 air philadelphia thrilled share latest work bayesian low rank bella practical approach bayesian deep currently presented aaai2025 computational complexity bayesian long barrier practical adoption large scale task bayesian neural network bnns offer exceptional robustness resilience unseen distribution input high computational cost often hinders practicality bella introduce novel framework dramatically reduces computational burden bnns applying multiple low rank perturbation parameter pre trained neural network bella allows vanilla deep ensemble advanced technique like stein variational gradient descent svgd scale effectively even large key highlight bella reduced computational load significantly fewer trainable parameter needed bayesian posterior approximation enhanced performance maintain surpass traditional bayesian method non bayesian baseline real world impact proven effectiveness large scale task like imagenet camelyon17 domainnet vqa clip llava huge shoutout incredible collaborator afshar shamsi assoc prof ehsan assoc prof damith ranasinghe read paper access code bayesian low rank bella practical approach bayesian neural network arxiv org 30",
    "cluster": 8,
    "keywords": "university, epistemic, prediction, paper, research, bayesian, deep, set, network, work"
  },
  {
    "filename": "02c5c467ccab66baf41577f0ade0f6f4.txt",
    "text": "08 app tuwe l fstr cavallin tuwe l fstr cavallin assistant professor computer science j nk ping university senior software engineer saab training simulation 11 pleased share helena l fstr ulf johansson cecilia nstr rudy matela paper calibrated explanation regression accepted publication machine journal part special issue conformal prediction distribution free quantification link preprint given comment work introduces solution regression general specifically novel approach generating probabilistic prediction explanation regarding likelihood target user specified threshold distinguishing aspect contribution lie providing epistemic prediction also feature weight explanation enabling deeper understanding behavior key feature proposed method include dual quantification calibrates aleatoric epistemic uncertainty providing insight related noise confidence across familiar unfamiliar scenario versatile explanation prediction feature support range task classification regression aware probabilistic alternative explanation adjust based complexity reliable interpretable output ensures well calibrated prediction robust feature importance explanation incorporating theoretical method conformal venn prediction enhanced interpretability trustworthiness implementation detail additional feature found project readme extend gratitude editor special issue contributed work explainableai xai machinelear",
    "cluster": 8,
    "keywords": "university, epistemic, prediction, paper, research, bayesian, deep, set, network, work"
  },
  {
    "filename": "1d4b486ca37074a6f9ec13201ff6c911.txt",
    "text": "08 app precise center penn engineering university pennsylvania driving innovation embedded cyber physical system 7 excited announce paper accepted transaction machine research tmlr work tackle fundamental challenge imprecise probabilistic machine empirically derive credal region set plausible probability without strong assumption leveraging conformal prediction introduce new method provides provable coverage guarantee reduces prediction set size allows disentangle different type epistemic aleatoric breaking new ground previous approach required consonance assumption required relate conformal prediction imprecise probability method remove constraint introduces practical approach handling ambiguous ground truth allowing prediction even label uncertain calibration property ensures true generating process included high probability breakthrough enables reliable estimation real world application matter work step forward trustworthy understanding trust prediction critical deploying healthcare ensuring diagnostic acknowledge autonomous system improving decision making uncertain environment finance risk assessment reliable probabilistic forecasting true collaborative effort paper result fantastic collaboration michele caprio former postdoc precise center penn university manchester david stutz google deepmind shuo li 5th year phd student precise center penn engineering arnaud doucet google deepmind set work apart existing research imprecise probabil",
    "cluster": 8,
    "keywords": "university, epistemic, prediction, paper, research, bayesian, deep, set, network, work"
  },
  {
    "filename": "3b3d2ecffd02818509da9ce03cb6c85b.txt",
    "text": "08 app kishan panaganti kishan panaganti agi nlp senior researcher tencent 11 excited co chair insightful session 2024 informs annual meeting attending sunday consider joining u strategic distributionally robust sequential decision making oct 20 2 15 pm 3 30 pm summit 435 session highlight session delf sequential decision making task robust face environmental strategic behavior agent motivation stem increasing prevalence dynamic environment traditional decision making could fail due inability adapt changing environment anticipate strategic action agent free robust phi divergence reinforcement using offline online kishan panaganti postdoc caltech minimax optimal computationally efficient algorithm distributionally robust offline reinforcement prof pan xu assistant professor duke wasserstein distributionally robust policy continuous context wenhao yang postdoc stanford sample efficient robust multi agent reinforcement face environmental laixi shi postdoc caltech co chair session networking opportunity eager connect attendee interested broad field decision making research orm aps umbrella job market applying tenure track faculty position computer science electrical computer engineering industrial engineering operation research similar engineering business school department research scientist role industrial research group field general machine focusing decision making game theoretic decision making epistemic aleatoric uncertainty inform",
    "cluster": 8,
    "keywords": "university, epistemic, prediction, paper, research, bayesian, deep, set, network, work"
  },
  {
    "filename": "3e6c94b7087b9b557de36ea211840405.txt",
    "text": "08 app keivan shariatmadar phd msc bsc keivan shariatmadar phd msc bsc sr research scientist imprecise robust epistemic reasoning optimization senior lecturer htw saar research manager fraunhofer izfp 8 excited share incredible milestone research year dedication advancing imprecise modelling artificial intelligence thrilled announce recent work accepted iclr 48 one prestigious venue achievement deeply rooted decade research imprecise probability theory laid foundation cutting edge development research focus pushing boundary robustness accuracy advanced quantification addressing challenge critical high stake application glimpse impactful contribution making paper 1 redefining prediction random set neural network groundbreaking work shireen kudukkil manchingal muhammad mubashar kaizheng wang fabio cuzzolin showcase moving beyond probability theory random set way approach robustness distribution detection prof random set prediction significantly outperform bayesian evidential ensemble based method setting new standard aware paper 2 credal wrapper averaging new paradigm estimation together kaizheng wang fabio cuzzolin collaboration david moens han hallez developed innovative framework integrates credal set convex set probability enhance rich estimation approach achieves unparalleled performance epistemic aleatoric quantification distribution detection solving key challenge reliability make",
    "cluster": 8,
    "keywords": "university, epistemic, prediction, paper, research, bayesian, deep, set, network, work"
  },
  {
    "filename": "50c31eda8bdfebe160c24c99b7a7c62e.txt",
    "text": "08 app fatma g ney fatma g ney assistant professor ko university 2 arno solin associate professor machine aalto university 2 wrapping fantastic day iccv2023 workshop quantification computer vision paris firstly huge thanks esteemed invited speaker eric nalisnick university amsterdam fatma g ney university koc janis postels eth zurich dan hendrycks university california berkeley zeynep akata university tuebingen insight different facet ranging anomaly detection interplay assumption self driving technology truly enriched discourse assigned largest hall convention centre tell interest quantification computer vision community thanks also contributor innovative research thought provoking presentation special mention contributed talk probabilistic mimo u net efficient accurate estimation pixel wise regression far away deep space dense nearest neighbor based distribution detection calibrated distribution detection generic representation gaussian latent representation estimation using mahalanobis distance deep classifier probabilistic mimo u net efficient accurate estimation pixel wise regression muad estimation semantic segmentation challenge another highlight deep appreciation fellow organizer making seamless experience workshop truly underscored importance integrating quantification vision system promoting safer reliable application thanks everyone joined u today paris convention center looking forward seeing future event let continu",
    "cluster": 8,
    "keywords": "university, epistemic, prediction, paper, research, bayesian, deep, set, network, work"
  },
  {
    "filename": "531db72ec7251be77d3cf0e9422d3af0.txt",
    "text": "08 app julian kooij julian kooij associate professor tu delft 2 call paper upcoming workshop epistemic held uai august 4 2023 pittsburgh usa welcome research estimation robustness distribution shift distribution generalisation adaptation datasets protocol evaluating robustness conformal prediction distribution free quantification optimisation estimation using deep ensemble bayesian deep approximate inference bayesian deep rl deep recognition variational inference epistemic real world application e g autonomous driving healthcare language encourage submit submission deadline june 2023 23 59 anywhere earth workshop organized colleague epistemic e pi project oxford brooke university ku leuven tu delft info workshop website epistemic machinelearning uai obu kuleuven tudelft robustness bayesian selfdrivingcars reinforcementlearning epistemic research fet open project funded h2020 4 year project started 1 march 2021 three international university 2 excited announce organizing e pi uai 2023 workshop focusing epistemic artificial intelligence workshop scheduled take place august 4 2023 pittsburgh usa title epistemic artificial intelligence organizer prof fabio cuzzolin oxford brooke institute ethical prof matthijs spaan tu delft dr keivan k1 shariatmadar ku leuven dr maryam sultana oxford brooke kaizheng wang ku leuven shireen kudukkil manchinga",
    "cluster": 8,
    "keywords": "university, epistemic, prediction, paper, research, bayesian, deep, set, network, work"
  },
  {
    "filename": "74a5b3626ec328065135da49537e2091.txt",
    "text": "08 app amit trivedi amit trivedi associate professor university illinois chicago 4 nastaran darabi aeon lab presenting two paper icra week paper 1 calibrated evidential quantile regression deep q network ceqr dqn author alex stutts phd danilo erricolo theja tulabandhula mohit mittal meta amit trivedi propose new statistical framework combine evidential deep conformal quantile calibration separately efficiently estimate aleatoric epistemic reinforcement tested minatar ceqr dqn improves speed decision reliability application safer sample efficient exploration mission critical robotics paper 2 enhancing 3d robotic vision robustness minimizing adversarial mutual information author nastaran darabi dinithi jayasuriya devashri naik theja tulabandhula amit trivedi introduce curriculum based adversarial training approach 3d perception minimizes mutual information perturbation without requiring adversarial sample method improves generalization robustness across point cloud modelnet40 kitti datasets application reliable 3d object detection adversarial noisy environment code available icra2025 deeplearning reinforcementlearning robotsafety 3dperception adversarialrobustness uncertaintyquantification uic github nstrndrbi mine n learn enhancing 3d robotic vision robustness minimizing adversarial mutual information curriculum approach github com 54 1 503 19 future trend reinforcement defensive strategy adversarial machine deep application mri imaging challenge benefit deep accelerate rob",
    "cluster": 8,
    "keywords": "university, epistemic, prediction, paper, research, bayesian, deep, set, network, work"
  },
  {
    "filename": "96c76bfa9e3a009d4b2342c9dbda8633.txt",
    "text": "08 app prof alejandro frangi freng prof alejandro frangi freng bicentenary turing chair computational medicine uk ceirsi executive director university manchester 5 exploring future personalised quantification machine intelligence published groundbreaking paper personalised quantification puq artificial intelligence paper tackle crucial need make individualised assessment especially high stake field like healthcare defence finance key insight aleatoric v epistemic understanding difference randomness knowledge gap prediction conformal prediction cp promising methodology provides robust prediction set minimal assumption grand challenge paper identifies eight major challenge puq including multimodal explainable equitable decision read full paper delve transformative insight discover puq enhance reliability fairness across diverse application great collaboration alan turing institute led tapabrata rohan chakraborty phd artificialintelligence machinelearning healthcareai innovation research naturemachineintelligence personalized quantification artificial intelligence nature machine intelligence nature com 40 3 alireza hayati remidio nasa citizen science awg 5 high demand thanks making happened cengizhan ozturk professor bogazici university bme lifesci ct3 httm founding chairman istanbul health industry cluster isek association 5 tapabrata rohan chakraborty phd group lead transparent reliable lab trail 5 thanks alejandro delight lead work first corresponding",
    "cluster": 8,
    "keywords": "university, epistemic, prediction, paper, research, bayesian, deep, set, network, work"
  },
  {
    "filename": "a18f7713b33230546eeb6be6559facb4.txt",
    "text": "08 app thierry peynot thierry peynot associate professor robotics qut 3 fantastic day 1 ausros australian school robotic system hosted qut queensland university technology brisbane year first lecture donald dansereau acfr uni sydney covered fundamental sensing robotics including type sensor non ideality e g noise error aleatoric epistemic even thinking sense think act cycle second lecture feras dayoub aiml uni adelaide covered essential modern robot including deep using cnns rnns transformer well fundamental reinforcement rl third lecture jen jen chung uni queensland covered essential motion planning including representation graph search method dijkstra sampling based method prm rrt planning hierarchy next attendee given tour qut centre robotics garden point campus ran krishna manaswi digumarti special guest representing elo2 consortium ben sorensen sami raines sam cleary discussing latest roo ver mission working first australian rover go moon inspiring talk whole generation current future roboticists pictured finally work day concluded first part hand workshop student programming turtlebot robot ros2 navigate around maze autonomously run jen jen chung dr john vial krishna manaswi digumarti incredibly insightful inspiring first day ausros ausros ausros25 ariam robotics robot qcr 88 3 ben sorensen leading australia roo ver mission driving australia future strategic innovation space exploration transformative project helping people",
    "cluster": 8,
    "keywords": "university, epistemic, prediction, paper, research, bayesian, deep, set, network, work"
  },
  {
    "filename": "a646d0680ef4b0daf74bcb0c61f98c11.txt",
    "text": "08 app han hallez han hallez associate professor ku leuven bruges campus 7 congratulation phd student kaizheng wang ku leuven group publishing work journal neural network know contains uncertainty also uncertain due fact trained complete training set certain outcome neural network correct correct use measure along output neural network kaizheng work developed explored credal set interval neural network creinns able measure parameter furthermore creinns provide computational efficient way determining opposed commonly used bayesian neural network using kaizheng work able assess certainty output neural network determine whether trust result certain application involving classification task discard output high leaf user result rather result wrong result looking collaborator future project develop method work context epistemic within fet open call horizon 2020 program many thanks collaborator oxford brooke university delft university technology fabio cuzzolin shireen kudukkil manchingal keivan shariatmadar phd msc bsc david moens creinns credal set interval neural network estimation classification task sciencedirect com 33 2 danny hughes professor computer science ku leuven 7 great job keivan shariatmadar phd msc bsc sr research scientist imprecise robust epistemic reasoning optimization senior lecturer htw saar r",
    "cluster": 8,
    "keywords": "university, epistemic, prediction, paper, research, bayesian, deep, set, network, work"
  },
  {
    "filename": "aa7c5e926b7475a269c59fbb6a6ac833.txt",
    "text": "08 app anton baumann anton baumann master student machine 2 glad published first paper probabilistic mimo u net efficient accurate estimation pixel wise regression coauthored thomas ro berg michael schmitt opportunity present workshop quantification computer vision iccv paris pleasure meeting many great researcher known work person first time thank organiser making happen paper code found arno solin associate professor machine aalto university 2 wrapping fantastic day iccv2023 workshop quantification computer vision paris firstly huge thanks esteemed invited speaker eric nalisnick university amsterdam fatma g ney university koc janis postels eth zurich dan hendrycks university california berkeley zeynep akata university tuebingen insight different facet ranging anomaly detection interplay assumption self driving technology truly enriched discourse assigned largest hall convention centre tell interest quantification computer vision community thanks also contributor innovative research thought provoking presentation special mention contributed talk probabilistic mimo u net efficient accurate estimation pixel wise regression far away deep space dense nearest neighbor based distribution detection calibrated distribution detection generic representation gaussian latent representation estimation using mahalanobis distance deep classifier probabilis",
    "cluster": 8,
    "keywords": "university, epistemic, prediction, paper, research, bayesian, deep, set, network, work"
  },
  {
    "filename": "d1bf97a591d63daea78799245866be5e.txt",
    "text": "08 app world conference explainable artificial intelligence world conference explainable artificial intelligence 4 427 9 explainable tuwe l fstr helena l fstr fatima rabia yapicioglu francesco sovrano alessandra stramiglio fabio vitali henrik bostr understanding managing increasingly important ensure transparency appropriate trust reliability track explores diverse approach integrating quantification explainable xai framework emphasizing explanation communicate confidence prediction reliability topic interest include method representing interpreting aleatoric epistemic uncertainty well technique use insight guide decision making process high stake environment submission addressing aware explanation area reject defer option time series human centric system particularly welcome track also seek novel evaluation metric domain specific application aware explanation goal advancing actionable interpretable system machinelearning trust reliability explanation aleatoric epistemic artificialintelligence 70 4 427 ensure trustworthiness transparency understanding reliability challenge explainable decision making understanding diagnosis prediction reliability building trust transparency diagnostics role transparency claim career productivity finance soft skill emotional intelligence project management education cookie",
    "cluster": 8,
    "keywords": "university, epistemic, prediction, paper, research, bayesian, deep, set, network, work"
  },
  {
    "filename": "dd1e44e17387fc66a57824b2d32fdc4b.txt",
    "text": "08 app fabio cuzzolin fabio cuzzolin director aidas institute oxford brooke university president founder udrive 2 excited announce team organiser upcoming uai 2023 workshop epistemic artificial intelligence part horizon2020 epistemic eu project e pi uai 2023 aim raise awareness around modelling epistemic machinelearning rapidly emerging topic artificialintelligence community represented uai aim broadest possible involvement invite submission peer reviewed paper covering following topic estimation robustness distribution shift distribution generalisation adaptation datasets protocol evaluating robustness conformal prediction distribution free quantification optimisation estimation using deep ensemble bayesian deeplearning approximate inference bayesian deep reinforcementlearning deep recognition variational inference epistemic real world application e g autonomousdriving healthcare languagemodels list exhaustive paper submitted e pi uai 2023 need follow uai format submission take place via cmt award prize best paper best student paper consider publication proceeding volume subject attracting sufficient number high quality paper event feature invited speaker calibre gert de cooman ghent marco zaffalon idsia usi supsi yarin gal university oxford aaditya ramdas carnegie mellon university organizer prof fabio cuzzolin oxford brooke university institute ethical",
    "cluster": 8,
    "keywords": "university, epistemic, prediction, paper, research, bayesian, deep, set, network, work"
  },
  {
    "filename": "df3fa22776a843853bbcaf9615d54e84.txt",
    "text": "08 app fabio cuzzolin fabio cuzzolin director aidas institute oxford brooke university president founder udrive 8 epistemic getting traction paper submitted iclr 25 aistats 25 got 100 5 paper originally submitted neurips 2024 published top venue congratulation fantastic team amazing result random set neural network led shireen kudukkil manchingal muhammad mubashar foundational paper proving network predict random set rather probability vector accurate robust better ood detection calibration rate competitor bayesian evidential ensemble etc credal wrapper averaging estimation classification led kaizheng wang keivan shariatmadar phd msc bsc kuleuven supervised han hallez david moens show wrapping bayesian ensemble prediction form credal set convex set probability improved estimation distribution detection last least unified evaluation framework epistemic prediction also led shireen kudukkil manchingal fill huge gap field proposing single coherent way assessing comparing prediction attached level epistemicai artificialintelligence machinelearning ml aistats iclr thisisbrookes horizon2020 deeplearning come check website update 95 7 alessio del bue head pavis research unit istituto italiano di tecnologia honorary professor department computer science durham university 8 impressive 1 dr maryam sultana research fellow associate lecturer oxford brooke uk research associate mbzuai uae phd kyung",
    "cluster": 8,
    "keywords": "university, epistemic, prediction, paper, research, bayesian, deep, set, network, work"
  },
  {
    "filename": "e2dda26a8b0c5b45b93ef9f168626b83.txt",
    "text": "08 app han hallez han hallez associate professor ku leuven bruges campus 9 past week conference week ku leuven group phd student one kaizheng wang got accepted neurips presented work neurips 2024 vancouver canada presented work credal deep ensemble quantification work standard neural network result single probability distribution among available class however due difference operating condition environment mismatch happen used train used operation result uncertainty turn result uncertainty resulting probability distribution called epistemic difficult quantify common bayesian neural network used suffer high computational load may difficult implement low latency high reliability application work kaizheng wang used credal set neural network basis improved yet computationally feasibale estimation applied measure distribution detection credal set neural network use interval arithmetic calculate class probability interval instead single probabilty class hence datapoint belongs class probability depicted interval rather single probability stop improve estimation used several credal set neural network deep ensemble aggregate result result show able obtain better measure within reasonable amount time opposed method also able detect distribution sample reliable way approach integrate result",
    "cluster": 8,
    "keywords": "university, epistemic, prediction, paper, research, bayesian, deep, set, network, work"
  },
  {
    "filename": "ec740deb23a1c4b3549d3b9d1487b9b8.txt",
    "text": "08 app fr ric barbaresco fr ric barbaresco thales quantum algorithm computing algo sensor segment leader 2 workshop cautiousness imprecision machine october 9 paris sorbonne universit motivation focus notion major importance machine constitutes key element modern machine methodology recent year gained importance due increasing relevance machine practical application many coming safety requirement regard new problem challenge identified machine scholar call new methodological development indeed long perceived almost synonymous standard probability probabilistic prediction recent research gone beyond traditional approach also leverage general formalism calculus example estimate called epistemic done genralisalition bayesian approach credal approach using set probability approach leverage geoetric consideration rather relying theory aim scope goal small scale workshop bring together researcher interested topic machine particular involves notion cautiousness robustness imprecision missingness etc meant provide place discussion recent development modeling processing quantification machine problem exploration new research direction field topic interest scope workshop cover limited following topic adversarial example aleatoric epistemic bayesian method belief function calibration classification",
    "cluster": 8,
    "keywords": "university, epistemic, prediction, paper, research, bayesian, deep, set, network, work"
  },
  {
    "filename": "f95c2d49bbf12dbd38deb75617a5341f.txt",
    "text": "08 app tuwe l fstr cavallin tuwe l fstr cavallin assistant professor computer science j nk ping university senior software engineer saab training simulation 9 explainable special track world conference xai author tuwe l fstr helena l fstr fatima rabia yapicioglu alessandra stramiglio francesco sovrano fabio vitali henrik bostr read understanding managing increasingly important ensure transparency appropriate trust reliability track explores diverse approach integrating quantification explainable xai framework emphasizing explanation communicate confidence prediction reliability topic interest include method representing interpreting aleatoric epistemic uncertainty well technique use insight guide decision making process high stake environment submission addressing aware explanation area reject defer option time series human centric system particularly welcome track also seek novel evaluation metric domain specific application aware explanation goal advancing actionable interpretable system explainable 37 1 tuwe l fstr cavallin assistant professor computer science j nk ping university senior software engineer saab training simulation 9 1 893 58 ensure trustworthiness transparency understanding reliability understanding diagnosis",
    "cluster": 8,
    "keywords": "university, epistemic, prediction, paper, research, bayesian, deep, set, network, work"
  },
  {
    "filename": "011795baacda8e586a4c1f916e5b29c5.txt",
    "text": "08 app luis pinto luis pinto science 5 alliance active transfer partially bayesian neural network material chemical maxim ziatdinov demonstrate partially bayesian neural network pbnns incorporate probabilistic weight selectively within layer yield accuracy estimation comparable fully bayesian network active task across molecular material science significantly reducing computational expense additionally show initializing prior distribution weight pre trained theoretical digital twin space enables pbnns effectively leverage computational prediction accelerate active experiment 30 2 618 124 latest breakthrough chemistry driven material science research changing material design machine transforms chemistry neural network training method machine improves molecular prediction career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "053e6eb198456819201b97b93d1d21f1.txt",
    "text": "08 app mario de florio mario de florio postdoctoral researcher nrel hybrid energy system research 6 trust human physiology fully understand latest paper quantification total physic informed reconstruction cvsim 6 physiology tackle challenge developing monte carlo x tfc novel physic informed approach decomposes quantifies source aleatoric random noise epistemic lack knowledge insufficient form misspecification equation matter computational physiology often rely predict patient outcome optimize treatment design digital twin human organ reality messy sparse sensor introduce noise perfectly capture full complexity human biology want trustworthy must make prediction also quantify confident prediction method mc x tfc blend physic based regularization driven reconstruct physiological state estimate parameter even case missing flawed put test cvsim 6 cardiovascular demonstrating interacts physic informed showing extract meaningful insight even working imperfect information work part special issue quantification healthcare biological system philosophical transaction royal society really grateful co author zongren zou daniele e schiavazzi george karniadakis collaboration exciting research read full paper uncertaintyquantification digitaltwins physicsinformedmachinelearning",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "0b0eeb75484c99397370fd2fc0a0a3cb.txt",
    "text": "08 app sergei kalinin sergei kalinin weston fulton chair professor university tennessee knoxville 5 goal machine physical science decision making past two decade worked across unsupervised supervised applied ml trained theory guide experiment contributed developing autonomous scientific workflow one key conclusion emerged experience real value machine physical science lie analyzing making decision yes ml help analyze governing physic complex computationally prohibitive case ml serf valuable approximation tool physical exist constructed remain gold standard real opportunity ml guide decision choosing next experiment designing optimal material planning efficient synthesis ml becomes powerful embedded decision loop orchestrating experiment real time integrating prior knowledge physical constraint already seen promising direction physic informed neural network pinns neural operator accelerate simulation pdes known symbolic regression extracting governing relationship unsupervised method uncover latent structure noisy high dimensional measurement yet advance must serve larger purpose advancing discovery well structured decision process often prediction method used guide exploration new space view ml substitute physic enabler autonomous reasoning making experimental theoretical workflow faster adaptive reproducible challenge u better better integration ml",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "218ca446bef81f4184761cecee7c9373.txt",
    "text": "08 app nanohub org network computational nanotechnology ncn nanohub org network computational nanotechnology ncn 3 002 4 new nanohub material science property prediction good bad uncalibrated presentation ashley dale university toronto introduces concept aleatoric epistemic within machinelearning demonstrates use alignn predicting bandgap value dataset introduce characterization workflow jupyter notebook act scaffolded coding exercise enable participant improving coding skill developing intuition trustworthiness prediction access presentation material science property prediction good bad uncalibrated nanohub org 2 3 002 machine improves molecular prediction improve molecular property prediction include optimization understanding predictive material science technique machine transforms chemistry manage supply chain career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "390ca9f43f3c28e9acb1fd87fdfd2831.txt",
    "text": "08 app jerome workman jerome workman executive editor mjh life science analytically speaking podcast host 1 smarter spectroscopy new machine approach estimate prediction new study demonstrates machine technique quantile regression forest provide accurate prediction sample specific estimate infrared spectroscopic work applied soil agricultural sample highlighting value chemometric modeling 16 harrison katz tech lead manager forecasting science airbnb 1 excited share paper rob weiss titled bayesian shrinkage high dimensional var comparative study published international journal statistic probability study offer systematic comparison shrinkage method high dimensional vector autoregressive var addressing parameterization lag order check full article bayesian shrinkage high dimensional var comparative study ccsenet org 33 1 tibor szilv si assistant professor university alabama 2 cover paper modeling behavior complex aqueous electrolyte show machine interatomic potential provide insight structure electrolyte paper 131 2 laurent duval 3 opendata publication reservoir modelling compression algorithm machine worflows graphical abstract christophe preux frederic payan lauriane mathon bouard lundisim mesh flow simulation scientific compression benchmark increasing scientific size impact computability interpretability sustainability",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "42fbbaeb9d28e7a34e97cf0f14ce9dcf.txt",
    "text": "08 app thomas swinburne thomas swinburne faculty university michigan visiting researcher cnrs physique 1 new paper npj computational material explore misspecification machine interatomic potential snap mace showing misspecification aware uq scheme pop swinburne perez ml allows u robustly bound md simulation output compared dft test efficiently propagating uq force field level directly relaxed simulation output via implicit differentiation maliyov et al npj currently featured article also show overcomes many issue ensemble uq method misspecification mathematical definition famous aphorism wrong useful matter parameter choose always error essentially always case surrogate replacing expensive routine cheap one standard bayesian scheme provably blind misspecification see cited ml article post earlier year current treatment misspecification scientific ml far behind aleatoric epistemic uq must treated want propagate max min bound complex simulation workflow trustworthy result great pleasure collaborate danny perez postdoc aparna subramanyam ivan maliyov microsoft quantum always quantification misspecified machine learned interatomic potential npj computational material nature com 131 1 giuseppe fisicaro theoretical computational material scientist cnr catania italy institute microelectronics microsystems 2 congratulation thomas swinburne jorge bravo abad ml science deeptech pi material lab prof",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "47d55b2c51e3d9ca549d68aa8a0d4279.txt",
    "text": "08 app dario coscia dario coscia phd candidate sissa collaboration university amsterdam deep 1 going machine interatomic potential mlips bayesian learned interatomic potential blip mlips transforming chemistry simulation often break scarce facing distribution furthermore rarely tell u confident prediction latest work introduce blip add bayesian layer around mlips deliver reliable prediction challenging regime scarce distribution trustworthy well calibrated estimate plug play boost predictive accuracy minimal overhead experiment blip outperform deep ensemble baseline mlips accuracy force energy estimation best part need one blip ensemble even fine tuning existing universal interatomic potential blip yield substantial gain compared standard ensemble fine tuning amazing collaboration pim de haan max welling cuspai code paper extra blip limited mlip applied message passing neural network look forward seeing application blip bayesian learned interatomic potential arxiv org 176 1 keqiang yan postdoc researcher princeton university 1 congratulation 1 bruno neri technical leader artificial intelligence deep enthusiast senior software engineer alten italia 1 blip bayesian learned interatomic potential dario coscia pim de haan max welling machine interatomic potential mlips becoming central tool simulation based chemistry howev",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "4862bc18a2e7f9e4f38367dd5097aeca.txt",
    "text": "08 app michael shield michael shield professor john hopkins university president principal engineer uquant inc 1 software update paper highlighting scientific machine sciml module uqpy available check new feature bayesian neural network neural operator sciml module integrates seamlessly pytorch includes several new feature plug play uq machine thanks everyone contributed especially connor krill ponkrshnan thiagarajan always uqpy would without sustained effort dimitris giovanis dimitris tsapetis uqpy version 4 2 quantification python sciencedirect com 221 2 pouya zarbipour looking phd position msc civil engineering coastal port marine structure engineering 1 update useful adding ml pankaj k mishra phd scientist geophysics geological survey finland 1 amazing riccardo farris phd theoretical chemistry material modeling simulation machine open source developer 2 new preprint excited share latest work arxiv bayesian neural network versus deep ensemble quantification machine interatomic potential study explore quantify deep based interatomic potential critical step toward making machine reliable deployable material simulation benchmark two major approach deep ensemble practical scalable variational bayesian neural network principled probabilistic introduce bayesian aenet open source framework built aenet pytorch pyro tyxe designed rigorous uq benchmarking ml",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "53534cc9b9fca4da4f7782f2b6ab7f0b.txt",
    "text": "08 app gurkan sin gurkan sin professor dtu chemical engineering technical university denmark 1 wrapped summer school sensitivity analysis august 18 22 16th edition row journey started back 2009 27 researcher mostly academia industry come together explore contemporary method analyzing numerical output especially monte carlo method mission simple build intuition get feel theory immediately apply hand practice key message participant randomize shake foundation nothing really pre ordained come choice one build good deal heuristic well call theory modeling whether first principle based machine ask challenge input parameter assumption structure see could still stand foot came part always exciting privilege really participant presented research sheer variety topic incredible showed numerical method part modern research saw chemical process engineering everything photocatalysis n2o wastewater treatment green ammonia production complex separation process like membrane system carbon capture dac energy sustainability salt based energy storage sustainable aviation fuel co2 electrolysis science reliability tackling ml biomass property prediction creating surrogate complex distillation system ml control health aware mpcs predicting time failure component parametric",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "53e3709e0aa9a4ac3d80ed820e69ba2e.txt",
    "text": "08 app sergei kalinin sergei kalinin weston fulton chair professor university tennessee knoxville 1 uncertainty ml driven material science fascinating intersection machine ml theory experimental science continually pushing boundary possible material discovery analysis however journey without challenge particularly come understanding mitigating uncertainty aleatoric epistemic uncertainty known challenge ml familiar concept aleatoric essentially refers noise inherent well documented challenge navigate refining collection processing method adding encounter epistemic may fully capture underlying truth generation process type require rigorous attention ensure reliability ml application third theory v experiment material science however third less discussed challenge emerges applying ml theoretical framework experimental science especially realm material science crux issue lie disparity theoretical real world entity aim represent instance chemical formula provide neat theoretical representation molecule reality molecule interacting within solution complexity solid material dopants impurity introduces significant gap theory experiment gap pose unique challenge material scientist decade condensed matter physicist strived create material close ideal theoretical possible yet material science reconciling microstructure real material theoretical depi",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "5cf70d9296a18d8bfb47cd24c0534d91.txt",
    "text": "08 app max welling max welling cto co founder cuspai prof u amsterdam 8 nice collaboration talented dario coscia barnn add layer bayesian quantification around sequence like rnns achieves good sota performance sequence prediction quantification number task among pde solving molecule generation dario coscia phd candidate sissa collaboration university amsterdam deep 8 presenting barnn fully bayesian scalable calibrated approach convert autoregressive recurrent neural network bayesian counterpart minimal training architecture change barnn combine advantage given bayesian inference great performance autoregressive recurrent net applied quantification neural pde solver molecule generation chemical language working done max welling nicola demo gianluigi rozza code barnn bayesian autoregressive recurrent neural network arxiv org 156 6 alex perchikov 8 barnn sound like really useful tool dario coscia phd candidate sissa collaboration university amsterdam deep 8 thanks max nice work 1 calina ciuhu pijlman 7 curti mitrofan giuseppe nicosia professor artificial intelligence synbio 8 dario coscia one best acdl student attendant kudos dario ad maiora 1 daniel kapitan scientist digital architect lecturer outdoor enthusiast buddhist necessarily order 8 alexander teeuwen perhaps next iteration 23 497 260 latest advance rna modeling technique latest breakthrough",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "6d712b68ba2cf518f080467a0eab2d91.txt",
    "text": "08 app matteo ravasi matteo ravasi senior research advisor mlops engineer shearwater pylops core developer 11 pre print alert quantification undoubtedly one hottest topic geophysics nowadays ensure estimate really meaningful product many hyper parameter enter inversion process phd student miguel angel corrales internship totalenergies visiting student sean berti university pisa spent considerable amount time investigating behavior svdg context fwi result obtained tell u svgd hyper parameter significantly impact final estimate much previously reported literature good news able identify number adjustment make effect less severe alongside two strategy obtain meaningful statistical analysis final particle link arxiv kaust pse deepwave research uq fwi annealed stein variational gradient descent improved estimation full waveform inversion arxiv org 83 1 dr izzatullah mustafa carbon market sustainable finance indeed great work congratulation 4 105 157 3 open open software hit sweet spot matteo ravasi 6 eage yp thirsty thursday interview wouter kimman matteo ravasi 8 eage yp thirsty thursday interview paolo dell aversana matteo ravasi 8 impact innovation new technique geophysical exploration include optimization economic affect hiring improve visualization technique reduce false positive scanning",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "6edbb0817e32d4f0eefd99c371ac2caa.txt",
    "text": "08 app ying li ying li associate professor uw madison 3 trust machine ml prediction answer question systematic quantification uq study polymer property prediction various ml uq technique work published journal chemical information modeling titled assessing machine polymer property prediction benchmark study kudos phd student hao tang tianle yue leading interesting study machine ml emerged transformative tool material science enabling accelerated discovery design novel molecule reducing experimental cost quantification uq crucial enhancing reliability ml prediction particularly high stake application functional polymer discovery study present comprehensive evaluation nine uq method ml ensemble gaussian process regression gpr monte carlo dropout mcd mean variance estimation mve bayesian neural network based variational inference bnn vi markov chain monte carlo bnn mcmc evidential deep edl quantile regression qr natural gradient boosting ngboost predicting key polymer property including glass transition temperature tg band gap eg melting temperature tm decomposition temperature td assessed using three independent metric including prediction accuracy r2 spearman rank correlation coefficient calibration area offering robust framework evaluating mean prediction estimate analysis span set four property distribution ood experimental molecular dynamic md derived high tg polymer diverse polymer type providing holistic perspective performance finding reveal optimal uq met",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "70f44f4587f5c93b788967a87ab53b0e.txt",
    "text": "08 app javier antor n javier antor n co founder ceo angstrom 3 past year working linearised laplace method probabilistic inference neural network method allows u choose hyperparameters without costly cross validation provides well calibrated estimate accompany neural network prediction however found naive application lin laplace modern deep system fail dramatically new paper adapting linearised laplace evidence modern deep identifies pathology lin laplace method applied modern nns provides adapted methodology fix issue proposal derived first principle empirically validated transformer lenet style cnns resnets batchnorm non scale invariant fixup pre resnets u net autoencoders presenting work icml2022 baltimore usa paper found deeplearning neuralnetworks research theory bayesianstatistics datascience icml 97 3 491 44 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "72511ff50700e5a9527fcffd38e84849.txt",
    "text": "08 app divyanshu vyas high performance computing shell founder petroleum scratch genai science energy 2 favorite research paper physic informed machine problem solving summary 1 physic informed neural network deep framework solving forward inverse problem involving nonlinear partial differential equation raissi perdikaris karniadakis 2017 pinns type machine used solve partial differential equation pdes pinns framework combine expressive power neural network physical constraint imposed pde paper demonstrates effectiveness pinns solving variety pdes including navier stokes equation heat equation schr dinger equation 2 physic constrained deep inverse problem zhang malley mcmahan 2019 paper proposes physic constrained deep framework solving inverse problem framework us bayesian optimization approach search parameter neural network satisfy known physical constraint observed paper demonstrates effectiveness framework solving variety inverse problem including problem estimating parameter pde noisy 3 physic informed machine quantification raissi perdikaris karniadakis 2020 paper present framework using physic informed neural network quantify solution pdes framework us bayesian approach propagate input neural network paper demonstrates effectiveness framework quantifying solution navier stokes equation 4 physic informed machine driven discovery",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "838f66d536b9b41a32bfe1acea7e0587.txt",
    "text": "08 app victor zavala victor zavala baldovin dapra professor university wisconsin madison 2 really excited share new preprint reporting physic constrained neural ode framework dynamic microbial community experimental call neural specie mediator nsm embeds neural network within physic structure skeleton given consumer resource crm also implement variational inference framework quantify prediction extensive validation conducted experimental several important thing learned research following pure neural ordinary differential equation difficult train numerical instabilties addition rather simple physic constraint help alleviate instability addition physic help obtain information consistent critical experimental design procedure variational inference powerful quantification framework directly formulated optimization problem absolutely obvious determine best strategy combine physic neural ode experimented many different variant ultimately boil trade flexibility interpretability work based collaboration group prof ophelia venturelli result heroic effort brilliant jaron thompson bryce connors preprint machinelearning dynamic datascience 306 11 moritz von stosch senior scientific advisor datahow 2 interesting work much line work performed hybrid semi parametric modeling different degree hybridization might worth contrast finding",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "8ac998911c20f3d9074136c378e4214f.txt",
    "text": "08 app javier antor n javier antor n co founder ceo angstrom 4 performing probabilistic reasoning weight deep analytically intractable furthermore vast size weight space limit u using crude approximation working recent work expressive yet tractable bayesian deep via subnetwork inference show performing rich approximate inference carefully chosen subset weight work better traditional bayesian deep approximation approach competitive state art deep ensemble despite significantly cheaper computationally find preprint paper deeplearning machinelearning datascience artificialintelligence research neuralnetworks algorithm 54 2 martin trapp assistant prof machine kth 4 congrats paper seems quite interesting try performing inference independently multiple sub network combining posterior approximation afterwards 3 492 44 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "8fef990f148ffa0238ab43a5c7ae9055.txt",
    "text": "08 app ba kessels ba kessels mechanical engineer phd special interest dynamic control thermal mechanical system quantifying black box output crucial gain trust use decision making past two publication developed inverse mapping parameter updating impu method method update physically interpretable parameter nonlinear dynamic near real time using feature extracted measured response using black box inverse mapping way stay accurate digital twin physical system operational lifetime may instance used increase performance based controller structural health monitoring predictive maintenance newest publication impu method extended provide quantification updated parameter value provide user valuable insight level trust user may updated parameter value furthermore methodology enhanced allow different identification experiment performed without compromising online computational efficiency method newest paper methodology demonstrated updating parameter value medical mechanical ventilation setup company demcon macawi inferred probability distribution updated parameter value lie within tolerance region true parameter value quantified uncertainty give decent indication relative e g high found true parameter value near edge training parameter space sound interesting find full paper would like thank nwo dutch research counc",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "92a59efa9ca19d142cf52b87a4c9ad47.txt",
    "text": "08 app michael shield michael shield professor john hopkins university president principal engineer uquant inc 11 happy share latest paper published computer method applied mechanic engineering entitled bayesian neural network predicting full field material response quantification method predict low dimensional quantity interest derived complicated work use bayesian convolutional neural network predict full field mechanical response heterogeneous material specified loading condition neural network predicts stress material discretized point space along measure prediction point compare three different strategy bayesian neural network implementation hamiltonian monte carlo hmc variational bayes backprop bbb algorithm monte carlo dropout result show hmc computationally tractable even large high dimensional problem variational bbb provides robust estimate largely consistent hmc monte carlo dropout meanwhile show inconsistency method sensitive design parameter detail see paper credit work go awesome postdoc george pasparakis incredible collaborator lori graham brady work supported center high throughput material discovery extreme funded u army devcom army research laboratory grateful support bayesian networknetwork computationalmechanics john hopkins whiting school engineering john hopkins department civil system engineering hopkins extreme material institute john hopkins university bayesian neural",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "95e8fb3447dd3b8be3d196128e7c2499.txt",
    "text": "08 app javier antor n javier antor n co founder ceo angstrom 3 imagine world neural network hyperparameters set automatically value generalise best without need held cross validation keep using held set tune hyperparameters take bayesian pill see deep rabbit hole go watch talk selection adapting linearised laplace evidence modern neural network symposium advance approximate bayesian inference tomorrow feb 1st 18 40 gmt full paper titled linearised laplace inference network normalisation layer neural g prior available neuralnetworks research bayesian statistic deeplearning artificialintelligence 93 1 prayag tiwari associate professor machine 3 interesting paper javier antor n 3 491 44 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "9c7b88a8266c8f7911b12620cb939963.txt",
    "text": "08 app youngsoo choi youngsoo choi computational scientist lawrence livermore national laboratory 1 host gianluca iaccarino stanford university driven physical simulation ddps seminar lawrence livermore national laboratory june 21st 2024 california time talk autoencoder aerodynamic prediction anyone join u following webex room expect wonderful talk gianluca iaccarino please take advantage abstract autoencoder work one trust prediction talk focus recent activity centered around development autoencoder unsupervised driven predict flow past wing geometry relies nonlinear compression construct low dimensional latent representation available relation physical input enables approach generate new unseen case careful construction dataset produce latent variable interpreted term aerodynamic performance attached separated flow condition important thrust work investigation effect uncertainty due autoencoder architecture hyperparameters amount training internal form uncertainty comparison gaussian process regression linear compression strategy illustrate advantage present approach extracting useful information prediction even absence effect internal uncertainty also compared impact variability induced uncertain operating condition external uncertainty showing importance accounting total establishing prediction confidence brief discussion incorporate multi fidelity da",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "a1d1de1afaf18c5cfe6866d572a873a7.txt",
    "text": "08 app alexandre wadoux alexandre wadoux phd habil soil scientist marie sk odowska curie fellow 6 new message paper spatial aggregation gerard heuvelink high resolution map climate ecosystem variable essential estimating carbon stock tracking ecosystem change supporting climate policy map often created using remote sensing machine include estimate always assessed correctly key issue spatial correlation map error often ignored leading inaccurate estimate aggregated regional national level real implication application like soil carbon mrv proper assessment crucial carbon crediting reporting note international journal applied earth observation geoinformation highlight issue discusses statistical method improve quantification environmental mapping read soilcarbon mrv spatialuncertainty geostatistics mapping climatescience remotesensing inrae isric world soil information soil science cluster wur 143 11 len strnad senior machine engineer geospatial 6 nice curious deciding simulate strong spatial correlation representative real life scenario natural phenomenon exhibit level spatial correlation also although many statistical method spatial correlation residual method around since 1970s still high barrier entry people may statistical experience justify particular method fit well know research comparing result aggregation well poor fit variograms kriging method example ar",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "a52be604d5649dcb7812123e686d4c96.txt",
    "text": "08 app youngsuk park youngsuk park hiring senior scientist team lead aws efficient training inference llm stanford phd 2 scientist aws shared two exciting effort kdd2023 1 presented tutorial training large scale foundation emerging chip yida wang jun luke huan rahul solanki christian bock aashiq muhamed gave overview llm system perspective distributed training package followed several case study train llm effectively e g bert gpt t5 using aws trainium interested using new accelerator reduced cost see website slide available soon please reach u 2 gave oral talk time series workshop dynamic ensemble probabilistic time series forecasting via deep reinforcement jun luke huan yuyang bernie wang karthick gopalswamy hilaf hasson intern yuhao ding work propose rl ensemble policy dynamically adaptively form ensemble weight prediction horizon especially effective time series forecasting setting base forecaster performs differently depending framework condition find paper llm machinelearnig timeseries reinforcementlearning forecasting distributedcomputing amazonscience kdd2023 gpu trn dynamic ensemble probabilistic time series forecasting via deep reinforcement amazon science 129 2 amazon science 2 great work youngsuk park team 1 curtis raymond mma enterprise analytics priceline master management analytics 2 youngsuk park awesome thanks sharing 3 780 32 career productivity finance soft skill emotional intelligence project management",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "b35025b0453ce2262a7567b93cdba407.txt",
    "text": "08 app oliver borchert oliver borchert machine engineering quantco 3 week finally received news maiden first paper got accepted natural posterior network deep bayesian exponential family distribution featured spotlight paper iclr2022 course could done alone owe huge thanks invaluable discussion contribution co first bertrand charpentier co author daniel zuegner simon geisler supervisor prof dr stephan g nnemann work attempt distill awareness deep crucial develop reliable machine system natural posterior network natpn provides fast high quality estimate used problem target distribution belongs exponential family notably includes classification regression count prediction task importantly natpn requires little change existing architecture produce estimate without need distribution leveraging property normalizing flow originally started work paper master guided research project summer 2020 two unsuccessful conference submission 2021 felt painful revision markedly improved paper since eventually tenacity paid facilitate adoption work additionally release pytorch implementation natpn writing paper put serious amount work hopefully make delight use implementation 1 provides intuitive interface enables using easily scikit learn estimator 2 feature modular design allows customize build upon different level abstraction find implementation git",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "c1dbf064d5702e315610fd57d9de584d.txt",
    "text": "08 app som dhulipala ph som dhulipala ph computational scientist 2 excited 17th u national congress computational mechanic set begin next week albuquerque plenty exciting special session driven modeling quantification computational modeling simulation looking forward hosting two session team idaho national laboratory external collaborator detail special session research presentation provided mini symposium m 404 ml aided quantification complex system analysis michael shield xu wu audrey olivier jarek knap u army devcom army research laboratory ting wang booz allen hamilton m 421 software tool quantification machine application computational science zachary prince p ter german dewen yushu yifeng che research presentation bayesian inference latent hamiltonian neural network l hnns presented arxiv preprint adaptive active multifidelity monte carlo method moose stochastic tool module presented zachary prince efficient rare event simulation using explainable active multifidelity surrogate modeling framework presented promit chakroborty arxiv preprint recently accepted publication journal engineering mechanic check attending machinelearning modelingandsimulation computational bayesian nuclearenergy ml aided quantification complex system analysis 17 usnccm org 40 1 675 27 career productivity finance soft skill emotional intelligence project management education",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "d0d52d310e96d2d951922d0a3392e8c6.txt",
    "text": "08 app lorenzo piu ph researcher universit libre de bruxelles rwth aachen sustainable combustion machine 6 next week european combustion meeting edinburgh presenting latest work quantification using bayesian neural network explored several approximate bayesian inference technique quantify deep neural network applied sub filter modeling turbulent combustion le incorporating estimate allows better handle generalizability challenge often faced traditional deterministic architecture particular well calibrated serve reliable indicator sample common source error driven sub filter closure graphical abstract top summarizes full pipeline input feature aware prediction bottom show demo bayes backprop algorithm weight trained gaussian distribution allowing capture mean aleatoric sparse machine physical modeling want get hand made simple code recreate video available github work result collaborative effort researcher many institute including universit libre de bruxelles rwth aachen university university cambridge take opportunity thank usual colleague supervised contributed throughout research arthur p quin salvatore iavarone rodolfo freitas hongchao chu kisung jung supervisor prof alessandro parente heinz pitsch invaluable support also want acknowledge horizon europe funding phd encoding project want forget thank tag patricia domingo alvarez promotin",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "d2c329c258db2fcd03e7df6875a6b291.txt",
    "text": "08 app shweta singh shweta singh associate professor purdue university working build sustainable equitable world 2 2019 two undergraduate intern group started exploring idea identifying governing equation manufacturing system using machine specifically worked sindy sym reg symbolic regression application widely used distillation column chemical process fascinating explore early many open question answer found interesting result mechanism elucidation published machine application back journal chemical engineering manufacturing skeptical idea got desk rejected recently lot researcher started using technique well good sign chemical engineering community ensure machine application developed domain specific knowledge domain application ugresearch raghav moar renganathan subramanian machinelearning processsystemsengineering white box machine approach identify governing equation overall dynamic manufacturing system case study distillation column sciencedirect com 71 1 shruti vyas assistant professor initiative mse c secondary joint appointment university central florida 2 thanks sharing shweta 5 727 106 understand application manufacturing understanding quantum machine application application improving manufacturing performance latest breakthrough chemistry changing manufacturing process machine transforms chemistry career productivity finance soft skill emotional intelligence project management education",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "e41309d82a6df557940e439b807045c1.txt",
    "text": "08 app mario de florio mario de florio postdoctoral researcher nrel hybrid energy system research 1 understanding quantifying simulation physical phenomenon critical challenge especially sparse noisy incomplete address developed monte carlo ensemble physic informed machine method framework allows u accurately estimate state parameter complex differential system total including aleatoric epistemic form uncertainty focus applying cvsim 6 cardiovascular physiology demonstrating robustness even limited thank co author zongren zou daniele schiavazzi george karniadakis invaluable contribution check preprint arxiv brown university university notre dame cardiovascular physiology physicsinformed neuralnetworks totaluncertainityquantification uq timeseries 212 7 william craig experience year 1 shown wonderful growth time brown dr deflorio best wish move colorado hope like ski 1 raj saini phd technology specialist r k technology service customized solution service energy technology r cfd ml integration alma mater iitb iitm financial modelling 1 congrats team work another study using openfoam share cad step stl look abolhassan mohammadi fathabad phd operation research analyst 1 cheer nancy gaucher thomas artist educator art administrator art advocate founder art league ri former president providence art club 1 amazing mario side brain working harmony 1 mahd",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "e57e1d75b4a040c3c891a90f5796b48a.txt",
    "text": "08 app michael shield michael shield professor john hopkins university president principal engineer uquant inc 7 wonderful week banff banff international research station workshop quantification neural network small intensive workshop amazing opportunity learn computer scientist mathematician engineer working neuralnetworks every variety pinns gpt talk long speaker 1 1 5 hour dive deeper meaningful conversation uq ml especially grateful organizer habib najm roger ghanem yue yu serge prudhomme prasanna balaprakash inviting give presentation recent development future opportunity bayesian neural network thanks engaging discussion learned even work place larger landscape beyond engineering 131 3 abhirup patra researcher shell computational chemistry material science machine research 7 would mind sharing slide michael shield peter kaspari 7 spannend 7 069 133 1 surg ahead discovering new material strong enough superman michael shield 5 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "e5b301e8e7158542f16116035c879cd2.txt",
    "text": "08 app leonardo ferreira guilhoto leonardo ferreira guilhoto final year phd candidade upenn former simulation intern atomic machine 7 great time attending computational science engineering conference organized society industrial applied mathematics siam week fort worth tx earlier week organized two session mini symposium quantification scientific machine uq4sciml short eight wonderful talk researcher industry academia topic ranged bayesian optimization driven solution pdes climate forecasting ecological modeling thank speaker accepting invitation traveling present work nicholas h nelsen elizabeth qian patrick wyrod toryn schafer maximilian balandat haiwen guan fernando rochinha also gave talk recent paper composite bayesian optimization function space using neon neural epistemic operator network read full paper paper main contribution 1 formulating leaky expected improvement l ei acquisition function bayesian optimization provably similar traditional ei significantly easier optimize via gradient based method 2 proposing neon neural network architecture operator built quantification using epinets originally proposed ian osband compared gps deep ensemble popular composite bo benchmark neon achieved best performance time requiring 40x less trainable parameter deeponet ensemble presentation ready might quick recording post youtube later lookout cse2025 uq4sciml machinelearning ai4science appliedmath siam 69 5 sarah helfert applied mathematics phd",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "ebb5b0af1c5dc1ae8c454b13fa8c4218.txt",
    "text": "08 app cristian lazo quispe cristian lazo quispe ml researcher 1 2 diffusion ddpm noise becomes continuing research journey diving denoising diffusion probabilistic ddpms inspired thermodynamics amazing adding noise step step carefully reverse produce high quality generative github next post ddim denoising diffusion implicit machinelearning deeplearning generativeai ddpm diffusionmodels researchjourney 57 utkarsh r ph student mit csail iit kanpur prev nvidia amazon science 2 checkout recent work accepted neurips explore flow based generative adopted scientific ml general enforce arbitrary constraint test time curious hear thought community bridging area neurips ai4science sciml pengfei cai phd candidate mit 2 exciting news paper physic constrained flow matching sampling generative hard constraint accepted neurips read introduce pcfm framework enforces arbitrary multiple physical constraint pde setting flow based generative enabling reliable constraint satisfying scientific ml highlight 1 general framework enforces conservation law boundary condition arbitrary nonlinear constraint hard requirement inference time 2 zero shot inference work directly pretrained flow based retraining constraint gradient needed 3 improved performance outperforms pde benchmark ensuring exact constraint satisfaction especially nonlinear constraint looking forward present neurips san diego december advance intersection generative scientific",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "ee6e9e04c21e087585db60b5b01e6ab2.txt",
    "text": "08 app juan daniel romero juan daniel romero ph student earth science engineering 1 reliable seismic inversion result tackle question recent paper help deep specifically implicit neural representation normalizing flow introduce framework efficient scalable quantification seismic inversion certain assumption best fairly benchmark result synthetic demonstrate framework performance 2d 3d field best part code open source adapt problem happy help get started special thanks ph advisor matteo ravasi wolfgang heidrich bayesian seismic inversion implicit neural representation academic oup com 178 6 jian sun associate professor ocean university china 1 believe first one introduce inr seismic inversion including fwi prefer call implicit fwi implicit seismic inversion beyond proved implicit strategy also extended eletromagnetic gpr inversion several benefit performing inversion inr convenient extra time cost analysis one idea also done analysis fwi result see work 2023 multilayer perceptron bayesian neural network based elastic implicit full waveform inversion 5 luis carlos escobar arena subsurface marine geoscientist msc geosciences 1 awesome juan daniel 1 matteo ravasi senior research advisor mlops engineer shearwater pylops core developer 1 congratulation juan great piece work new idea also precis",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "f7e31b9aad6726022e80061e663c5a54.txt",
    "text": "08 app george karniadakis george karniadakis charles pitt robinson john palmer barstow professor applied mathematics brown university 1 uq pinns neural operator critical topic trustworthy scientific machine work focused especially noisy input noisy output network new study zongren zou xuhui meng develop new bayesian framework uq noisy input output aply pinns deeponet fno neuralnetworks machinelearning pinns trustworthyai operator pde 418 9 joseph morlier professor multidisciplinary design optimization group leader sustainable aerostructures interaction 1 pramudita satria palar geetha gopakumar nair head ml computational science engineering energy csee halliburton 1 pradyumna singh rathore nicolas spogis founder ceo ai4tech turning simulation intelligence driving engineering 1 jo lucas de sousa almeida alberto costa nogueira junior 1 katayoun eshkofti postdoctoral fellow physic informed neural network pinn researcher 1 wow another great paper thank sharing u 2 florian de vuyst professeur de universit labo biom canique et bioing nierie resp mention de master ing nierie de syst me complex isc ancien membre de comit anr expertise de candidature iuf expert hc re 1 lucas wicher 1 jos morale escalante phd assistant professor department mathematics physic astronomy university texas san antonio 1 isaul garcia fyi anirudh deodhar principal architect r quantiphi digital twin physical ex tc research 1 rishi parekh milad ramezankhani rohan dekate phd student purdue 1 christian moya amir",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "fb40e8bccfc6ca38e5dc7e2e6e12347e.txt",
    "text": "08 app philip obiorah philip obiorah scientist development coach computing co organizer pydata milton kenyes lead gdg cloud port harcourt 4 pydata london three packed day june 6 8 sharing connecting global community responsible probabilistic modeling llm event full insight conversation valuable chris fonnesbeck tutorial bayesian time series pymc hand walkthrough modeling forecasting elizabeth osanyinro delivered insightful talk building inclusive machine grounded real world impact luca baggi broke llm served production demystifying prefill decode caching salman khan shared practical strategy applying transfer audio classification even limited cheuk ting ho reflected real world usefulness coding assistant ian ozsvald facilitated open honest leadership roundtable john sandall ran hand fairness workshop leanne fitzpatrick closed keynote grounded u really mean scientist fast moving era ines montani showed extract structure document using spacy layout jay alammar broke llm evolution clarity great visuals gave useful perspective direction nlp equally memorable people met ali shakil andrea melloncelli tamara romero hernandez anthony ashwin olga pustovalova olamide faroun faroun ufuoma ibude elizabeth osanyinro john sandall john carney tun shwe kuna fomboh ryan jessie priyanka fei phoon thanks engaging conversation shared curiosity many parallel track definitely wished could one room time big thanks schoo",
    "cluster": 9,
    "keywords": "machine, neural, bayesian, network, paper, work, app, 08, ml, prediction"
  },
  {
    "filename": "078914ca0bad39f883eb53fe048bd462.txt",
    "text": "08 app pat walter pat walter deploy machine drug discovery essential provide intuitive confidence estimate prediction new paper former colleague steven kearnes patrick riley describes method calibrating regression author also provide github repository code example kudos pat steve ordinal confidence level assignment regression prediction pub ac org 243 3 derek debe aloha paper 2012 put everything probability space 0 1 essentially smoothing ordinal space achieved binning spline fitting advantage easy combine score addition get nice poisson binomial distribution function behavior upon combination many term 7 597 157 1 response response pat walter machine improves molecular prediction impact large language drug discovery understanding diagnosis prediction reliability improves drug development tip machine success predictive modeling application medicine career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 10,
    "keywords": "prediction, machine, cookie, 08, app, conformal, com, phd, ml, decision"
  },
  {
    "filename": "0a1a21356f234848813903bdb3f76c0f.txt",
    "text": "08 app red buffer red buffer 23 125 8 rely heavily quality training encounter fall outside training distribution called distribution ood sample prediction become unreliable understanding addressing essential creating reliable robust system machine engineer talha explains article intro deep medium com 30 23 125 understanding reliability understanding generate false information influence outcome poor affect result importance supply chain impact accuracy career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 10,
    "keywords": "prediction, machine, cookie, 08, app, conformal, com, phd, ml, decision"
  },
  {
    "filename": "0be50495d2543eebf0a55572214e8df9.txt",
    "text": "08 app maryam miradi phd maryam miradi phd vp chief scientist 20 year agent training llm vision science training ml dl newsletter 32k member 1 deep revolutionized field one major challenge prediction probability often used measure context classification always correct predicted probability overly confident even true probability well known occur due lack noise presence conflicting information lead incorrect prediction aleatoric due inherent randomness epistemic due lack knowledge information discrepancy prediction inconsistent reality form systematic error adjusts predicted probability common technique platt scaling isotonic regression construct prediction interval quantify communicate individual prediction use probability distribution weight neural network randomly drop neuron training reduces overfitting generalize better us dropout inference time obtain multiple prediction estimate similar ensemble different initialization architecture reduce mapie quantifying uncertainty controlling risk tensorflow probabilit",
    "cluster": 10,
    "keywords": "prediction, machine, cookie, 08, app, conformal, com, phd, ml, decision"
  },
  {
    "filename": "29b89aa44a1624af6ed813fe21f0a9f6.txt",
    "text": "08 app carl mcbride elli phd carl mcbride elli phd predictive analytics tabular orange book machine 9 one machine prediction always come hypothetically decomposed aleatoric epistemic subdivided estimation review paper take closer look nature component machinelearning datascience source machine statistician view arxiv org 64 2 mohamed doctoral student university bonn quantification explainable machine crop monitoring 9 one best lee slutes statistician econometrician complexity scientist 8 interesting read understood anyone building statistical 14 058 502 1 barbie gone los alamo carl mcbride elli phd understanding end end machine process machine transforms chemistry latest trend machine tip machine success ensemble improves prediction importance clean prediction career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 10,
    "keywords": "prediction, machine, cookie, 08, app, conformal, com, phd, ml, decision"
  },
  {
    "filename": "3c8abbb7a4339594fde2e98ba425e6da.txt",
    "text": "08 app maryam miradi phd maryam miradi phd vp chief scientist 20 year agent training llm vision science training ml dl newsletter 32k member 1 deep reason type solution 20 top python library present significant challenge deploying reliable deep system refers inability predict performance different scenario reason impact uncertain prediction lead unreliable unsafe application outcome type epistemic due limited knowledge aleatory inherent randomness distributional change distribution time discrepancy mismatch assumption real world dynamic structural inaccuracy structure affecting prediction solution conformal prediction provide valid prediction interval assumption bayesian neural network incorporate network weight ensemble method combine multiple reduce error dropout bayesian approximation use dropout estimate monte carlo method sample prediction gauge variability bootstrapping resample enhance robustness adversarial training train perturbed input improve resilience quantification measure interpret confidence distribution detection identify handle novel input python library solution conformalpy simplifies implementing conformal prediction python pymc3 bayesian modeling probabilistic ml tensorflow probability probabilistic reasoning statistical analysis",
    "cluster": 10,
    "keywords": "prediction, machine, cookie, 08, app, conformal, com, phd, ml, decision"
  },
  {
    "filename": "4101eaae35c245912a5364f30b619d18.txt",
    "text": "08 app aaron pickering aaron pickering applied science science lead 3 big believer trying understand machine roll make using decision process much easier wrong sometimes understanding let value control mitigate instance advance conformal prediction technique making round estimating interval black box betting going popular stomach bit math check paper giving introduction topic highly recommended gentle introduction conformal prediction distribution free quantification 53 5 rod rivera empowering next billion automators 3 interesting specific idea gain popularity sudden conformal prediction around decade part relatively obscure limited small subset student vovk thank valeriy manokhin phd mba cqf intensive effort popularising subject making approachable non academic 2 3 487 71 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 10,
    "keywords": "prediction, machine, cookie, 08, app, conformal, com, phd, ml, decision"
  },
  {
    "filename": "499029f65098f440e725dd426b6153bf.txt",
    "text": "08 app aijun zhang aijun zhang head machine validation engineering well fargo 1 new piml tutorial quantification prediction pre trained machine take deep dive conformal prediction regression introduce several way running split conformal prediction demonstrate detect unreliable region via piml reliability testing agus sudjianto valeriy manokhin phd mba cqf patrick hall parul pandey conformalprediction reliabilitytest modeldiagnostics machinelearning piml diagnostics prediction piml medium com 158 2 patrick hall machine risk management 1 love conformal approach important keep waiting science field awaken notion variance 5 serigne cisse quantitative analyst scientist 1 lucas morin 1 5 762 87 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 10,
    "keywords": "prediction, machine, cookie, 08, app, conformal, com, phd, ml, decision"
  },
  {
    "filename": "5195b9c9987f1afd574967aa67eca245.txt",
    "text": "08 app maryam miradi phd maryam miradi phd vp chief scientist 20 year agent training llm vision science training ml dl newsletter 32k member 2 deep revolutionized field one major challenge prediction probability often used measure context classification always correct predicted probability overly confident even true probability well known occur due lack noise presence conflicting information lead incorrect prediction aleatoric due inherent randomness epistemic due lack knowledge information discrepancy prediction inconsistent reality form systematic error adjusts predicted probability better align true class proportion two common calibration technique platt scaling isotonic regression use probability distribution weight neural network randomly drop neuron training reduces overfitting help generalize better combine multiple improve accuracy reduce us dropout inference time obtain multiple prediction estimate similar ensemble use different initialization architecture reduce",
    "cluster": 10,
    "keywords": "prediction, machine, cookie, 08, app, conformal, com, phd, ml, decision"
  },
  {
    "filename": "52b1db2800db819e765b310939fce33e.txt",
    "text": "08 app biohack medium biohack medium 1 391 4 earn clinician trust new study say yes rethink measured peer reviewed research article plo digital health july 29 tested several bayesian approach predicting prostate cancer specific mortality using large scale plco reached strong performance auroc 0 86 distribution ability communicate prediction trusted varied widely standout method spectral normalized neural gaussian process sngp use explicit distance aware bayesian prior approach provided reliable estimate epistemic ensemble variational bayesian neural network signaling faithfully sngp offer safeguard overconfidence clinical decision support system matter personalized medicine depends knowing predicts confident research point toward tool powerful trustworthy practice source published july 29 comment biohack want science health news like biohackyourself healthnews sciencenews researchupdates biohack disclaimer content educational entertainment purpose substitute medical advice always consult healthcare professional full disclaimer explore angle ancient wisdom modern science everything allegiance big pharma big natural cite study encourage read question funding review method stay curious journal equal peer reviewed perfect check source think critically decide one study full story science evolves inform",
    "cluster": 10,
    "keywords": "prediction, machine, cookie, 08, app, conformal, com, phd, ml, decision"
  },
  {
    "filename": "56203993cd555738c5e9a31009673d1f.txt",
    "text": "08 app agus sudjianto agus sudjianto geek speak co creator piml modeva svp risk technology h2o retired evp head well fargo mrm 1 understanding prediction critical validation prior deployment piml us conformal prediction slicing identify weak region aijun zhang head machine validation engineering well fargo 1 new piml tutorial quantification prediction pre trained machine take deep dive conformal prediction regression introduce several way running split conformal prediction demonstrate detect unreliable region via piml reliability testing agus sudjianto valeriy manokhin phd mba cqf patrick hall parul pandey conformalprediction reliabilitytest modeldiagnostics machinelearning piml diagnostics prediction piml medium com 138 8 eduardo canabarro engineer financial economist 1 agus sudjianto future unpredictable especially extreme event us past implicitly assumes future like past know physic law nature permanent economics context always changing especially affect probability nature extreme event careful may look neat intuitive mostly wrong extreme risk capital estimation much experience since 1980s see clearly today 4 patrick hall machine risk management 1 variance real 0 8888 auc better 0 8887 auc variance could start thanks agus team making tool help real world problem 8 jack xu mod",
    "cluster": 10,
    "keywords": "prediction, machine, cookie, 08, app, conformal, com, phd, ml, decision"
  },
  {
    "filename": "5bfc62583c11dae08e7ff329da7d57b3.txt",
    "text": "08 app pankaj bhagwat pankaj bhagwat postdoctoral research fellow university alberta statistician ml researcher fairness privacy instructor 5 excited share presenting work cbma improving conformal prediction bayesian averaging iclr2025 singapore paper explores bayesian averaging combined conformal prediction produce reliable efficient prediction set step forward quantification ml attending iclr stop poster presentation talk bayesian perspective new insight conformal prediction potential application trustworthy reliable poster session info thursday april 24 00 12 30 p hall 3 hall 2b poster 426 singapore expo paper author pankaj bhagwat linglong kong bei jiang institution university alberta thanks iclr community reviewer feedback support working bayesian inference robust ml fairness privacy let connect actively exploring applied collaboration space bayesianmodelaveraging conformalprediction posterpresentation uncertaintyquantification machinelearning trustworthyai iclr2025 airesearch mlfairness 24 1 akash nagare analyst fnf india ex intern c dac set qualified msc statistic dept statistic sppu pune inspire scholar iit jam m air 258 5 truly inspiring pankaj bhagwat dada someone like passionate statistic bayesian inference ml work integrating cbma conformal prediction fascinating incredibly relevant wishing best presentation iclr way join view poster session online 825",
    "cluster": 10,
    "keywords": "prediction, machine, cookie, 08, app, conformal, com, phd, ml, decision"
  },
  {
    "filename": "61dcc8ee80bac63cedf72df435fea338.txt",
    "text": "08 app bertjan broeksema bertjan broeksema technical leadership craftsman problem solver 1 driven decision making new although market might felt way past year prediction neither new concept though new technique rise similar pace new modeling technique arise colleague robbert van kortenhof great writeup different kind deal conformal prediction datascience decisionmaking robbert van kortenhof machine engineer bigdata republic 1 science limitation path reliable fair prediction latest blog post bigdata republic dive conformal prediction help quantify improve decision making find link comment blog cover epistemic v aleatoric distinction knowledge gap inherent randomness key robust better decision making incorporating lead fair transparent outcome application credit scoring investment conformal prediction agnostic framework generates statistically valid prediction interval set class going step step example conformal prediction python using library like crepe datascience conformalprediction uncertaintyquantification 4 1 775 348 6 reflecting dutch summit bertjan broeksema 3 five principle effectuation bertjan broeksema 4 capable mature bertjan broeksema 4 future driven decision making modeling influence decision making driven decision mislead make driven design decision question ask driven decision importance",
    "cluster": 10,
    "keywords": "prediction, machine, cookie, 08, app, conformal, com, phd, ml, decision"
  },
  {
    "filename": "65c1cf1ca366d1c555d49e95a6276c12.txt",
    "text": "08 app owkin owkin 57 260 6 latest ml research paper quantification uq machine accepted presentation aistats developed new metric binary classification validated along existing metric 1 first metric uq auc identifies input higher misclassification probability 2 second uq c index detects case strongly incorrectly confident prediction metric computed using standard class label making practical implement matter ml system become prevalent healthcare application understanding uncertain essential safe deployment however estimation complex difficult validate providing theoretically grounded metric evaluate utility estimator finding pave way broader adoption uq deep application read pre print 65 57 260 owkinews happened september owkin 1 owkinews happened august owkin 1 owkinews happened july owkin 2 latest trend machine machine impact healthcare addressing reliability challenge medical quantization transforming performance impact class imbalance predictive improving diagnostic accuracy career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 10,
    "keywords": "prediction, machine, cookie, 08, app, conformal, com, phd, ml, decision"
  },
  {
    "filename": "6ba5c5578305d70e927e995ca6203ae3.txt",
    "text": "08 app richard riley r richard riley r prof biostatistics university birmingham 7 new bmj paper risk estimate clinical prediction rationale challenge approach provide risk estimate argue presenting associated includes pro con ppie method hope helpful risk estimate clinical prediction rationale challenge approach bmj com 255 7 amel murrani software development genomics professional driving quality health tech 7 evolving landscape healthcare important clinical prediction provide prediction also transparently quantify approach mirror real world decision making process clinician routinely assess probability uncertainty patient care embracing reflects healthcare complexity step toward reliable human centered application medicine 2 matthew dunn trustee institute physic engineering medicine ipem 7 true patient get full information would really interesting look gp risk score outcome see right identify cohort differ 2 2 295 80 impact innovation include optimization predictive modeling application medicine understanding risk attribution predict cardiac risk recent development disease modeling career productivity finance soft skill emotional intelligence project management education cookie 163 com",
    "cluster": 10,
    "keywords": "prediction, machine, cookie, 08, app, conformal, com, phd, ml, decision"
  },
  {
    "filename": "71167e857ffcd3fd6b7626754be9e4f7.txt",
    "text": "08 app eric durand eric durand drug discovery 6 research ml quantification uq accepted aistats paper developed validated new metric uq posted importance ml robustness especially method deployed real world biomedical setting ml system become prevalent healthcare application understanding uncertain essential safe deployment however estimation complex difficult validate providing theoretically grounded metric evaluate utility estimator finding pave way broader adoption uq deep application read preprint congrats talented team arthur pignet chiara r gniez john klein 46 5 097 148 include optimization addressing reliability challenge medical evaluating accuracy generated insight ensure trust medical output application early diagnosis patient outcome quantization transforming performance career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 10,
    "keywords": "prediction, machine, cookie, 08, app, conformal, com, phd, ml, decision"
  },
  {
    "filename": "93ac03d871a254caf0de865752a05885.txt",
    "text": "08 app dolamu oludare dolamu oludare telecommunication infrastructure management technology engineering applied machine cloud 11 published article probability role machine discussing naif bayes theorem used reliability study also risk assessment also implemented explained algorithm scratch python probability mathematics naivebayes python probabilitydistribution statistic probabilistic modeling building machine system plainenglish io 5 1 187 171 role probability analysis understanding reliability role machine wildfire prevention impact innovation resource interpretable machine understanding diagnosis prediction reliability career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 10,
    "keywords": "prediction, machine, cookie, 08, app, conformal, com, phd, ml, decision"
  },
  {
    "filename": "9be0e42785eecc32940119ebfaa6ce87.txt",
    "text": "08 app yannig goude yannig goude senior researcher statistical edf lab 2 glad attend course conformal prediction quantify machine given margaux zaffran enbis european network business industrial statistic conference valencia today julie josse aymeric dieuleveut olivier feron 97 13 yannig goude senior researcher statistical edf lab 2 samuel amadigwe mastercard foundation scholar uoe 24 25 operation research science finance 2 yannig goude link course love see mohamed amine kacef ph researcher financial mathematics actuary stochastic process monte carlo simulation option pricing statistical inference 2 est il possible avoir une version consultable de ce cours matteo fontana assistant professor scientist curious person 2 cool wow judith g 2 wonder anyone view quantify v cross validation accuracy tight band poor mse r 2 cross validation conclusion trustworthy inconclusive understand two aspect validation right way relating 2 142 125 2 phd position sound yannig goude 2 online prediction expert aggregation package r opera available cran yannig goude 9 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 10,
    "keywords": "prediction, machine, cookie, 08, app, conformal, com, phd, ml, decision"
  },
  {
    "filename": "b34f422d4bc7e0c4e75da1916c8b7a50.txt",
    "text": "08 app elvis madavane ondego elvis madavane ondego senior health information specialist research analytics 16 year healthcare innovation cancer registry medical coding digital health client experience leader 9 next publication work title heisenberg principle implication health analytics introduction heisenberg principle fundamental quantum mechanic posit certain pair property position momentum cannot measured absolute precision simultaneously heisenberg 1927 principle though rooted physic provides useful framework understanding uncertainty field health analytics health often characterized incomplete record variability patient condition measurement error face challenge echo principle article explores concept quantum mechanic applied health analytics especially predictive modeling patient outcome theoretical background quantum mechanic heisenberg principle suggests measuring one variable precisely inherently introduces measurement complement heisenberg 1927 health analytics similar issue arise collecting analyzing patient variable like disease progression patient response treatment environmental factor introduce significant healthcare datasets aikins et al 2019 complicates predictive analytics rely attempt forecast patient outcome understanding parallel becomes clear health inherent unavoidable factor must accounted research clinical application sweeney et al 2020 material method review recent literature health analytics",
    "cluster": 10,
    "keywords": "prediction, machine, cookie, 08, app, conformal, com, phd, ml, decision"
  },
  {
    "filename": "b645014e24318612fe3e72eaa2fd3b26.txt",
    "text": "08 app timur bikmukhametov phd timur bikmukhametov phd helping grow ml skill career content world top 3 ml linkedin voice favikon rank 2 7 best ml description pro con creates reliable confidence interval analyzing historical prediction error work architecture mathematically guarantee coverage become computationally expensive volume grows bayesian framework quantifies learned covariance structure delivers theoretically sound estimate strong calibration struggle computational efficiency large scale problem directly prediction interval estimating conditional quantile function straightforward implementation clear interpretability distributional assumption requires training separate per quantile limited aleatoric maintains probability distribution weight rather point estimate natural regularization overfitting principled bayesian computationally intensive inference sensitive prior specification generates sample posterior distribution using probabilistic sampling chain theoretically exact result broad applicability across type extremely time consuming particularly high dimensional parameter space construct flexible posterior approximation via sequence invertible transformation handle complex distribution efficiently exact likelihood computation architecture design challenging train",
    "cluster": 10,
    "keywords": "prediction, machine, cookie, 08, app, conformal, com, phd, ml, decision"
  },
  {
    "filename": "c2fa58e0956e309f319f0de37e549081.txt",
    "text": "08 app christoph molnar christoph molnar interpretable machine modeling mindset christophmolnar com 8 quantification method ml usually attach score prediction conformal prediction turn upside fix level get matching set interval prediction learn cp recommend book 204 3 christoph molnar interpretable machine modeling mindset christophmolnar com 8 link book 1 pablo conte merging intuition quantum engineer scientist qiskit advocate phd candidate 8 valeriy manokhin phd mba cqf 1 abdul jalil shreim 8 baha abusalem check 28 982 491 include optimization improve predictive accuracy quantization transforming performance use accurate forecasting technique ensemble improves prediction machine transforms chemistry career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 10,
    "keywords": "prediction, machine, cookie, 08, app, conformal, com, phd, ml, decision"
  },
  {
    "filename": "d90ab549537a6706a550d07b994dfda4.txt",
    "text": "08 app hesham moubarak hesham moubarak senior principal geophysicist ml innovator seismic interpretation expert reservoir optimization energy transition university lecturer 20 year global leadership 2 explainable xai quantification uq addressing black box nature complex build trust understand prediction especially critical high stake decision advanced application interpretability technique using method like shap shapley additive explanation lime local interpretable agnostic explanation attention map understand made specific prediction e g flagged area fault aware modeling developing explicitly quantify associated prediction e g bayesian neural network deep ensemble conformal prediction vital risk assessment exploration drilling hazard mitigation decision support system integrating xai uq output user friendly interface provide geoscientists engineer transparent prediction confidence level supporting informed decision making darren brown tech girl 3 artificial intelligence beyond buzzword longer futuristic idea already shaping live work interact machine deep automation neural network transforming industry lightning speed innovation come responsibility infographic highlight type application also ethical concern must address privacy security bias fairness algorithm impact job society responsible human centered design big question balance innovation ethical responsibility love hear perspective",
    "cluster": 10,
    "keywords": "prediction, machine, cookie, 08, app, conformal, com, phd, ml, decision"
  },
  {
    "filename": "d987c0b53ed14db522f281a8ad825d5f.txt",
    "text": "08 app towards science towards science 643 514 6 stop guessing start quantifying archit datar new article introduces ml python package brings estimation machine learn get reliable prediction understand limitation quantification machine easy python interface towards science 16 643 514 reading list week towards science 3 reading list week towards science 4 reading list week towards science 4 latest trend machine tip machine success quantization transforming performance include optimization optimize machine performance manage supply chain career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 10,
    "keywords": "prediction, machine, cookie, 08, app, conformal, com, phd, ml, decision"
  },
  {
    "filename": "dfdf171e85c3f5524a96ef932499ce6b.txt",
    "text": "08 app jarne verhaeghe jarne verhaeghe scientist russel phd trustworthy causal storyteller 3 one step closer enhancing antimicrobial treatment artificial intelligence machine published paper developed machine predict antibiotic plasma concentration compared currently used method called population pharmacokinetics even provides quantification prediction offer even interpretability assistance clinician evaluated using new quantification metric go look interested machinelearning development evaluation quantifying machine predict piperacillin plasma concentration critically ill patient bmc medical informatics decision making bmcmedinformdecismak biomedcentral com 26 354 13 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 10,
    "keywords": "prediction, machine, cookie, 08, app, conformal, com, phd, ml, decision"
  },
  {
    "filename": "ef3b92ff9a321f772b1b6df2beda15a5.txt",
    "text": "08 app mojtaba farmanbar phd mojtaba farmanbar phd head research scientist 1 looking forward presenting pdamsterdam2024 quantification trustworthy excited dive discussion peer big shout organizer making happen trustworthyai uncertaintyquantification conformalprediction pydata amsterdam 5 372 1 world machine increasingly used make critical decision understanding quantifying key building reliable trustworthy join mojtaba farmanbar phd pdamsterdam2024 talk quantification much trust machine imagine machine predicts whether image contains cat traditional give simple yes confident prediction quantification come play mojtaba introduce powerful framework called conformal prediction cp conformal prediction add layer confidence prediction providing range possible outcome along measure certainty agnostic distribution free approach ensures stakeholder make informed decision especially high stake area like healthcare finance autonomous system talk mojtaba guide fundamental conformal prediction practical application regression classification problem game changer quantification end clear understanding apply framework enhance decision making project miss opportunity learn trust make better informed decision 93 2 daria lazko antwerp portrait personal branding",
    "cluster": 10,
    "keywords": "prediction, machine, cookie, 08, app, conformal, com, phd, ml, decision"
  },
  {
    "filename": "f9ad3783d9216aa9346f6bb856c66ccc.txt",
    "text": "08 app arthur pignet arthur pignet research scientist owkin 4 great presented work aistats2025 last week thanks everyone insightful discussion hope contribution quantification uq prove helpful paper provides theoretical insight two metric evaluating binary classifier show maximizing metric yield score allow controlling misclassification error disagreement bayes optimal classifier respectively experiment also demonstrate easy compute metric correlate well costly annotation heavy alternative paper code matter ml system become prevalent especially critical application like healthcare understanding uncertain essential safe reliable deployment however estimation complex challenging validate providing theoretically grounded metric evaluate utility estimator finding aim pave way broader adoption uq deep application 83 1 f lix balazard statistical project leader 4 perfect dimension poster 1 995 8 machine impact healthcare addressing reliability challenge medical importance annotation accuracy challenge writing classifier evaluating accuracy generated insight impact accuracy career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 10,
    "keywords": "prediction, machine, cookie, 08, app, conformal, com, phd, ml, decision"
  },
  {
    "filename": "fc8862d0fe221a23fd131eab93cf6de0.txt",
    "text": "08 app ali ganji phd ali ganji phd scientist specializing marketing analytics clv prediction machine science deep python spark sql 4 prediction interval used quantify around prediction made machine word help u quantify confidence prediction made ml quantile regression traditional resampling bootstrap jacknife bayesian approach used obtain interval however method suffer lack theoretical guarantee computationally expensive dependent problem mapie implement state art resampling method provide well calibrated prediction interval theoretical guarantee ml trained using sklearn package machinelearning artificialintelligence ml mapie uncertainty back machine towardsdatascience com 17 1 1 466 52 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 10,
    "keywords": "prediction, machine, cookie, 08, app, conformal, com, phd, ml, decision"
  },
  {
    "filename": "2d314d455a62d17fad9e8a84dc78bcc7.txt",
    "text": "08 app serg ma serg ma science interpretable machine 1 robust machine thrives formulating precise prediction also articulating certainty prediction effectively however ml give u point estimate single number prediction however point estimate tell whole story lack critical component confident prediction need uq come play ensuring machine application trustworthy enter cp groundbreaking approach fill gap providing robust framework quantifying unlike traditional method cp give guess give range possible outcome guarantee likely true outcome fall within range crucial allows better decision making clearly outlining risk associated different action based prediction cp best choice quantification cp agnostic meaning work pre trained including complex neural network produce prediction set contain ground truth user specified probability make cp incredibly versatile applicable across various field computer vision natural language processing easy understand use provides explicit non asymptotic guarantee without needing assume anything underlying distribution imagine comprehensive guide introduces world conformal prediction also expertly walk application real world scenario practical guide applied conformal prediction python valery manokhin guide book treasure trove knowledge cover",
    "cluster": 11,
    "keywords": "prediction, conformal, machine, book, interval, method, ml, quantification, use, distribution"
  },
  {
    "filename": "31c5e8464f5ba41402a1f3f8ef20b2d6.txt",
    "text": "08 app christoph molnar christoph molnar interpretable machine modeling mindset christophmolnar com 1 trouble quantification ml lack guarantee prediction interval short class probability miscalibrated solution conformal prediction get started wrote beginner friendly hand book 266 11 william chiu vp predictive modeling national mortgage insurance corporation 1 liked book explanation intuitive code example helpful hope see venn abers predictor next edition book 1 stanley lazic co founder cso prioris inc building bayesian ml 1 enjoyed balanced presentation book conformal prediction without hype could alternative title unlike book market 2 craig mcneile lecturer theoretical physic high performance computing science 1 enjoyed reading 1 benjamin l cdso directeur associ product concrets et durables 1 ordered team eulidia next one interpretability thanks christoph molnar writing high quality deeply relevant content accessible way 5 sandeep k head machine strategy delivery allianz personal line insurance 1 maria isabel navarro jim nez 1 adrian olszewski clinical trial biostatistician 2kmm 100 r based cro frequentist nhst non bayesian paradigm scientist ml big anti car meat cash house c40 restriction 1 valeriy manokhin phd mba cqf good day good news 1 joe h 1 christian mitrache 1 28 982 491 career productivity finance soft skill emotional int",
    "cluster": 11,
    "keywords": "prediction, conformal, machine, book, interval, method, ml, quantification, use, distribution"
  },
  {
    "filename": "566bf750d88f936878801dd3f2c69327.txt",
    "text": "08 app nirakar padhy nirakar padhy lead scientist machine python aws mlops mathematics statistic 1 valeriy manokhin phd mba cqf past five year scientist worked relentlessly improve accuracy reliability machine even though optimized error metric used multiple explainable xai method make ml reliable make non technical people understand prediction used often wonder truly guarantee prediction accurate certain degree confidence probability calibration method offered help giving probability instead class score always wanted certainty prediction accuracy discovered conformal prediction 2024 amazing technique provides calibrated probability range confidence level instead point prediction ensuring prediction backed statistical guarantee conformal prediction could really change approach predictive modelling task giving reliable calibrated prediction interval course fantastic opportunity dive deep method integrate current making even better winning spot course would improve skill also help deliver reliable insight work excited explore technique apply project better decision making thank considering entry looking forward chance learn conformal",
    "cluster": 11,
    "keywords": "prediction, conformal, machine, book, interval, method, ml, quantification, use, distribution"
  },
  {
    "filename": "5ef0b6d1826170c349851e26b0611d6b.txt",
    "text": "08 app scikit learn scikit learn 121 781 pydata paris 2024 45 minute keynote olivier grisel handling predictive machine machine practitioner build predictive noisy resulting uncertain prediction noise mean machine context python machinelearning datascience opensource keynote olivier grisel handling predictive machine pydata paris 2024 104 5 frits herman scientist ing interesting presentation nice show well tuned pipeline containing logisticregression result well calibrated prediction actually think would work classification algorithm example randomforestclassifier almost never result well calibrated probability even applying thourough hyperparameter tuning algorithm unlikely produce probability close 0 1 score tree ensemble need predict 0 1 order mean 0 1 olivier grisel machine engineer probabl btw explore idea related code detail masterclass event organized probabl paris office december 11 2 r wehrung j adore 1 anderson chaves lead scientist light management coating essilorluxottica attended one believe nannyml people might interested topic 1 bagus aji scientist enthusiast industrial engineer driving cost optimization sale growth strategic thinking advanced analytics highly",
    "cluster": 11,
    "keywords": "prediction, conformal, machine, book, interval, method, ml, quantification, use, distribution"
  },
  {
    "filename": "619cd5de5161d015b9be9f976312b110.txt",
    "text": "08 app chelsea parlett chelsea parlett statistician scientist 2 confirm excellent read highly recommend check michael clark talent explaining thing rigorous approachable way pleasure review book michael clark senior machine scientist 2 thrilled announce book completed editing process ready publication incredible journey excited share book dive key concept across modeling landscape traditional statistic deep approach also cover processing estimation interpretation hope serf valuable resource practitioner learner alike print release august 15 publisher crc press part science series web version huge thank team crc press assistance malcolm barrett chelsea parlett demetri pananos ph thoughtful review helped make book even better crcpress datascience modeling machinelearning causalinference demystified clark github io 33 3 john flournoy social science research computing consultant 2 high praise chelsea putting front queue course michael work already come super frequently kagi search 1 sep dadsetan bio professional startup minded turning idea impact 2 well written thank michael 2 michael clark senior machine scientist 2 super thanks review chelsea parlett definitely better result 4 390 164 4 new stats post chelsea parlett 8 basic hypothesis test cle",
    "cluster": 11,
    "keywords": "prediction, conformal, machine, book, interval, method, ml, quantification, use, distribution"
  },
  {
    "filename": "80b4028125c3c6faf38867e2fee53e31.txt",
    "text": "08 app christoph molnar christoph molnar interpretable machine modeling mindset christophmolnar com 2 picking ml class interpretability built convenience quantification sure way lose performance thanks agnostic tool cake eat least bite attached certain class pick winner selection agnostic come option extra uq interpretability agnostic tool list save later quantification use conformal prediction quantify cp turn weak score rigorous prediction interval example class probability classification set quantile regression conformalized quantile regression interpretation many agnostic interpretation method could write book selection shap explaining individual prediction permutation feature importance partial dependence plot feature effect analysis variance ml function decomposed lower dimensional component decomposition related interpretability offer advantage attribution target variance individual feature like anova stats causality orthogonal double machine brings supervised causal effect estimation based training two ml one treatment one control design loss evaluation metric task seemingly limited range specialized solve like quantile regression turn agnostic task translate task l",
    "cluster": 11,
    "keywords": "prediction, conformal, machine, book, interval, method, ml, quantification, use, distribution"
  },
  {
    "filename": "83812c4cd30d335411d974ec7dde3cbb.txt",
    "text": "08 app rafael izbicki rafael izbicki associate professor statistic ufscar phd carnegie mellon university science statistic machine cnpq research fellow 8 book announcement excited share new book targeted practitioner researcher good mathematical background book provides tool make prediction reliable addressing aleatoric epistemic uncertainty explores essential topic conditional density estimation gaussian process bart conformal prediction calibration technique featuring real world application cosmology disease surveillance equips reader move beyond point prediction artificial intelligence better communicate find hand example code accompanying github repo download free get printed copy rafaelizbicki com uq4ml access outside brazil rafaelizbicki com uq4mlpt brazil hope find useful machinelearning uncertaintyquantification datascience predictiveanalytics statistic book artificialinteligence 708 30 vinicius albani applied math math modeling inverse problem 8 prezado rafael izbicki esse um tema de grande import ncia tenho muito interesse assunto muito obrigado por compartilhar 1 juv ncio santos nobre professor titular na universidade federal cear estat stico 8 parab n pelo livro e muito obrigado por disponibiliz lo ontem j adquiri meu 1 thyago rezende intelig ncia de neg cio intelig ncia competitiva business intelligence cultura de dado 8 sensacional rafael izbicki parab n 1 ivan vitor medeiros head latam para ia e arquitetura tc 8 parab n j vou pedir minha c pia importante que te",
    "cluster": 11,
    "keywords": "prediction, conformal, machine, book, interval, method, ml, quantification, use, distribution"
  },
  {
    "filename": "a289aebb278e74455c1b04b9d9e73c73.txt",
    "text": "08 app christoph molnar christoph molnar interpretable machine modeling mindset christophmolnar com 3 machine suck quantification solution almost sound good true conformal prediction work black box requires line code fast come statistical guarantee thread conformal prediction method quantification machine method take heuristic score turn rigorous one confused first conformal prediction 1 algorithm general recipe split training calibration train calculate heuristic score calibration calibrate score use calibrated scoring rule new recipe enables many use case prediction set multi class problem calibrate classification score interpreted probability fix coverage quantile regression conformal predictive distribution regression get started conformal prediction tutorial paper awesome list conformal prediction approach scikit ready python package r package subscribe newsletter mindful modeler learn conformal prediction technique make machine better 398 47 nguyen hai chau associate professor computer science uet vnuh 2 great vijay nair managing director head advanced technology modeling well fargo 3 fundamental problem conformal prediction marginal interval confidence average across x practice one need conditional prediction interval valid given x practitio",
    "cluster": 11,
    "keywords": "prediction, conformal, machine, book, interval, method, ml, quantification, use, distribution"
  },
  {
    "filename": "bce8d2b21385c4a1777ca7ad23ee033e.txt",
    "text": "08 app ivan marroquin ph ivan marroquin ph professional experience machine technology regression classification clustering community detection mining 1 packt invited review valery manokhin book conformal prediction book entertaining give helpful introduction subject anyone new statistical framework quantify reference experienced scientist one thing quickly caught attention first chapter user friendly manner introduces describes theoretical concept behind conformal prediction explanation simple follow allowing newcomer understand workflow framework quickly interested diving deeper mathematical concept find several link peer reviewed publication book appreciate appeal diverse audience giving reader option much detail learn conformal prediction throughout chapter demonstrates usability conformal prediction various machine scenario gladly surprised see brings forward advantage compared machine technique understandable way judge applicability conformal prediction interested book please follow link practical guide applied conformal prediction python learn apply best framework industry application amazon com 25 2 vinishka kalra growth lead software engineering programming packt product growth marketing 1 thank sharing insight helping others discover book ivan marroquin ph 2 vinishka kalra growth lead software engineering p",
    "cluster": 11,
    "keywords": "prediction, conformal, machine, book, interval, method, ml, quantification, use, distribution"
  },
  {
    "filename": "c6a8ccd358891865f1baafab80e97da1.txt",
    "text": "08 app vincenzo ventriglia vincenzo ventriglia machine engineer scientist analyst organiser pydata rom capitale physicist training passionate personal finance 1 physicist training scientist developing believe crucial avoid pretentious meaningless statement like cat 95 probability house estimated value 571 415 97 even though trained predict well necessarily provide prediction interval crucial correct especially high risk setting recently got interested relatively new statistical framework quantifying predictive calibrating heuristic score calibration set approach turn prediction prediction e g set classification interval regression framework come desirable perk agnostic retraining required distribution assumption needed many case requires line code come conformal prediction transcending niche status coming year could become new standard part stack many scientist already heard already used technique let talk datascience machinelearning python 225 15 vincenzo ventriglia machine engineer scientist analyst organiser pydata rom capitale physicist training passionate personal finance 1 extensive professionally curated collection conformal prediction sklearn compatible python package quantifying uncertainty controlling risk ml",
    "cluster": 11,
    "keywords": "prediction, conformal, machine, book, interval, method, ml, quantification, use, distribution"
  },
  {
    "filename": "Conformal Prediction with Python (Uncertainty Quantification in Time Series Analysis).txt",
    "text": "source reddit datascience title conformal prediction python quantification time series analysis content recently discovered conformal prediction nutshell give prediction interval instead single point prediction given input far understand work like simplified started t cp take everything write grain salt 1 choose forecasting single point prediction usual arima e nbeats nhits etc train 2 define significance level alpha e g 0 1 90 prediction lay within calculated interval 3 set coverage rate iteratively calibrate prediction interval using validation set n step 4 throw new point get prediction interval really interested using method quantification combination regression goal find swiss army knife conformal prediction feature relevant t cp feature easy use set found python library github neuralforecast mlforecasts statsforecast sktime river dart tsai lightautoml mapie crepe fortuna already researched lot honestly feel bit overwhelmed complexity math statistic coding well terminology used documentation hoping experienced scientist could help question provide clearer picture approach focus 1 general opinion conformal prediction worth digging business related task empirically valid classical approach hype 2 difference probabilistic forecasting conformal prediction conformal prediction modern approach probabilistic forecasting synonym 3 mentioned libs offer conformal prediction feature read sktime example probabilistic forecasting unsure library offer real deal case difference two 4 favorite tool libs conformal prediction include relevant easy use 5 alternative approach use calculate conformal prediction example facing regression problem excited answer sorry messed lingo anything else feel free correct awesome conformal prediction repository information source image comment try stacking method use one defined base weight according performance time series forecasting prediction interval common typically supported assuming parametric distribution eg normal package like dart sktime support probabilist forecasting conformal prediction interval non parametric ie need assume certain distribution aware python package currently support let know support quantile regression probably come close case would first get familiar standard probabilist forecasting blog post also great dive cp cool picture article use library implemented conformal prediction includes nixtla ecosystem neuralforecast mlforecasts statsforecast sktime river mapie amazon fortuna also implementation enbpi 1 empirically valid classical approach hype classical approach mean classical approach theoretical guarantee correct prediction interval sample conformal prediction 2 difference probabilistic forecasting conformal prediction forecasting definition time series conformal prediction could applied time series non time series including classification regression much 3 mentioned libs offer conformal prediction feature read sktime example probabilistic forecasting unsure library offer real deal case difference two see nixtla ecosystem neuralforecast mlforecasts statsforecast sktime river mapie amazon fortuna also implementation enbpi 4 favorite tool libs conformal prediction include relevant easy use mapie 1 conformal prediction library library listed time series non time series multiple language including python r even julia 5 alternative approach use calculate conformal prediction example facing regression problem plenty way addition inductive conformal prediction popular method include conformalized quantile regression jacknife book practical guide applied conformal prediction learn apply best framework industry application much covered available preorder thanks input think found two dedicated conformal prediction library nonconformist puncc would love hear think",
    "cluster": 11,
    "keywords": "prediction, conformal, machine, book, interval, method, ml, quantification, use, distribution"
  },
  {
    "filename": "da2155cee94b33843d6877dc952a5d4e.txt",
    "text": "08 app marco huber marco huber full professor university stuttgart head digitalization machine vision fraunhofer ipa 2 quantifying uncertainty machine conformal prediction gained importance recent year conformal prediction frequentist approach providing confidence set prediction machine pleasant property distribution free assumption underlying distribution therefore conformal prediction applied almost trained machine still provide mathematically rigorous guarantee book algorithmic random world book conformal prediction theory december 2022 2nd edition book published reading activity christmas vacation first book vladimir vovk considered inventor conformal prediction dating back late 1990s book begin basic idea behind conformal prediction extends idea advanced topic probabilistic prediction classification regression emphasis proving property various algorithm rather concrete example therefore aimed reader want dive deep theory conformal prediction like reading theorem lemma corollary instead prefer practical approach subject book probably right choice recommendation would start tutorial gentle introduction conformal prediction distribution free quantification instead addition also recommend taking christoph molnar email course conformal prediction machinelearning artificialintelligence uncertaintyquantification confor",
    "cluster": 11,
    "keywords": "prediction, conformal, machine, book, interval, method, ml, quantification, use, distribution"
  },
  {
    "filename": "e23efc816032bf80db7432fbde6fd86a.txt",
    "text": "08 app tuwe l fstr cavallin tuwe l fstr cavallin lefteris koulierakis machine architect asml strategy lead inphocal 1 machine framework designed provide prediction without quantifying usually infer prediction interval common approach quantification must product incorporation bayesian method method though come cost introducing subjective prior often result unreliable prediction interval fortunately machine domain active ever one promising framework quantification conformal prediction cp fan cp interacted valeriy manokhin one prominent ambassador framework happy share serve technical reviewer new book practical guide applied conformal prediction published packt machine project need provide prediction interval quantify uncertain prediction curious cp work book quickly intuitively introduce fascinating framework book released week already order amazon practical guide applied conformal prediction learn apply best framework industry application amazon com 68 5 893 58 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 11,
    "keywords": "prediction, conformal, machine, book, interval, method, ml, quantification, use, distribution"
  },
  {
    "filename": "f098cccc46a697920fe140d1c74accd5.txt",
    "text": "08 app sebastian raschka phd sebastian raschka phd sebastian raschka phd ml research engineer build large language scratch amzn 4fqvn0d ahead magazine sebastianraschka com llm work latest development field 2 two important concept quantifying machine prediction confidence interval conformal prediction n experience often mixed instance share result resource including confidence interval get recommendation use conformal prediction confidence interval quantifying certainty estimate conformal prediction point estimate create prediction interval confidence around given prediction paper typically report confidence interval around prediction accuracy praxis may use conformal prediction capture prediction classifier set label set contains label given confidence approach useful address different use case machinelearning datascience deeplearning 451 41 sebastian raschka phd ml research engineer build large language scratch amzn 4fqvn0d ahead magazine sebastianraschka com llm work latest development field 2 case curious creating confidence interval ml article describing 4 different approach conformal prediction christoph molnar nice series post 42 achint chaudhary ml operation research evolutionary computing enthusiast 2x gate topper c air 22 da air 24 2",
    "cluster": 11,
    "keywords": "prediction, conformal, machine, book, interval, method, ml, quantification, use, distribution"
  },
  {
    "filename": "f232689360442480c3692e6440f89140.txt",
    "text": "08 app tmls summit tmls summit 15 139 1 high stake application like medical diagnostics industrial understanding quantifying ml crucial prevent critical failure join u welcoming mahdi torabi rad president mlboost video delivers workshop quantification conformal prediction ensuring trustworthiness ml mahdi workshop dive conformal prediction robust approach provides distribution free validity explicit guarantee without relying specific distribution assumption key insight gain set conformal prediction apart quantification method foundation behind conformal prediction quantification ml creating statistically rigorous set interval using conformal prediction applying conformal prediction pre trained like neural network real world example python code sample jupyter notebook use case financial forecasting nlp computer vision mahdi torabi rad mahdi computational scientist engineer youtube content creator year experience developing ml computer code predicting complex phenomenon ml lead various deeptech startup run popular youtube channel mlboost video ml topic including conformal prediction garnered ten thousand view save register toronto machine summit looking forward see 13 15 139 career productivity finance soft skill emotional intelligence project management education cookie 163 com",
    "cluster": 11,
    "keywords": "prediction, conformal, machine, book, interval, method, ml, quantification, use, distribution"
  },
  {
    "filename": "fc3fcdbd23a1fc6332160de9a2927592.txt",
    "text": "08 app neil leiser neil leiser lead scientist artefact podcast host story imperial ucl graduate 1 conformal prediction probably one best approach quantify ml guaranteed coverage prediction region generated conformal prediction come coverage guarantee true outcome easy use conformal prediction easily implemented within couple line code agnostic work machine distribution free method make assumption regarding underlying distribution retraining used without retraining broad application work classification regression time series forecasting many task want learn get started three resource 1 strongly recommend introduction conformal prediction python christoph molnar post inspired 2 mapie python library easily quantify ml 3 great repository valeriy manokhin phd mba cqf method usually use quantify think conformal prediction good choice let know comment datascientist machinelearning mapie 416 12 mahdi torabi rad ph cto energy material ai4science educator 1 using conformal prediction make sure avoid common misstep discussed 11 tuwe l fstr cavallin assistant professor computer science j nk ping university senior software engineer saab training simulation 1 would also recommend python library crepe library super powerful conformal predictive system",
    "cluster": 11,
    "keywords": "prediction, conformal, machine, book, interval, method, ml, quantification, use, distribution"
  },
  {
    "filename": "[D] Conformal Prediction in Industry.txt",
    "text": "source reddit machinelearning title conformal prediction industry content hi everyone conformal prediction popular statistic machine community quantification wondering academic popularity deployed pipeline industry us conformal prediction tool limited understanding look like research group industry using method still reached production anyone experience industry comment comment blog post industry lab talking conformal prediction could actually used practice also joined cult kolmogorov actively repudiate bayesian prior also work industry conformal prediction got street cred scientist mostly confused exec actually using anomaly detection team book senior guy well fargo assuming using hundreeds company using production anomaly detection microsoft azure decade astra zeneca used decade speed development new drug joined pharma major multiple top bank well fargo bbva nvdia deepmind many plenty deployment across industry conformal prediction powering microsoft azure anomaly detection since 2016 astra zeneca saving hundreeds million dollar streamline drug discovery pipeline huge bank like bbva using better customer product recommendation related anyone short intro conformal prediction trying dont really feel like reading 50 page gentle introduction motivation 1 fair non d oriented executive explaining notion prediction set calibrated confidence guaranteed coverage tall order want told predicted value z value b value x time act information real time post hoc analysis thought consulting gig based social medium posting frequency use windowing function estimate confidence interval forecast inspired use heuristic stepwise validator instrument deployment stamp passport source fintech prediction set used tabular computer vision exec understand concet propability well cause combined value maybe time hand working cushy corporate job see point cover book got looking closely enough forward evp well fargo clear using prediction interval typo mobile",
    "cluster": 11,
    "keywords": "prediction, conformal, machine, book, interval, method, ml, quantification, use, distribution"
  },
  {
    "filename": "[D] Research Topics in Conformal Prediction.txt",
    "text": "source reddit machinelearning title research topic conformal prediction content background econometrics soon start work master thesis already supervisor would like come idea could integrate research one thing recently got attention quantification method specifically conformal prediction one thing seems particularly cool adapted ensure coverage across specific group covariates even label additionally recently research community able tackle limiting assumption exchangeability meaning applied example time series question two fold one curiosity personal interest 1 real world scenario seen conformal prediction shine scenario think would work 2 think interesting question yet addressed thought general feedback welcome thanks advance comment time series use variant block bootstrap cross validation way provide better confidence interval without many backtest cross validation window practice find slow compute still faster using distribution based dl also find unless lot cross validate inspire confidence free lunch applies conformal prediction definite trade make based availability train time came hybrid approach trade tended underestimate true interval fascinated quantile regression dug deeply found interesting thing called quantiles guaranteed ordered create odd stuff comparing due method sort result internally intuitive implies statistical guarantee might lacking attacked using random forest regression computing quantiles various amount dropped estimator dropped predictor could also use permutation approach turn ordering problem persists method using loss function generating quantity including ubiquitous pinball loss subject issue well randomized method use currently estimator dropout method feature dropout method least understand feel open question quantile method see pretty cool cp framework use overcome non exchangeability enbpi spci free lunch applies conformal prediction definite trade make based availability train time hi could possibly elaborate imo seems pretty good cursory glance ofc im sure people really think practice approach described coverage provide correct prediction interval conformalized quantile regression seem useful could pls clarify mean quantiles guaranteed ordered mean exist crossover quantile value method coverage guarantee produce correct prediction interval one posed variant split conformal non exchangeability issue adjust penalize conformity score find recent paper two framework kind overkill adopted major forecasting framework seen use variant split cp read old post sentence still completely missed part proactively acknowledge underestimate true interval yes exactly agree conformal prediction probably promising approach variant split conformal sentence enough see understand basic probabilistic prediction recommend gneiting paper starter first cite paper gneiting even cursory search show work relation conformal prediction close example paper critiquing quantile loss fundamentally agree use work also want people read book look repo posted subreddits frequent engage positive manner fact needed make redundant reply disclaimer already said exactly wrote try insult mean lack reading comprehension class familiar subject would realize way probabilistic predictor evaluated nothing conformal prediction evaluation method established within probabilistic prediction anyone familiar field know exactly paper defines pointing someone unfamiliar subject making claim simply stating fact personally care less whether read book used repository correcting false claim conformal prediction false claim method mentioned adaptation conformal prediction time series aka rolling cv split multi step forecasting implemented nixtla reference repo block bootstrapping train forecast horizon training length allow multiple cv window transparently mention drawback implementation could avoided",
    "cluster": 11,
    "keywords": "prediction, conformal, machine, book, interval, method, ml, quantification, use, distribution"
  },
  {
    "filename": "[N] A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification.txt",
    "text": "source reddit machinelearning title n gentle introduction conformal prediction distribution free quantification content check new gentle intro conformal prediction distribution free uq tutorial video tutorial give instruction quantifying machine algorithm accompanied python pytorch code lot explanatory illustration worked example meant hand introduction reader interested practical implementation distribution free uq necessarily statistician end able generate valid confidence interval set distribution without ever retraining please let know thought question feedback hope enjoy watching reading want learn research area come icml workshop distribution free quantification comment love pdf name gentle intro conformal dfuq pdf one need conformal prediction wow confidence interval confidence set mission critical many application theory practical implementation github repo practitioner caveat compatibility pytorch classifier academic starting show true color anyway big deal thinking making raw numpy version would helpful nice wait icml presentation great work congrats",
    "cluster": 11,
    "keywords": "prediction, conformal, machine, book, interval, method, ml, quantification, use, distribution"
  },
  {
    "filename": "[N] Tutorial on Conformal Prediction and DFUQ Part 2_ Conditional Coverage + Diagnostics.txt",
    "text": "source reddit machinelearning title n tutorial conformal prediction dfuq part 2 conditional coverage diagnostics content hey everyone posted part 2 tutorial conformal prediction distribution free quantification youtube focus conditional coverage diagnostics make sure conformal procedure working properly slightly advanced last one leave strong understanding implement evaluate conformal code let u know feedback shooting email best anastasios comment",
    "cluster": 11,
    "keywords": "prediction, conformal, machine, book, interval, method, ml, quantification, use, distribution"
  },
  {
    "filename": "[P] ðŸš€ AWS launches Fortuna, an open-source library for Uncertainty Quantification.txt",
    "text": "source reddit machinelearning title p aws launch fortuna open source library quantification content aws released fortuna library quantification fortuna support conformal prediction bayesian inference method try github star welcome github repo comment oooh aws jax benchmark verify well quantified best end end example showing would nice visual explainer thanks lot feedback best example may depend trying achieve mnist one good candidate show get calibrated estimate prediction 1 starting untrained neural network 2 starting output trained one furthermore 3 show get conformal set example include least one image run working adding example near future course contribution welcome thanks could save notebook image already displayed even though would add mb source code",
    "cluster": 11,
    "keywords": "prediction, conformal, machine, book, interval, method, ml, quantification, use, distribution"
  },
  {
    "filename": "[R] Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification - Link to a free online lectur.txt",
    "text": "source reddit machinelearning title r introduction conformal prediction distribution free quantification link free online lecture comment content comment hi free zoom lecture reddit community lecture discus quantifying ml algorithm particular test time instance rigorously guaranteeing consequential error happen frequently link event april 18 talk abstract deep computer vision common present certain begin deploying machine consequential setting like medical diagnostics self driving vehicle knowing accuracy enough need way quantifying algorithm particular test time instance rigorously guaranteeing consequential error happen frequently example car hit human discussing generate rigorous finite sample confidence interval prediction task dataset free chalk talk begin short tutorial method called conformal prediction tease flexible method work larger class prediction problem including high dimensional structured output e g instance segmentation multiclass hierarchical classification protein folding talk based speaker paper gentle introduction conformal prediction distribution free quantification set image classifier using conformal prediction distribution free risk controlling prediction set learn test calibrating predictive algorithm achieve risk control presenter bio anastasios nikolas angelopoulos third year ph student university california berkeley advised michael jordan jitendra malik 2016 2019 electrical engineering student stanford university advised gordon wetzstein stephen p boyd homepage talk recorded uploaded youtube see past lecture recording r 2d3dai",
    "cluster": 11,
    "keywords": "prediction, conformal, machine, book, interval, method, ml, quantification, use, distribution"
  },
  {
    "filename": "[R] Work in Progress_ Advanced Conformal Prediction â€“ Practical Machine Learning with Distribution-Free Guarantees.txt",
    "text": "source reddit machinelearning title r work progress advanced conformal prediction practical machine distribution free guarantee content hi r machinelearning community working deep dive project modern conformal prediction technique wanted share hand practical guide built ground aimed making advanced estimation accessible everyone basic school math python skill highlight cover everything classical conformal prediction adaptive mondrian distribution free method deep strong focus real world implementation challenge covariate shift non exchangeability small computational bottleneck practical code example using state art library like crepe torchcp others written python first applied mindset bridging theory practice love hear thought feedback question community especially anyone working quantification prediction interval distribution free ml technique anyone interested early draft guide want chat method feel free dm thanks much comment",
    "cluster": 11,
    "keywords": "prediction, conformal, machine, book, interval, method, ml, quantification, use, distribution"
  },
  {
    "filename": "[R] ðŸ“˜ New Release_ _Practical Guide to Applied Conformal Prediction in Python_ - Master Uncertainty in ML!.txt",
    "text": "source reddit machinelearning title r new release practical guide applied conformal prediction python master ml content hello reddit community excited share release must read book machine enthusiast practical guide applied conformal prediction python book fantastic resource anyone looking deepen understanding machine focus conformal prediction framework key feature expertise conformal prediction dive rapidly growing ml framework apply using python innovative technique explore state art method measurement management industry setting beyond standard ml approach discover conformal prediction different effective traditional machine technique book overview practical guide applied conformal prediction python comprehensive exploration conformal prediction framework reshaping managed machine application book cover everything basic practical application binary classification regression time series forecasting imbalanced computer vision nlp chapter offer detailed insight best practice backed practical example python using real world datasets ensures thorough understanding innovative framework quantification objective understand core concept principle conformal prediction learn difference conformal prediction traditional ml method apply conformal prediction real world industry scenario including advanced topic like imbalanced multi class cp deep dive conformal prediction framework including application forecasting nlp intended audience book ideal foundational knowledge machine python programming including scientist ml engineer academic looking advance skill quantification ml content glance introducing conformal prediction overview conformal prediction fundamental conformal prediction validity efficiency conformal prediction type conformal predictor conformal prediction classification conformal prediction regression conformal prediction time series forecasting conformal prediction computer vision conformal prediction natural language processing handling imbalanced multi class conformal prediction whether budding scientist experienced ml engineer academic researcher book invaluable resource enhancing machine toolkit get copy today step world advanced quantification ml feel free discus ask question book let explore fascinating world conformal prediction together comment also unofficial companion repo r",
    "cluster": 11,
    "keywords": "prediction, conformal, machine, book, interval, method, ml, quantification, use, distribution"
  },
  {
    "filename": "137a59b48d19bd266250c72f82b611d0.txt",
    "text": "08 app jim woodcock jim woodcock 1 pleased see paper finally print probabilistic unifying relation modelling epistemic aleatoric semantics automated reasoning theorem proving sciencedirect com 33 4 frank soboczenski assistant professor ml consultant nasa techleap prize winner quantum ml stem scientist nasa noaa globe ibm quantumresearcher bigscience huggingface nasa genelab awg roman spacecraft wg explorer 1 congratulation 1 dr seemal asif ceng fhea miet course director intelligent robotics senior lecture associate professor robotics cranfield university 1 congratulation looking forward read 1 matt banham 1 one reading list tonight 1 colin halloran technical director risq 1 already downloaded eager read 2 1 242 5 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 12,
    "keywords": "cookie, finance, machine, management, 08, com, app, project, intelligence, education"
  },
  {
    "filename": "275213c7ad47e8405e795acb461c9172.txt",
    "text": "08 app janne merilinna janne merilinna 1 towards higher epistemic today mark last official day vtt math serf right total 20 year three month joined vtt conduct master thesis february 2004 back quality driven software architecture development focus accompanied driven architecture mda believed could software system generate code press button amazing would attempted generate code thesis go planned ended writing implementation c like caveman later introduced domain specific modelling language dsml delivered coffee break encounter work found drawn towards robot distributed system production rule system ultimately led true passion machine since 2016 involved machine work diverse project privately exploring quantitative finance second long term passion deepen understanding machine participated quantitative finance competition quantopian numerai raised question robustness numerai attempted understand work consistently root cause might aleatoric perhaps epistemic concept aware back similar issue began emerge work driving desire understand underlying factor spent considerable time exploring modelling focusing random forest eventually developing method called macau macau one assess discern type explain sample novel work concluded original question raised numerai answered left",
    "cluster": 12,
    "keywords": "cookie, finance, machine, management, 08, com, app, project, intelligence, education"
  },
  {
    "filename": "2bcc30502b42f2888d4a8c173a06fbde.txt",
    "text": "08 app saurabh kumar saurabh kumar engineering adora llm infra finetuning scaling prev rapyuta ml yahoo ml nokia iit delhi 1 want go ml deep bayesian inference list topic must read 1 bayes theorem prior og formula understand intuitively anything else 2 maximum likelihood v maximum posteriori prior work 3 bayesian linear regression start simple linear regression 4 monte carlo method mcmc 5 elbo evidence lower bound everyone optimizes explain clearly 6 reparameterization trick used vaes core idea differentiable sampling 7 bayesian neural network aware deep start weight distribution 8 dropout bayesian approximation yes regular dropout kinda bayesian stuff 174 3 sagar shahari knight leetcode peak 2028 specialist codeforces deep agent analyst aspiring quant vit cse 26 1 keep sharing thing pavan kumar dubasi multi agent architect sustainable tech innovator 1 implemented bayesian nn dropout approximation quantification legit use prod confidence matter promit haldar software engineer ml adobe agentic system iit guwahati ece former adobe research kaggle expert llm machine 1 would appreciate point resource thanks sharing 25 784 1 811 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 12,
    "keywords": "cookie, finance, machine, management, 08, com, app, project, intelligence, education"
  },
  {
    "filename": "3ccbb54de275536308e848393f7e0680.txt",
    "text": "08 app matt elli matt elli svp software aurora embodied robotics autonomous vehicle ex ibm watson amazon hp startup 7 probabilistic gold great see great material pulled together marco adriano magno cto mecas eth zurich robotics alumnus 7 418 page probabilistic gold dropped easily one best course attended eth taught andreas krause note cover essential aleatoric v epistemic gaussian process mcmc method active bayesian optimization reinforcement truly grateful amazing resource available community link article 6 3 298 303 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 12,
    "keywords": "cookie, finance, machine, management, 08, com, app, project, intelligence, education"
  },
  {
    "filename": "4c4eaf95685b02771b025a58d5da1dbe.txt",
    "text": "08 app omesh tickoo omesh tickoo senior principal engineer senior director emerging system lab adjunct professor 2 understanding system know important step towards developing truly trustworthy explainable black box system built using deep principled quantification powerful method determine trust output system following release bayesian torch framework allowing developer seamlessly convert target dnn equivalent bayesian neural network quantification glad announce availability comprehensive quantization capability part bayesian torch user build aware system without sacrificing inference speed hit blog detail ranganath krishnan keyur ranipa mahesh subedar intelai xai bdl bnn bayesian intellabs quantization framework bayesian deep medium com 75 2 312 88 1 powering industry 4 0 real time defect detection edge omesh tickoo 3 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 12,
    "keywords": "cookie, finance, machine, management, 08, com, app, project, intelligence, education"
  },
  {
    "filename": "79971614bc793d79e08cfc521a2fa1fe.txt",
    "text": "08 app preetam verma preetam verma machine engineer x hcl software driving driven solution 1 backbone generative llm 1 probability distribution 2 latent variable 3 architecture 4 loss function 5 sampling technique 6 optimization algorithm summary strong foundation probabilistic equips tool understand underlying principle gpt llm enabling design evaluate improve effectively 285 25 1 thing know working llm optimization inference preetam verma 9 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 12,
    "keywords": "cookie, finance, machine, management, 08, com, app, project, intelligence, education"
  },
  {
    "filename": "7c2aaad80124ccf6118b2ca0085ca6a5.txt",
    "text": "08 app michael kirchhof michael kirchhof research scientist apple quantification 5 aleatoric epistemic clear cut concept right right new iclr blogpost let different school thought speak contradict revisit chatbots character aleatory transforms epistemic 187 4 anderson chaves lead scientist light management coating essilorluxottica 5 carl mcbride elli phd might like 1 jihye choi c phd candidate university wisconsin madison research intern google cloud research ex phd intern visa research trustworthy 5 matthew engelhard relevant long lasting discussion p joseph morlier professor multidisciplinary design optimization group leader sustainable aerostructures interaction 5 enrico foglia kajetan schweighofer johannes kepler universit linz 4 cool write michael 2 3 455 45 1 never create dataset alone michael kirchhof 4 navigate smart choice strategy communicating clearly understand concept value acknowledging leadership include optimization understanding interpretability mechanism career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 12,
    "keywords": "cookie, finance, machine, management, 08, com, app, project, intelligence, education"
  },
  {
    "filename": "823128c36109a1c60bd5d57064a70152.txt",
    "text": "08 app venkata ramesh gudapati venkata ramesh gudapati 11 recently started diffusion far one best video come across please take look deeplearning ml machinelearning denoising diffusion probabilistic ddpm explained 2 1 603 62 latest trend machine tip machine success optimize machine performance roadmap newcomer understanding behavior best gpu training technique career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 12,
    "keywords": "cookie, finance, machine, management, 08, com, app, project, intelligence, education"
  },
  {
    "filename": "9fc71534b3e481fadd9954a051337e0b.txt",
    "text": "08 app cornellius cornellius product strategy chief product officer scientist engineer helping orgs scale product 9 language predict following sequence word assigning probability potential word sequence know work process involves several step rooted probability theory use specific lecture slide julia hockenmaier contains foundation information need understand probability miss want learn get daily science tip email inbox subscribe newsletter want miss python tip datascience machinelearning knowledge python follow cornellius press bell learn together 218 8 nazeeruddeen shaik scientist agentic workflow llmops mlops nlp 9 thanks sharing 2 gina acosta guti rrez daily post resource science engineering mentor google wtm ambassador 9 thanks sharing guide cornellius yudha w 3 navya sri kurapati analyst well versed excel sql power bi python experienced preparation transformation insightful visualization dashboarding foundational knowledge ml statistical analysis 9 thanks sharing 1 abdullah rizwan engineer scientist ml llm llmops google certified analyst fastapi python api architect backend developer 9 great job thanks sharing cornellius 1 43 012 2 686 3 biology science cornellius 5 time series stationary property cornellius 6 time series building block cornellius 6 understanding foundation",
    "cluster": 12,
    "keywords": "cookie, finance, machine, management, 08, com, app, project, intelligence, education"
  },
  {
    "filename": "a2e710f2f1c1d0e30ea0a973c6b7dc5d.txt",
    "text": "08 app stefan feuerriegel stefan feuerriegel professor institute management lmu munich 11 2nd paper causalml neurips estimate randomness treatment effect random variable use partial identification obtain conditional distribution treatment effect propose new au learner neyman orthogonal doubly robust neural implementation valentyn melnychuk mihaela van der schaar munich center machine quantifying aleatoric treatment effect novel orthogonal learner arxiv org 114 979 561 understanding causal inference technique address causal inference challenge understanding neural mechanism research include optimization significance causal inference research understanding correlation causation career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 12,
    "keywords": "cookie, finance, machine, management, 08, com, app, project, intelligence, education"
  },
  {
    "filename": "a712640d6759a1748db7b1cd22782aaa.txt",
    "text": "08 app prashant seetharaman prashant seetharaman product engineer eda siemens digital industry software 6 variance discussed predictive distribution high epistemic modeler know much better job interesting exploration towardsdatascience predictive distribution uncertainty tell bayesian neural network towardsdatascience com 8 5 725 42 6 context engineering prashant seetharaman 2 powered eda pitch tactic chip tactic prashant seetharaman 2 agent prashant seetharaman 6 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 12,
    "keywords": "cookie, finance, machine, management, 08, com, app, project, intelligence, education"
  },
  {
    "filename": "a806bc8d6b64b729aa052ff20edeada2.txt",
    "text": "08 app vadim smolyakov vadim smolyakov machine engineer microsoft ml developer mit 4 interested quantification check new video gaussian process gp regression gaussian process define distribution function posterior computed closed form observe application gp regression include 1 bayesian optimization hyperparameter tuning experimental design 2 time series forecasting financial market 3 spatial analysis geo statistic weather prediction 4 robotics trajectory planning rl link gaussian process regression 36 1 david stroud staff software engineer advanced large language operation springer september mlops llmops neurips 2023 4 love vadim 1 2 717 166 include optimization manage supply chain practical application generative application machine finance role probability analysis importance precision decision making career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 12,
    "keywords": "cookie, finance, machine, management, 08, com, app, project, intelligence, education"
  },
  {
    "filename": "b5dcc1556c78adde1707490cd7b6d81e.txt",
    "text": "08 app anurag singh anurag singh genai nlp cv backend 1 ever wonder machine quantify prediction high stake scenario like healthcare finance knowing confident important prediction day 15 machine interview question difference softmax sigmoid function would use answer softmax sigmoid function activation function used transform output probability applied different scenario sigmoid function range output value 0 1 formula sigmoid z 1 1 exp z use case sigmoid function typically used binary classification problem output need interpreted probability one two class e g 0 1 map output probability score making ideal logistic regression binary classification task softmax function range output value 0 1 class sum output probability equal 1 formula softmax z exp z exp z j class use case softmax used multi class classification problem want assign probability multiple class often used final activation function like neural network problem two class ensures predicted probability class sum 1 3 16 441 42 machine applied cardiology tip machine success optimize machine performance best use case evaluating medical application address overfitting machine career productivity finance soft skill emotional intelligence project management education",
    "cluster": 12,
    "keywords": "cookie, finance, machine, management, 08, com, app, project, intelligence, education"
  },
  {
    "filename": "b69157d2cfcfba5ec1ddce23807e2c4d.txt",
    "text": "08 app shagun singh shagun singh indian institute technology madras product management workflow designer iit madras bridging business building code workflow 4 probability future move beyond label boundary one thing clear probabilistic rising fast unlike traditional supervised unsupervised method density estimation predict understands generates imago chatgpt generative video autonomous system future lie handle learn fewer label generate new intelligent output entering world probability becomes powerful label takeaway probabilistic modeling another tool foundation future ready generativeai machinelearning datascience deeplearning futureofai probabilisticmodels 4 1 936 242 future direction innovation beyond language future video creation gpts change application generative update trend best use case top prediction future career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 12,
    "keywords": "cookie, finance, machine, management, 08, com, app, project, intelligence, education"
  },
  {
    "filename": "b7bb352dbf8d553b10613abd65238567.txt",
    "text": "08 app david stutz david stutz research scientist google deepmind 1 new pre print conformal credal set collaboration led alireza javanmardi eyke h llermeier nutshell assume first order multiple annotation per example allows u get approximate distribution label multiclass classification directly conformalize categorical distribution constructing credal set set distribution associated coverage guarantee corresponding credal set intuitively represent epistemic aleatoric shown chaosnli conformalized credal set predictor arxiv org 84 14 804 191 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 12,
    "keywords": "cookie, finance, machine, management, 08, com, app, project, intelligence, education"
  },
  {
    "filename": "bb2962ea804d09c3de192d6d375324f9.txt",
    "text": "08 app aayush sugandh aayush sugandh iit kgp synopsys jadavpur university 3 thing generally might heard ml novice epistemic v aleatoric uncertainity one signifying lack knowledge input output mapping one signifying inherent stocasticity mapping population risk loss actually want compute end computing empirical risk thus concept generalization gap well known probabilistic principal component analysis special case factor analysis linear framed observing high dimension output sample normal distribution mean dependent latent covariance matrix covariance identity take log tf idf formulation hint log function grows handling missing missing completely random mcar missing random mar missing random nmar nmar cannot ignore missingness certain imputation scheme take remedy 11 9 323 252 tip machine success optimize machine performance challenge faced ml engineer common mistake financial modeling common mistake hinder adoption challenge training neural career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 12,
    "keywords": "cookie, finance, machine, management, 08, com, app, project, intelligence, education"
  },
  {
    "filename": "dfa0501d8057dbf07217e82cfea64d55.txt",
    "text": "08 app jong min kim jong min kim professor statistic measurement journal editor chief 2 paper titled llm guided ensemble contextual bandit copula gaussian process officially accepted today work proposes novel contextual bandit framework combine copula modeling large language enhancing ensemble copula feature gp dynamic policy selection driven large language yield superior adaptability performance result highlight effectiveness combining structured probabilistic llm based guidance robust adaptive decision making skewed high variance environment download full paper accompanying r code available egade business school del tecnol gico de monterrey adjunct professor university minnesota science hub core member university minnesota morris 63 2 ajay verma assistant professor 2 many congratulation sir 1 26 358 216 llm process language evaluate language performance ensemble improves prediction innovation context length llm train custom language improve agent performance llm career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 12,
    "keywords": "cookie, finance, machine, management, 08, com, app, project, intelligence, education"
  },
  {
    "filename": "007b16cd618a4e1b0b9eaf4b288966f8.txt",
    "text": "08 app dr robert k bler dr robert k bler senior staff scientist bei aldi dx algorithmen science machine 3 get estimate neural network free given right loss function standard neural network output well machinelearning deeplearning artificialintelligence neuralnetworks bayes confidence get estimate neural network free 55 20 matias valdenegro toro assistant professor tenured machine university groningen quantifier 3 method called variance attenuation standard method present literature note estimate aleatoric regression recent paper see aleatoric epistemic estimated using loss another quantification method 8 airin dutta statistical self sensing state awareness autonomous vehicle driven real time health monitoring urban air mobility electric vehicle 3 great article clever math 1 marco huber full professor university stuttgart head digitalization machine vision fraunhofer ipa 3 nice article version mixture density network estimating parameter single gaussian density instead parameter entire gaussian mixture 2 5 347 302 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 13,
    "keywords": "cookie, neural, network, com, intelligence, energy, project, bayesian, finance, education"
  },
  {
    "filename": "17a75e12c1a232be88255e3508a9752f.txt",
    "text": "08 app ortensia amoroso ortensia amoroso 4 chapter estimation via ensemble deep dropout layer seismic trace published book artificial intelligence seismic interpretation modeling springer explore cnn ensemble dropout improve estimation robustness mislabeled seismic trace classification check collaboration giovanni messuti ferdinando napolitano mariarosaria falanga paolo capuano silvia scarpetta advanced neural artificial intelligence theory application link springer com 13 110 5 ensemble improves prediction latest development deep application deep breakthrough trend improve output quality impact accuracy include optimization career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 13,
    "keywords": "cookie, neural, network, com, intelligence, energy, project, bayesian, finance, education"
  },
  {
    "filename": "4985f6752cf19473f1e603fabe0c7261.txt",
    "text": "08 app laurits fredsgaard larsen laurits fredsgaard larsen phd student applied math computer science specializing bayesian neural network graph neural network postdoc position available team dtu compute machinelearning academicjobs postdoc jobopportunity mikkel schmidt associate professor technical university denmark passionate quantifying neural network join team dtu compute postdoc developing cutting edge bayesian neural network molecular discovery fully funded position dynamic environment apply postdoc machine quantification graph neural network dtu compute efzu fa em2 oraclecloud com 505 5 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 13,
    "keywords": "cookie, neural, network, com, intelligence, energy, project, bayesian, finance, education"
  },
  {
    "filename": "6033c3a3e74fbce38144d0b3a51a8362.txt",
    "text": "08 app helga jord helga jord assistant professor instituto superior cnico lisbon university mineral energy resource engineering department 1 excited share latest article using bayesian neural network assessment ore type boundary complex geological explored compared monte carlo dropout bayesian neural network assess deep convolutional neural network trained predict geological domain conditioned drill hole check using bayesian neural network assessment ore type boundary complex geological natural resource research link springer com 31 679 29 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 13,
    "keywords": "cookie, neural, network, com, intelligence, energy, project, bayesian, finance, education"
  },
  {
    "filename": "65fff493579309f37a1ad6be664eaf0f.txt",
    "text": "08 app gian antonio susto gian antonio susto co founder statwolf associate professor universit degli studi di padova 1 happy announce publication latest research ieee access paper adversarially robust fault zone prediction smart grid bayesian neural network tackle two major challenge smart grid fault prediction adversarial vulnerability false alarm using bayesian neural network bnns achieved high level accuracy developed based detection scheme successfully distinguishes normal adversarial framework offer promising approach bolstering smart grid security ensuring stability efficiency energy demand grow check full paper special thanks co author emad efatinasab alberto sinigaglia nahal azadi mirco rampazzo collaboration hard work proud contribute exciting research part amco lab unipd smartgrids bayesianneuralnetworks gridsecurity adversarialattacks ieeeaccess research energyefficiency 59 4 878 252 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 13,
    "keywords": "cookie, neural, network, com, intelligence, energy, project, bayesian, finance, education"
  },
  {
    "filename": "6eb98ca593cb0998ea51267392d2abf7.txt",
    "text": "08 app ramin bostanabad ramin bostanabad associate professor uc irvine 2 fusion play important role wide rang application material design climate modeling recent work develop opensource framework leverage bayesian neural network probabilistic fusion kudos student worked project specially proud likith high school student purdue worked u year learn machinelearning able fundamentally contribute work link paper see paper link opensource code datafusion multifidelity please wait 79 9 643 28 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 13,
    "keywords": "cookie, neural, network, com, intelligence, energy, project, bayesian, finance, education"
  },
  {
    "filename": "716ebb50a5ba14073bba451e40fed730.txt",
    "text": "08 app phuong dao phuong dao incoming assistant professor university texas austin 6 new preprint plant trait retrieval check new preprint posted egusphere assessment deep based plant trait retrieval hyperspectral study introduces distance based estimation method quantifying prediction deeplearning based plant trait retrieval hyperspectral proposed framework improves reliability estimation vegetation monitoring offer promising approach broader application great work eya cherif leipzig university link 64 3 732 58 tip growing include optimization embrace professional growth challenge benefit deep value acknowledging leadership foundation remote sensing application career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 13,
    "keywords": "cookie, neural, network, com, intelligence, energy, project, bayesian, finance, education"
  },
  {
    "filename": "920197886a713011a5274f61cb010815.txt",
    "text": "08 app kristoffer wickstr kristoffer wickstr associate professor machine uit arctic university norway 7 finished poster presentation aaai paper repeat improving estimation representation explainability new method quantification xai presented work done together colleague robert jenssen michael kampffmeyer also guest researcher time thea br sch dtu compute 81 2 dtu compute 7 great hear kristoffer wickstr thanks sharing dkforsk machinelearning 1 thea br sch phd student deep explainability biomedical time series 7 great job kristoffer looking forward hearing doubt interesting conversation people poster 1 1 483 39 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 13,
    "keywords": "cookie, neural, network, com, intelligence, energy, project, bayesian, finance, education"
  },
  {
    "filename": "980fcd68b8dffc05a5307b827ac1a9c7.txt",
    "text": "08 app julija zavadlav julija zavadlav gregor hner sc aerospace phd candidate tfd tum 2 happy announce latest publication collaborating published journal chemical theory computation started semester thesis supervision stephan thaler julija zavadlav turned interesting publication analyzing method scalable bayesian quantification neural network potential looking forward future work field neural network modelling quantification tum neuralnetworks publication moleculardynamics scalable bayesian quantification neural network potential promise pitfall pub ac org 48 1 1 533 29 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 13,
    "keywords": "cookie, neural, network, com, intelligence, energy, project, bayesian, finance, education"
  },
  {
    "filename": "a0b9492ea0d1a824368b7433dabb3870.txt",
    "text": "08 app jerry yan jerry yan chair professor editor chief nexus applied energy advance applied energy 11 energy efficient based control semi closed greenhouse leveraging robust optimization deep reinforcement aibasedcontrol greenhouse optimization deepreinforcementlearning 27 12 143 3000 energy saving processing option understanding environmental impact energy water usage improve energy use mitigate climate change risk powered energy solution changing physical environment career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 13,
    "keywords": "cookie, neural, network, com, intelligence, energy, project, bayesian, finance, education"
  },
  {
    "filename": "b11a48da772c1177834765c99d53687e.txt",
    "text": "08 app dtu compute dtu compute 8 652 1 dtu compute happy welcome another phd student laurits fredsgaard larsen section cognitive system project titled bayesian neural network molecular discovery developing new method aware gnns compromise performance feasibility serve oracle scientist guiding towards promising molecule drug discovery moreover flag area prediction uncertain inviting experimental verification essence new create feedback loop research informs turn informs research making whole process drug discovery efficient reliable info link phd dtucompute research cogsys bayesian neuralnetworks bayesian neural network molecular discovery orbit dtu dk 34 1 david ribberholt ipsen machine engineer algorithmic energy trading 1 fedt laurits fredsgaard larsen 1 8 652 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 13,
    "keywords": "cookie, neural, network, com, intelligence, energy, project, bayesian, finance, education"
  },
  {
    "filename": "b5304b5ef208ebe2efbf7c75f21160e7.txt",
    "text": "08 app tejs vegge tejs vegge professor dtu energy director pioneer center capex co founder phasetree 1 great work jonas busk peter bj rn j rgensen jonas busk phd scientific software developer dtu energy 1 happy share new paper quantification calibration graph neural network published pccp graph neural network interatomic potential ensemble calibrated aleatoric epistemic energy force pub rsc org 20 2 872 227 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 13,
    "keywords": "cookie, neural, network, com, intelligence, energy, project, bayesian, finance, education"
  },
  {
    "filename": "ca02f11482f289574d40bfd1a081030f.txt",
    "text": "08 app william green william green director mit energy initiative hoyt c hottel professor mit dept chemical engineering co founder thiozen 1 nice work hao wei accuracy improvement predicting radical thermochemistry sure improve predictive kinetics help speed energy transition hao wei pang senior scientist cheminformatics ml mit ph 1 exciting news latest research paper subgraph isomorphic decision tree predict radical thermochemistry bounded estimation published journal physical chemistry compiled dataset thermochemical parameter 2210 radical derived quantum chemical calculation extended subgraph isomorphic decision tree sidt machine algorithm estimate hydrogen bond increment correction sidt estimator mark leap kinetic modeling providing interpretable reliable scalable framework predicting radical thermochemistry deeply grateful collaboration insight co author xiaorui dong matt johnson william green interested diving deeper finding methodology read full publication subgraph isomorphic decision tree predict radical thermochemistry bounded estimation pub ac org 36 1 naresh k mallenahalli intrapreneur fiete smieee smacm lmisrs lmnhrdn 1 congratulation 4 052 87 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 13,
    "keywords": "cookie, neural, network, com, intelligence, energy, project, bayesian, finance, education"
  },
  {
    "filename": "cf73c3df5ac0bd869495f1d527857be2.txt",
    "text": "08 app ghifari adam faza ghifari adam faza phd researcher ku leuven 1 pleased share latest work published knowledge based system likely heteroscedastic gaussian process via kernel smoothing paper present method heteroscedastic gaussian process regression reduces computational cost requiring one gp instead two estimating noise level kernel smoothing approach lower complexity 2n n n resulting roughly 2 faster training experiment also observe improved stable performance several test case including bayesian optimisation task many thanks co author nasrulloh loka han hallez keivan shariatmadar phd msc bsc david moens contribution collaboration likely heteroscedastic gaussian process via kernel smoothing sciencedirect com 75 1 205 41 optimization technique artificial intelligence strategy optimizing best technique high performance computing optimize machine performance multi gpu parallelism technique improve performance new technique career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 13,
    "keywords": "cookie, neural, network, com, intelligence, energy, project, bayesian, finance, education"
  },
  {
    "filename": "d58e6c1c81b2590088fd8ba3c2317aee.txt",
    "text": "08 app sunghyun kim sunghyun kim ph candidate department architecture architectural engineering seoul national university 5 hybrid better hvac system modeling pleased share another recent publication energy building hybrid chiller using transfer aleatoric epistemic study present hybrid framework chiller performance prediction integrating synthetic specification based real operational transfer key contribution include combining physic based consistency driven adaptability enhancing extrapolation reliability unseen operating condition quantifying aleatoric epistemic via bayesian deep result show proposed achieves comparable accuracy significantly reduced making practical solution application like predictive control mpc real time operation variable condition sincere thanks prof cheol soo park co author jinhong kim youngsub kim seon young heo great collaboration full paper available hybridai transferlearning chiller modeling energyefficiency hvac buildingsimulation energyandbuildings 25 2 cheol soo park full professor department architecture architectural engineering seoul national university 4 thanks sharing sunghyun 146 3 cooling technology advancing importance liquid cooling system improve energy use sustainable cooling strategy center impact renewable energy affect climate change career productivity finance soft skill emotional intelligence project management education",
    "cluster": 13,
    "keywords": "cookie, neural, network, com, intelligence, energy, project, bayesian, finance, education"
  },
  {
    "filename": "0a03edf5c9cfee1d84fa9bef146682d9.txt",
    "text": "08 app jayaraman thiagarajan jayaraman thiagarajan generative researcher 3 two paper personally excited sometime finally accepted neurips2022 thanks collaborator persisting bringing exciting work light airesearch deeplearning contrastivelearning uq mlsafety graph 1 delta uq new principle estimation exploit lack shift invariance ntks delta uq effective ood generalization anomaly rejection sequential optimization paper rushil anirudh vivek sivaraman narayanaswamy peer timo bremer 2 take deep look graph contrastive find centric property known crucial success visual cl fail hold graph cl paper puja trivedi mark heimann danai koutra ekdeep lubana 88 13 joshua feinglass ph researcher 3 congrats lot exciting work insight coming collaboration nirmalkumar seshachalam digital solution architect cloud digital experience martech certified google genai leader 3 want talk jt would nice hear principle subramanian kl application development manager ups india technology center driving strategic technical delivery program execution 3 congratulation 1 saravana kumar 3 congrats guru 1 madhusudan shekar cto speaker 3 congrats 1 angela sheffield transforming industry government agency serve defense artificial intelligence nuclear matter expert former super er 3 congratulation jayaraman wait read thanks sharing keep awesome work 1 sandeep madireddy computer scientist researcher argonne national laboratory 3 nice work congra",
    "cluster": 14,
    "keywords": "congratulation, phd, research, estimation, work, paper, university, app, engineering, 08"
  },
  {
    "filename": "21951fcd62ee3c04d8078cda65ad05e0.txt",
    "text": "08 app milica gasic milica gasic professor university sseldorf chair dialog system machine 1 carel van niekerk successfully defended thesis titled estimation management utilisation human computer dialogue picture protesting christopher manning ended hat instead 73 6 xiaochen zhu student university cambridge 1 congratulation aadil cajee fassa 1 carel van niekerk knew day met congratulation mate jarod smith phd senior scientist machine engineer sub product owner bmw hub south africa 1 congrats carel abdallah bashir member technical staff contextual nlp generative 1 congratulation carel van niekerk 1 huge congratulation leon mlodzian consultant fine 1 congratulation carel 775 36 3 newcomer know dialogue milica gasic 5 two phd position dialogue modelling dialog system machine group milica gasic 6 hiring milica gasic 9 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 14,
    "keywords": "congratulation, phd, research, estimation, work, paper, university, app, engineering, 08"
  },
  {
    "filename": "2c483bf4631cc5d29d66bce865f8ba70.txt",
    "text": "08 app david stutz david stutz research scientist google deepmind 3 last week finally defended phd max planck institute informatics universit de saarlandes saarland informatics campus got great hat lot adversarial example deepmind reference colleague stay tuned whole thesis covering work adversarial robustness estimation ood detection conformal prediction bit error robustness deep accelerator get slightly outdated short overview special thanks advisor bernt schiele matthias hein university tuebingen well committee pawan kumar mario fritz eddy ilg jan eric lenssen also pleasure work great collaborator nandhini chandramoorthy ibm research ali taylan cemgil arnaud doucet krishnamurthy dvijotham deepmind google 272 16 amanda bower machine scientist passion responsible ml 3 look way fun graduation congrats linnea herman software engineer bloomberg prev google amazon 3 congrats great achievement maybe see 6ps sometime martin randler co founder transform technology better 3 herzlichen gl ckwunsch john stephenson director logikk u deep computer vision 3 congratulation david david stutz research scientist google deepmind 3 thanks everyone benedict hadjistojanov partner kastell personalberatung 3 stark alles gute 1 vedika agarwal product google researcher turned engineer turned pm ex grab tomtom 3 congratulation david stutz onwards upwards 1 dr julian steil projektmanager bei robert bosch gmbh 3 congratulation 1 stephanie manuela winkler program manager de",
    "cluster": 14,
    "keywords": "congratulation, phd, research, estimation, work, paper, university, app, engineering, 08"
  },
  {
    "filename": "36a7ca2470443318bcbd19ac8e94f925.txt",
    "text": "08 app ravi kanth sriwastav phd ravi kanth sriwastav phd structural engineer seismic hazard risk assessment researcher 2 happy share latest publication led dr jyothi yedulla co authored anuj choubey kammula sai tulasi prof raghukanth stg title bayesian neural network based ground motion horizontal v h spectral ordinate epistemic european region artificial neural network anns revolutionized development non parametric ground motion often overlook epistemic providing deterministic point estimate study explored power bayesian neural network bnns probabilistic predict median aleatory well epistemic matter better modeling seismic hazard analysis improved reliability ground motion prediction performance based design step toward probabilistic driven earthquake engineering earthquakeengineering machinelearning bayesianneuralnetworks groundmotionmodels seismichazard artificialintelligence structuralengineering researchpublication bnn ann gmm 57 1 dr bonisha borah 2 congratulation 1 1 944 31 latest trend machine improves hurricane prediction understand neural network llm update new release optimize machine performance current engineering approach trend career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 14,
    "keywords": "congratulation, phd, research, estimation, work, paper, university, app, engineering, 08"
  },
  {
    "filename": "3ca225ef4d4af995ace34101fd3d3e56.txt",
    "text": "08 app lahiru nagasingha lahiru nagasingha ph candidate mineral engineering 1 delighted share second publication phd research published ore geology review generative neural network approach risk return analysis mineral prospectivity modelling first study tackled imbalance mineral prospectivity modelling using conditional variational autoencoder cvae improved predictive accuracy address key challenge understanding risk behind prediction new paper build foundation introducing cvae based framework capture epistemic aleatoric uncertainty integrates novel risk return framework us measure getis ord g spatial clustering approach categorize target four risk return group result show high return zone occupy 4 canadian land area yet contain nearly 95 known deposit also highlighting newly identified prospective zone align within four risk return category huge thank prof charles l b rub dr reza ghanati invaluable guidance support throughout research read full paper excited continue pushing improve mineral exploration delivering prediction accurate transparent truly useful decision making generative neural network approach risk return analysis mineral prospectivity modelling sciencedirect com 58 16 arosh perera business manufacturing excellence specialist lssbb msc ait mba pim manager manufacturing excellence cbl food international pvt ltd 1 congratulation bro 1 prabhani ranaweera project engineer tfnsw phd cpeng ner mieaust 1 congrats lahiru",
    "cluster": 14,
    "keywords": "congratulation, phd, research, estimation, work, paper, university, app, engineering, 08"
  },
  {
    "filename": "4209c12c76e54a587cccac3856de84ff.txt",
    "text": "08 app ermiyas birihanu belachew phd ermiyas birihanu belachew phd aws ml engineer phd computer science researcher e tv lor nd university assistant professor computer science science engineering 8 happy share recent research paper published pisa italy discovery science lncs paper focus proposing enhancing real time anomaly detection framework measuring estimation deep tailored industrial control system ic work address critical challenge ensuring reliability security ic find paper research deeplearning industrialcontrolsystems anomalydetection uncertaintyestimation realtime enhancing industrial control system security real time anomaly detection estimation link springer com 35 6 nega wake hundera ph postdoctoral researcher uestc phd public key cryptography post quantum cryptography federated secure communication protocol 7 congratulation dr ermi 1 nega wake hundera ph postdoctoral researcher uestc phd public key cryptography post quantum cryptography federated secure communication protocol 8 congratulation 1 ali abosinnee phd candidate computer vision engineer analyst machine developer 8 congratulation brother 1 adolf kamuzora phd student 7 congratulation ermiyas 1 loubna seddiki phd student science 8 congratulation 1 endalkachew k mekonen phd student economics university messina italy 8 congratulation keep good work 1 1 037 14 anomaly detection method insight recent research deep breakthrough trend latest deve",
    "cluster": 14,
    "keywords": "congratulation, phd, research, estimation, work, paper, university, app, engineering, 08"
  },
  {
    "filename": "70013384465e9847535206127ced5e0c.txt",
    "text": "08 app natalie memarsadeghi natalie memarsadeghi research physical scientist 1 excited share latest publication journal hydroinformatics team developed hybrid machine physic based modeling framework improve streamflow prediction ungauged river integrating quantification better inform risk based decision making applied river basin colorado approach reduced prediction error approximately 40 compared purely driven also providing credible bound essential flood management water resource planning work result multi year collaborative effort hottinger bruel kjaer solution university michigan university california san diego erdc chl building previous research hydrologic modeling ml approach hybrid modeling streamflow prediction ungauged river quantification comparative analysis open access iwaponline com 55 klohn crippen berger 38 338 2 today presenting final kcb session 52nd congress international association hydrogeologists lead hydrogeologist damian lee discussing leveraging hole geophysical hydrogeologic characterization following session senior hydrogeologist claire kent presenting embracing digital hydrogeology future practical application business intelligence bi software python efficient analysis visualisation groundwater monitoring co authored hydrogeological engineer emily mee lastly hydrogeologist kevin de bruin showcasing poster presentation navigating hydrogeological challenge remote geothermally active tropical island today tomorrow 52nd congress international association",
    "cluster": 14,
    "keywords": "congratulation, phd, research, estimation, work, paper, university, app, engineering, 08"
  },
  {
    "filename": "886393534a431c73843c73ae1c421b45.txt",
    "text": "08 app ethan jones ethan jones solution microsoft technical community leader co founder 2 monday fortunate enough celebrate end 4 year university sheffield finished first class honour computer science chapter life concluded one full self development many life long friend memory made final year pleasure working alongside haiping lu lawrence schobs dissertation project explored epistemic aleatoric estimation medical landmark localisation task privilege contribute state art research important topic responsible artificial intelligence alongside truly inspiration people completion degree also signal end near 4 year sheffield solar year involved multitude different project ranging group near real time historical api photovoltaic generation gb transmission network contributing journal article open dataset solar installation across country huge thank go line manager jamie taylor first trusting microgen project year ago sheffield blast 144 56 jessica millar support worker sheffield child looked service cla 2 congratulation ethan 1 sureena gill customer success account manager microsoft 2 congratulation ethan zion oyemade product manager arrow ec united kingdom cloud protection 2 congrats ethan 1 matt watt year industry placement officer faculty engineering university sheffield 2 congratulation ethan thank help placement event wishing best future endeavour",
    "cluster": 14,
    "keywords": "congratulation, phd, research, estimation, work, paper, university, app, engineering, 08"
  },
  {
    "filename": "9285be88376cfad9114ba999a84b906f.txt",
    "text": "08 app olivier l de weck olivier l de weck apollo program professor astronautics engineering system mit associate department head mit aero astro editor chief journal spacecraft rocket 2 congratulation june stenzel successfully defending phd thesis yesterday mitaeroastro thesis break new ground quantification uq management particularly complex space program instrumentation epistemic aleatoric present main outcome work framework optimizing testing plan v v strategy program three case study including ccd qualification llama instrument jwst jitter control turn subset parameter system strongly influence behavior quantity interest bayesian inference approach joint use system performance experimental novel used optimize test plan budgetary constraint vice versa quantify testing budget given desired level residual additional use thesis prioritization test facility yield highest utility come testing reduction june interned jet propulsion laboratory jpl great doctoral committee one best phd defense seen believe work quite influential field rebecca masterson youssef marzouk david miller amy braverman 191 14 june stenzel nasa space technology research fellow mit 2 thank word support oli thank mentorship guidance 1 mich lin nasa phd candidate mit aeroastro design austere environment 2 congratulation june 2 ba ak yazgan enel msc pmp cryo em program execution manager 1 loo",
    "cluster": 14,
    "keywords": "congratulation, phd, research, estimation, work, paper, university, app, engineering, 08"
  },
  {
    "filename": "936708aa70858a136d602d5b7bbd1fdc.txt",
    "text": "08 app saptarshi da saptarshi da ackley professor engineering penn state university 2 latest contribution group bayesian computing based 2d material device congratulation amritanand sebastian executed work single handedly congratulation material grower nick trainor prof joan redwing pennstate pennstateesm pennstateengineering 2dmaterials device two dimensional material based probabilistic synapsis reconfigurable neuron measuring inference using bayesian neural network nature communication nature com 203 4 ehsanul 2 congrats eknath sarkar ph ece georgia tech 2 congrats prof da nikhil nath research assistant ra red green research centre bangladesh 2 congratulation professor rohit john semiconductor technology development process engineering material science electronics engineering asm ethz ntu 2 congratulation prof 14 203 155 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 14,
    "keywords": "congratulation, phd, research, estimation, work, paper, university, app, engineering, 08"
  },
  {
    "filename": "9719a48784f5357da7127730b90779e4.txt",
    "text": "08 app michael kirchhof michael kirchhof research scientist apple quantification 1 phd thesis online reveals happened paper scientifically anecdotally perspective gained future estimation importantly penguin 260 12 mohamed doctoral student university bonn quantification explainable machine crop monitoring 1 congrats shuman p ph student computer science simon fraser university 1 congrats amazing work michael really enjoyed paper looking forward reading thesis lawrence schobs phd machine computer vision medical imaging estimation 1 interesting thesis enjoying reading carl eric pehme junior research fellow tallinn university 1 congratulation absolutely wonderful work matthias de lange phd senior researcher techwolf phd adaptive machine ex meta 1 really interesting work read hritam basak 2x applied scientist intern amazon c phd student stony brook ny 3x cvpr 1x eccv 1x neurips 4x miccai 1x icassp 1 congratulation great work 3 457 45 1 never create dataset alone michael kirchhof 4 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 14,
    "keywords": "congratulation, phd, research, estimation, work, paper, university, app, engineering, 08"
  },
  {
    "filename": "b7adcf2fd4e622de02d49ca9ab28a8c3.txt",
    "text": "08 app bertrand charpentier bertrand charpentier founder president chief scientist pruna making faster cheaper smaller greener ph machine tum graduate school 1 recently successfully defended phd thesis estimation independent non independent viva manuscript available online check want learn estimation image tabular graph sequential estimation critical practical theoretical reason intro chapter present motivation concept terminology key quickly jump field thesis discus desideratum metric estimation different input output type found real world estimation classification ch 3 estimation regression ch 4 practicality estimation ch 5 robustness estimation ch 6 estimation graph ch 8 estimation sequential ch 9 estimation reinforcement ch chapter accompanied retrospective promising future direction estimation ml hope people enjoy thanks lot supervisor pr stephan g nnemann examiner pr eyke h llermeier pr eric nalisnick coauthor everyone supported research would possible without 214 5 nicholas clarke enablement director leading strategy engineering innovation regulated enterprise transformation 1 congratulation thanks sharing immense work 1 ranganathan rajkumar global executive digital transformation leader genai strategy multi cloud p l owner 1 thanks share",
    "cluster": 14,
    "keywords": "congratulation, phd, research, estimation, work, paper, university, app, engineering, 08"
  },
  {
    "filename": "c17a7a446ccdd3dbe45cf90643881108.txt",
    "text": "08 app soumya dutta ph soumya dutta ph cse professor iit kanpur past lanl scientist ohio state phd 1 excited share new research paper titled aware deep neural representation visual analysis vector field accepted presentation main track ieee visualization conference 2024 florida usa paper published special issue top visualization journal ieee transaction visualization computer graphic ieee tvcg work delf quantification visual interpretation vector field compressive deep used reconstruct vector datasets study different method estimation technique deep neural network prediction epistemic help better interpreting vector field feature streamlines critical point extracted predicted vector field making result explainable reliable trustworthy special thanks student atul kumar siddharth garg dedication hard work preprint available early access paper ieeexplore deeplearning vectorfield deeplearninguncertainty visualization montecarlodropout deepensemble uncertaintyvisualization 120 4 ayan biswas staff research scientist 3 los alamo national laboratory 1 congratulation 1 pranjal srivastava machine google cse iitk ex isro 1 amazing congratulation atul kumar soumya sir 2 komal yadav sr sde full stack genai automation engr sol architect team lead genai sol rag prompt eng qa automation apps tech iit kanpur cse lecturer full scholarship ug pg acad excellence b tech 16 17 1 congrats ieee visualization conference acceptance soum",
    "cluster": 14,
    "keywords": "congratulation, phd, research, estimation, work, paper, university, app, engineering, 08"
  },
  {
    "filename": "d18dd824758db0288ccd89754d418419.txt",
    "text": "08 app mohamad kharseh mohamad kharseh associate professor sustainable renewable energy american university ra al khaimah uae 1 excited share latest research paper titled artificial neural network concrete strength prediction based ultrasonic pulse velocity measurement published material journal paper propose two mathematical using nonlinear regression artificial neural network anns accurately predict compressive strength concrete based ultrasonic pulse velocity upv measurement demonstrated exceptional accuracy reliability high coefficient determination low standard error read full paper research concrete machinelearning artificialintelligence engineering aurak uae ann international concrete abstract portal concrete org 16 452 159 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 14,
    "keywords": "congratulation, phd, research, estimation, work, paper, university, app, engineering, 08"
  },
  {
    "filename": "d746b7bef77abbbbcf90db6fcc2352bd.txt",
    "text": "08 app kshitij sirohi kshitij sirohi computer vision deep scientist autonomous driving estimation 1 hoorray start icra2024 yokohama paper titled uplam robust panoptic localization mapping leveraging perception uncertainty best paper award back future robot go probabilistic workshop icra2024 could possible without awesome ever supporting co author daniel b scher wolfram burgard also many thanks workshop organizer putting effort bringing together many cool work field interested work please check paper 338 34 isa jahnke founding vice president academic international affair professor technology 1 congratulation 1 xiaoli wang student national university singapore 1 nice work talk 2 chenguang huang phd student university freiburg 1 congrats superstar kshitij 1 rohit menon full stack roboticist researcher phd candidate 1 congrats 1 eduardo alvarado postdoc max planck institute informatics virtual avatar human motion researcher 1 congratulation kshitij sirohi 1 sindhu singh embedded system engineer expertise automotive system software integration industrial robotics sensor system additionally skill machine simulation numerical optimisation 1 congratulation 1 himanshu lead engineer senior qualcomm chipset power system engineer 1 congratulation 1 ripan deep trainee decision scientist mu sigma inc 1 congratulation kshitij 1 nipun dhananjaya chief engineer robotics sintez llc roboticist 1 congratulation kshitij 1 1 942",
    "cluster": 14,
    "keywords": "congratulation, phd, research, estimation, work, paper, university, app, engineering, 08"
  },
  {
    "filename": "e105835f166a2e11e8df9a80d5c06f46.txt",
    "text": "08 app georgia papacharalampous georgia papacharalampous engineer working mainly intersection geoscience statistical modelling focus machine algorithm combination physic based predictive estimation 6 happy share new paper introduce deep huber quantile regression network hristos tyralis nilay dogulu kwok sun chun deeplearning machinelearning predictiveuncertainty 59 4 francesco granata associate professor hydraulic engineering universit degli studi di cassino e del lazio meridionale 6 congratulation interesting 1 salim heddam professeur top 2 world scientist stanford university 2020 2021 2022 2023 2024 outstanding senior researcher algerian scopus award 2022 6 j adore 1 2 409 162 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 14,
    "keywords": "congratulation, phd, research, estimation, work, paper, university, app, engineering, 08"
  },
  {
    "filename": "e73c15a79c193724a29ddcabaaba421b.txt",
    "text": "08 app agnimitra sengupta agnimitra sengupta assistant professor civil engineering 1 announcing latest publication transportation research part c emerging technology journal thanks amazing collaborator penn state university transportation engineering lab dr ilgin guler adway da dr sudeepta mondal contribution traffic prediction often struggle adapt distribution datasets due limited generalizability recent paper introduces bayesian recurrent neural network framework quantification traffic prediction enhanced generalizability introducing spectral normalization hidden layer controlling complexity mitigating overfitting method improves performance even limited training additionally prediction interval empower traffic management system make real time informed decision address congestion effectively innovation present significant advancement traffic prediction offering practical solution real world mobility challenge bayesian approach quantifying uncertainty improving generalizability traffic prediction sciencedirect com 64 1 subir ghosh doctoral student penn state university 1 congratulation bhai 1 880 52 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 14,
    "keywords": "congratulation, phd, research, estimation, work, paper, university, app, engineering, 08"
  },
  {
    "filename": "fe457ee51b6a17558b97e4be2838fc0a.txt",
    "text": "08 app mohamed noureldin mohamed noureldin professor structural engineering application 5 new publication driven modeling concrete durability predicting long term durability concrete structure remains key challenge civil engineering particularly due complex uncertain nature aggressive ion ingres e g chloride sulfate modeling material science perspective accounting maintaining reliability persistent gap current method recent study published construction building material present driven framework address challenge integrating generative adversarial network gans dataset augmentation bayesian neural network bnns aware prediction read full article hope work contributes growing field infrastructure resilience open opportunity integrating generative probabilistic material durability research concretedurability aiinengineering generativeai bayesianneuralnetworks civilengineering infrastructuremonitoring mlformaterials structuralhealthmonitoring uncertaintyquantification based framework concrete durability assessment using generative adversarial network bayesian neural network sciencedirect com 73 6 engr irfan ullah meticulous civil engineer researcher reviewer machine deep generative bridge structural health monitoring 5 congratulation professor khaled allam pmp rmp vma 6sigma tqm pmp rmp value engineering structural engineering consultant project manager design manager khatib alami 5 musitefa adem yimer senior structural engineer researcher p e aci asce sc b",
    "cluster": 14,
    "keywords": "congratulation, phd, research, estimation, work, paper, university, app, engineering, 08"
  },
  {
    "filename": "07e07876d62c98948919a90d2874b250.txt",
    "text": "08 app sanbao su sanbao su sde google pre sde huawei pre intern amazon google bosch cadence 2 thrilled announce paper quantification collaborative detection self driving accepted international conference robotics automation icra 2023 huge achievement team excited share research community thanks icra review committee recognizing importance work looking forward presenting icra2023 london multi agent collaborative object detection proposed leverage viewpoint agent improve detection accuracy compared individual viewpoint however connected autonomous vehicle cavs may still uncertainty object detection due distribution object sensor measurement noise poor weather work propose novel quantification method called double quantification estimate collaborative object detection method capture epistemic aleatoric one inference based offline double training process used different collaborative object detector experiment v2x sim set show method achieve great improvement score accuracy score compared state art quantification website paper github v2x sim 16 1 yao duan ph application scientist semiconductor nanophotonics 2 congratulation 575 career productivity finance soft skill emotional intelligence project management education cookie",
    "cluster": 15,
    "keywords": "estimation, research, prediction, phd, deep, paper, work, app, 08, detection"
  },
  {
    "filename": "123132f6d60f6c4a2252ccd1c0a739a0.txt",
    "text": "08 app marija popovi marija popovi assistant professor tu delft 2 team going iros 2023 detroit check paper neu nbv next best view planning using estimation image based neural rendering liren jin xieyuanli chen julius r ckin multi uav adaptive path planning using deep reinforcement jonas westheider julius r ckin graph based view motion planning fruit detection tobias zaenker julius r ckin rohit menon maren bennewitz see detroit neu nbv next best view planning using estimation image based neural rendering arxiv org 191 4 toni rosinol co founder ceo stackai mit phd always hiring 2 see marija 1 dmitrii dobriborsci phd engineering professor autonomous robotics intelligent robotics lab head 2 impressive 1 tianqi l researcher 2 liren jin glad listen paper person iros also presenting active sensing paper let get connected 2 4 431 220 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 15,
    "keywords": "estimation, research, prediction, phd, deep, paper, work, app, 08, detection"
  },
  {
    "filename": "2d3d151c7006989b3a211fe5a6877582.txt",
    "text": "08 app hanieh shojaei research assistant phd candidate leibniz university hannover 2 last week opportunity present research estimation lidar scene perception phd colloquium dgk geoinfo bonn shared use gaussian mixture gmms epistemic detect distribution sample addition presented approach quantifying aleatoric classifying lidar point cloud gaining valuable feedback future improvement short video showcase bird eye view bev lidar scene interpretation left side show prediction different object visualized distinct color example street highlighted pink right side display map reddish area indicate high truly inspiring connect researcher intersection ml geoscience learn keynote prof jan dirk wegner dr jens behley enjoy many insightful discussion big thank jan henrik haunert hosting colloquium bonn prof monika sester prof claus brenner continuous support research institut f r kartographie und geoinformatik ikg phdcolloquium dgkgeoinfo lidar uncertaintyestimation machinelearning geoscience epistemic aleartoric 146 1 katayoon yazdan panah ph management computer expert 1 1 jacob hirschberg gautschi senior research assistent chair engineering geology eth zurich 1 excited share latest publication ieee transaction geoscience remote sensing deep based object detection tracking debris flow 3 lidar camera fusion collecting high resolution moving debris flow essential better understanding physic",
    "cluster": 15,
    "keywords": "estimation, research, prediction, phd, deep, paper, work, app, 08, detection"
  },
  {
    "filename": "32902304c81e1265b2c34b50e929abba.txt",
    "text": "08 app kshitij sirohi computer vision deep scientist autonomous driving estimation 3 excited share latest work aware panoptic segmentation work aim enable perception system provide holistic scene understanding together estimate related prediction method try optimize performance maximize segmentation quality evaluate network estimation quality however safe operation system real world crucial consider prediction well work introduce novel task aware panoptic segmentation aim predict per pixel semantic instance segmentation together per pixel estimate define two novel metric facilitate quantitative analysis aware panoptic quality upq panoptic expected calibration error pece propose novel top evidential panoptic segmentation network evpsnet solve task architecture employ simple yet effective probabilistic fusion module leverage predicted uncertainty additionally propose new lovasz evidential loss function optimize iou segmentation utilizing probability provided deep evidential furthermore provide several strong baseline combining state art panoptic segmentation network sampling free estimation technique extensive evaluation show evpsnet achieves state art aware panoptic metric w mohammad sajad marvi daniel buescher wolfram burgard paper code autonomousvehicles autonomousdriving robotics computervision 139 14 tim schulte software developer spe",
    "cluster": 15,
    "keywords": "estimation, research, prediction, phd, deep, paper, work, app, 08, detection"
  },
  {
    "filename": "369cb7f37327b160be0799b5dc241f27.txt",
    "text": "08 app vasileios belagiannis vasileios belagiannis machine deep computer vision fau 2 pixel wise depth prediction reliable check new iccv2023 paper post hoc distribution detection follow work julia h last year eccv2022 paper estimation monocular depth estimation code released soon deeplearning machinelearning computervision fau uulm fau erlangen n rnberg faculty science fau erlangen n rnberg ulm university julia h 2 happy share paper titled distribution detection monocular depth estimation got accepted iccv2023 conference link pre print arxiv object environment called distribution ood impact reliability neural network prediction ood part due missing knowledge classification active research area however must still explored complex task monocular depth estimation therefore introduce post hoc ood detection approach already trained depth estimation train image decoder using feature trained depth encoder distribution inspired anomaly detection ood detected using image reconstruction current approach address evaluation build extensive evaluation protocol ood detection standard depth estimation benchmark kitti nyu depth v2 distribution various ood datasets approach demonstrates astonishing performance compared estimation method thanks co author adrian h vasileios belagiannis support 27 1 912 32",
    "cluster": 15,
    "keywords": "estimation, research, prediction, phd, deep, paper, work, app, 08, detection"
  },
  {
    "filename": "44b261eb80607a258ea0ecce4700d5f8.txt",
    "text": "08 app nikita durasov phd nikita durasov phd research scientist vlm semantic search nvidia excited share internship project nvidia 3 internship developed framework 3 leveraging evidential improve reliability robustness critical application like autonomous vehicle robotics solution seamlessly integrates bird eye view bev representation addressing key challenge detecting distribution ood scene identifying erroneous bounding box highlighting missed detection importantly method making ideal real world deployment key achievement 20 improvement baseline detecting ood scene localization error integration fully driven auto labeling framework enhancing detection metric remaining computationally efficient check paper project page insight preprint project page big thank amazing collaborator nvidia epfl including rafid mahmood jiwoong choi marc law james lucas pascal fua jos lvarez nvidia 3dobjectdetection uncertaintyestimation evidentiallearning autonomousdriving robotics 69 6 241 29 include optimization object detection segmentation computer vision nvidia growing influence vision technology improves robotics tip navigating induced team innovation depth estimation technique career productivity finance soft skill emotional intelligence project management educatio",
    "cluster": 15,
    "keywords": "estimation, research, prediction, phd, deep, paper, work, app, 08, detection"
  },
  {
    "filename": "4c1bd10a7ec6f07407c3236e30b4d213.txt",
    "text": "08 app kshitij sirohi kshitij sirohi computer vision deep scientist autonomous driving estimation 2 international telecommunication union united nation specialized agency information communication technology organizing free webinar digital transformation mobility paving way road safety giving talk importance safe perception system autonomous driving together glimpse research work towards registration fee interested sign looking forward interact event link research webinar autonomousvehicles autonomousdriving perception safedriving 42 1 942 11 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 15,
    "keywords": "estimation, research, prediction, phd, deep, paper, work, app, 08, detection"
  },
  {
    "filename": "5d2f9ffbe71d4c4663e6998e5192ea63.txt",
    "text": "08 app vasileios belagiannis vasileios belagiannis machine deep computer vision fau 1 teaching neural network say know designed aware likelihood ratio estimation method detect object category pixel level meet marc h lle honolulu week discus paper made fau erlangen n rnberg iccvw2025 fau deeplearning automateddriving neuralnetworks marc h lle teaching machine know know 1 teaching say know matter autonomous driving real world autonomous driving perception system need accurate need know know standard semantic segmentation often misclassify object alarming confidence creating serious safety risk latest research introduce aware likelihood ratio estimation method reliably detect object pixel level even complex cluttered driving scene know life saving new instead relying single point prediction approach leverage evidential deep explicitly enables reduce confusion rare known object truly one increase robustness distributional shift leverage synthetic outlier exposure effectively key result across five benchmark datasets lowest average false detection rate 2 5 state art high average precision 90 91 negligible computational overhead drop distribution segmentation accuracy important beyond immediate result work lay foundation general framework statistical hypothesis testing likelihood ratio distributional promising path toward safer trustworthy system",
    "cluster": 15,
    "keywords": "estimation, research, prediction, phd, deep, paper, work, app, 08, detection"
  },
  {
    "filename": "646297672cbcb9b8666ae768a26ad74c.txt",
    "text": "08 app eddy ilg eddy ilg ex meta professor university technology nuremberg 3 3 see saarland university one leading location computer science germany europe computer vision machine perception research lab headed eddy ilg close tie mpi saarbr cken meta open postdoc position 3d computer vision continual although deep conquered field perception algorithm today omnipresent strong limitation conventional system operate 2d use fixed training dataset goal project advance state art machine perception algorithm lifting representation 3d enabling system increase knowledge continuously incoming unlabeled end developing deep based reason knowledge combining state art 3d reconstruction technique work involve 3d reconstruction physical light transport estimation continual connect natural language processing reinforcement fundamental research position target publication cvpr eccv iccv neurips set outstanding career academia industry research highly relevant future practical use research 3d scene understanding contextual virtual shopping visual search working interdisciplinary setting embedded environment saarland informatics campus leading researcher field building international relationship requirement postdoc position outstanding phd strong visibility cvpr eccv iccv neuri",
    "cluster": 15,
    "keywords": "estimation, research, prediction, phd, deep, paper, work, app, 08, detection"
  },
  {
    "filename": "6667411537d00025a2f865cf8c017e97.txt",
    "text": "08 app nikita durasov phd nikita durasov phd research scientist vlm semantic search nvidia 1 visiting vienna icml next week come see poster present several novel method field deep estimation detail 1 nikita durasov doruk oner jonathan donier hieu le pascal fua thursday july 25th 1 30 3 00 pm cest hall c 4 9 1308 2 nikita durasov nik dorndorf hieu le pascal fua sunday july 21th 9 30 11 00 2 15 3 45 pm cest 6th symposium advance approximate bayesian inference look forward engaging conversation insightful feedback see icml2024 icml uncertaintyestimation enabling estimation iterative neural network icml cc 23 6 241 29 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 15,
    "keywords": "estimation, research, prediction, phd, deep, paper, work, app, 08, detection"
  },
  {
    "filename": "751c34bb6edf8351fbd2b3a63822315f.txt",
    "text": "08 app nikita durasov phd nikita durasov phd research scientist vlm semantic search nvidia 3 pleased share paper accepted icml int l conference machine vancouver thanks co author assaf shocher doruk oner phd gal chechik alexei efros pascal fua great collaboration joint effort epfl nvidia university california berkeley bilkent university method offer promising way adapt inference rely domain specific auxiliary task difficult generalize require substantial tuning work take different approach introduce simple task agnostic method enforces making prediction stable repeated application allows adapt distribution input inference using current test instance need auxiliary loss extra achieves distribution shift across diverse set domain imagenet classification road segmentation age prediction tabular regression aerodynamic simulation gnns provide ready use implementation torch ttt run line code explore project page detail motivation watch short video explainer link comment dive code try colab demo icml2025 icml robustness robustml testtimetraining 3 idempotent test time training norange io 106 6 nikita duraso",
    "cluster": 15,
    "keywords": "estimation, research, prediction, phd, deep, paper, work, app, 08, detection"
  },
  {
    "filename": "7f4caaa64bae2c6b0062a7e48527388a.txt",
    "text": "08 app nikita durasov phd nikita durasov phd research scientist vlm semantic search nvidia 4 phd done last month successfully defended phd officially became dr durasov diploma formally state huge thanks epfl epfl school computer communication science advisor pascal fua committee collaborator friend family supported throughout journey well entire cvlab team made past year intellectually stimulating enjoyable thesis focused estimation deep neural network particularly safety critical application computer vision robotics autonomous driving investigated design adapt handle distribution provide reliable prediction complex dynamic environment autonomous become increasingly prevalent topic become crucial ensuring reliable decision making today world grateful opportunity pursued passion research epfl looking forward new challenge opportunity lie ahead thanks everyone part phd journey 576 37 nikita durasov phd research scientist vlm semantic search nvidia 4 masksembles estimation cvpr 2021 leveraging self supervision cross domain crowd counting cvpr 2022 zigzag universal sampling free estimation two step inference tmlr 2024 enabling estimation iterative neural network icml 2024 it3 idempotent test time training icml",
    "cluster": 15,
    "keywords": "estimation, research, prediction, phd, deep, paper, work, app, 08, detection"
  },
  {
    "filename": "901c874015444bfa0158fa9c95d807ae.txt",
    "text": "08 app vasileios belagiannis vasileios belagiannis machine deep computer vision fau 1 recent research machinelearning system also motivation behind teaching activity fau erlangen n rnberg currently building new lab course teach student bring machine algorithm resource constrained agent including topic estimation neural architecture design domain adaptation related recent publication topic consistency regularisation unsupervised domain adaptation monocular depth estimation collas 2024 multi conditioned graph diffusion neural architecture search tmlr 2024 joint distribution detection estimation trajectory prediction iros 2023 picture taken group research discussion amir el ghoussani marc h lle rohan asthana michele de vita fau 90 3 neel thakkar automotive engineer 1 really great inspiring vipul patil senior engineer waiys ex allianz boeing acl digital m computational engineering dl nlp genai llm agentic automation chatbots mlops 1 interesting sheraz ahmad siemens healthineers 1 interesting 1 912 32 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 15,
    "keywords": "estimation, research, prediction, phd, deep, paper, work, app, 08, detection"
  },
  {
    "filename": "a018fe7b0472fc495ad4e257597f2958.txt",
    "text": "08 app thomas monninger thomas monninger staff machine engineer mercedes benz research development north america 2 thrilled share paper accepted iros work introduces generative capture real world ambiguity constructing multiple plausible hd map camera image real time variance across map sample serf reliable estimate show 31 higher occluded area enabling safer aware navigation image show action truck block view generates diverse plausible prediction occluded road boundary variance map sample correctly highlight region high showing know see huge thank co author zihan zhang zhipeng mo md zafar anwar steffen staab sihao ding fantastic collaboration check paper iros2025 autonomousdriving robotics deeplearning computervision diffusionmodels onlinemapping 181 4 fran ois piednoel de normandie athos silicon cofounder ex performance guru glorious intel ex mercedes benz ada hardware architect ieee member 2 congrats thomas 1 lihao wang software engineer deep 2 congrats thomas 2 1 850 42 generative update trend latest development deep application vision capability assessing reliability generative understanding generate false information include optimization career productivity finance soft skill emotional intelligence project management educat",
    "cluster": 15,
    "keywords": "estimation, research, prediction, phd, deep, paper, work, app, 08, detection"
  },
  {
    "filename": "b211a50b76be329b964bc00bd82fb2ba.txt",
    "text": "08 app kshitij sirohi computer vision deep scientist autonomous driving estimation 2 today iros2023 presenting work aware panoptic segmentation 15 00 deep perception ii session later poster t19 11 collaborative work wonderful co author sajad marvi daniel b scher wolfram burgard looking forward seeing guyz nice conversation paper code 110 1 cl ment le bihan r software engineer easymile working perception autonomous driving 1 awesome 1 1 942 11 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 15,
    "keywords": "estimation, research, prediction, phd, deep, paper, work, app, 08, detection"
  },
  {
    "filename": "ed1f0d2688cbbac1b1a3c5fe9b192b44.txt",
    "text": "08 app vasileios belagiannis vasileios belagiannis machine deep computer vision fau 2 designed trajectory predictor measure confidence read brand new work joint estimation distribution detection robotics machinelearning deeplearning iros2023 fau fau erlangen n rnberg julian wiederer phd candidate mercedes benz ag friedrich alexander university erlangen 2 failure trajectory prediction severe consequence automated vehicle open research question detect prediction failure complex unseen scenario recent paper propose solution question presented iros 2023 detroit paper pre print arxiv jointly estimate prediction recognize novel scenario shared latent space encoder decoder network trajectory prediction module estimate current prediction error simultanously gaussian mixture recognizes distribution scenario result experiment use state art trajectory prediction figure apparently prediction wrong predicts straight maneuver red actual future maneuver right turn green module correctly identifies prediction error hand side ood score low since scenario new respect training thanks lot contributed project especially julian schmidt ulrich kre el klaus dietmayer special thanks professor vasileios belagiannis always beeing supportive looking forward coming collaboration automateddriving robotic machinelearning deeplearning research mercedesbenz opensource fau uniulm 13 1 mahdi torabi rad ph",
    "cluster": 15,
    "keywords": "estimation, research, prediction, phd, deep, paper, work, app, 08, detection"
  },
  {
    "filename": "f7a01cf3ae7babe804fec16f30f2ee75.txt",
    "text": "08 app wolfram burgard wolfram burgard professor robotics artificial intelligence university technology nuremberg 2 another cool work present icra2023 one novel approach lidar panoptic segmentation provides utilizes estimate improve robustness please come london check kshitij sirohi computer vision deep scientist autonomous driving estimation 2 thrilled share paper aware lidar panoptic segmentation got accepted icra2023 london greatly thank editor reviewer wonderful co author sajad marvi daniel buescher wolfram burgard constant support collaboration make happen arxiv code autonomousvehicles lidar autonomousdriving robotics uncertaintyestimation computervision artificialintelligence segmentation objectdetection 125 12 208 89 career productivity finance soft skill emotional intelligence project management education cookie 163 com cookie",
    "cluster": 15,
    "keywords": "estimation, research, prediction, phd, deep, paper, work, app, 08, detection"
  },
  {
    "filename": "fc62f010804dbed21af4b8ab99babdcf.txt",
    "text": "08 app till beemelmanns phd candidate rwth aachen computer vision 6 happy share latest work occuq exploring efficient quantification 3d occupancy prediction accepted icra2025 paper code dataset video 3d occupancy prediction become active field robotics actually trust prediction propose adaptation efficient estimation technique disentangle epistemic aleatoric voxel wise occupancy prediction allows u dynamically calibrate confidence enhancing safety handling adverse condition snow sensor failure etc providing indicator rare unseen object approach outperforms standard baseline offering reliable robust 3d occupancy prediction presenting finding icra world leading robotics conference may atlanta big shout severin heidrich thesis provided foundation work special thanks alexey nekrasov excellent contribution guidance work acknowledge aithena project funding research institute automotive engineering ika icra2025 robotics occupancyprediction uncertaintyquantification computervision 88 1 709 15 impact automotive development understanding diagnosis prediction reliability improve predictive accuracy vision technology improves robotics balance trust skepticism robot improve emergency response career productivity finance soft skill emotional intelligence project management education cookie",
    "cluster": 15,
    "keywords": "estimation, research, prediction, phd, deep, paper, work, app, 08, detection"
  }
]