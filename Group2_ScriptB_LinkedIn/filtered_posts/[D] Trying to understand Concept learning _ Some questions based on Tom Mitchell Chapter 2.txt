Source: Reddit/MachineLearning
URL: https://reddit.com/r/MachineLearning/comments/1649kmr/d_trying_to_understand_concept_learning_some/
Title: [D] Trying to understand Concept learning | Some questions based on Tom Mitchell Chapter 2

Content:
Hi, Im going through Tom Mitchell's Machine Learning and have a couple of questions based on the 2nd chapter  : Concept learning. I was hoping I could get some external point of view on these:

&#x200B;

1. Pg 44, para 2, part 1 : "advantage of viewing inductive inference systems in terms of their inductive bias is that it provides a nonprocedural means of characterizing their policy for generalizing"

* Are there any general procedures to identify and validate the inductive bias of a system?
* Are there any guidelines to ensure the inferred definition of inductive bias is without errors?
* Assuming all/most predictive algorithms can be defined in terms of their inductive bias, while concentrating on choosing the algorithms which aligns with our philosophy of talking a problem, how can we weigh parts of inductive bias originating from implementation architecture vs procedure of the algorithm?

1. Pg 44, para 2, part 2 : "second advantage is that it allows comparison of different learners according to strength of inductive bias they employ"

* From the example given in the text, the comparison seems to be more qualitative in nature, what could be some methods to qualitative measure the strength? although the "number of individual assumptions" to "number of training data instances" ratio seems like something which can be used, however, assumption count does not directly proportionate to strength of generalization, in which case can we use size of the version space?
* how do we define version space and hypothesis space in reinforcement learning? how do we estimate the size of version space for RL algorithms?
   * Self answering : hypothesis space in RL is the set of all possible policies over the given state-action space for a policy based method, and set of all possible value functions over given state-action space.  Based on the definition [Generalized VS](https://link.springer.com/content/pdf/10.1007/BF00993863.pdf) for policy based RL would be the set of all possible policies which are consistent with most of the training steps, with a similar definition for value based RL methods.  The size of HS and VS is bounded by state-action spaces, which is quantifiable for discrete spaces, however is infinite for continuous spaces, so comparing size of VS directly does not sound like a feasible option in many cases. Additionally using VS for comparison might make more sense when the training data is generated by an expert, maybe like imitation learning, It doesn't make sense in an exploratory system. Most RL methods apart from ensemble techniques maintain a single hypothesis at any time, therefore comparison of size of VS cannot be done.
* In the paper "On Inductive Biases in Deep Reinforcement Learning", Hessel et al, quotes : "Stronger biases can lead to faster learning, but weaker biases can potentially lead to more general algorithms", does a general algorithm refer to something that can generalize well ? this does not align with the example of rote learner, as well as the text book, quote, page 45, para 2, "more strongly biased methods make more inductive leaps, classifying a greater proportion of unseen instances" i.e generalize better. Does the definition of weak and general in the paper refer to a particular kind of models/algorithms? (haven't gone through the paper completely yet). Does general learner in the paper refer to a model with general hypothesis?
* It doesn't seem that a version space(VS) ***V\_1*** with over training data ***D*** being larger (more general) is necessarily better than another slightly smaller version space ***V\_2***, (can belong to different algorithms/ different configuration of same algorithm), reason : a very large VS has higher chances of misclassification, similar to a very small VS. So how do we determine which algorithm is better? should we only look at it with respect to respective efficiency of convergence of the hypothesis space asymptotically and sample efficiency? ( measuring loss in some sense describes the size of the current VS wrt hypothesis space and training data for an algorithm).
* It also seems like utility of "strength of inductive bias" could be a lever in solution design, i.e how liberal or constrained we want our solution to be in association with seen examples, depending on the problem area.
* Would measuring loss be also a measure of uncertainty regarding the identity of the target concept? so from above, "Strength" of inductive bias is uncertainty of identifying target concept by an algorithm?

Comments:
