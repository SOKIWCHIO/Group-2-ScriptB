Source: Reddit/MachineLearning
URL: https://reddit.com/r/MachineLearning/comments/1kf3pes/discussion_what_exactly_are_world_models_in_ai/
Title: [Discussion] What exactly are World Models in AI? What problems do they solve, and where are they going?

Content:
Hi all, I‚Äôve been reading a lot about "World Models" lately, especially in the context of both reinforcement learning and their potential crossover with LLMs.     I‚Äôd love to hear the community‚Äôs insights on a few key things:

**‚ùì What problem do world models actually solve?**

From what I understand, the idea is to let an agent build an internal model of the environment so it can predict, imagine, and plan, instead of blindly reacting. That would massively reduce sample inefficiency in RL and allow generalization beyond seen data. Is that accurate?

**‚≠êÔ∏è How do world models differ from expert systems or rule-based reasoning?**

If a world model uses prior knowledge to simulate or infer unseen outcomes, how is this fundamentally different from expert systems that encode human expertise and use it for inference?     Is it the learning dynamics, flexibility, or generative imagination capability that makes world models more scalable?

**üß† What technologies or architectures are typically involved?**

I see references to:

* Latent dynamics models (e.g., DreamerV3, PlaNet)
* VAE + RNN/Transformer structures
* Predictive coding, latent imagination
* Memory-based planning (e.g., MuZero)

Are there other key approaches people are exploring?

**üöÄ What's the state of the art right now?**

I know DreamerV3 performs well on continuous control benchmarks, and MuZero was a breakthrough for planning without a known environment model. But how close are we to scalable, general-purpose world models for more complex, open-ended tasks?

**‚ö†Ô∏è What are the current challenges?**

I'm guessing it's things like:

* Modeling uncertainty and partial observability
* Learning transferable representations across tasks
* Balancing realism vs. abstraction in internal simulations

**üîÆ Where is this heading?**

Some people say world models will be the key to artificial general intelligence (AGI), others say they‚Äôre too brittle outside of curated environments. Will we see them merged with LLMs to build reasoning agents or embodied cognition systems?



**Would love to hear your thoughts, examples, papers, or even critiques!**

Comments:
- Was this generated by AI? For some reason, it looks very similar to the format chatgpt uses with the emoji section headers and stuff.
- In my opinion, it's just a general concept: the ability to predict the next state of the world based on the previous state of the world. It's your understanding of the world. How well you grasp the behaviour of nature, people and even abstract concepts. 

You can claim to have created a world model by using completely different techniques: from deep learning-based vision systems, from symbolic AI, from LLMs, etc.

In a sense, you can almost give it any definition at this point.

I think you might like this video [https://www.reddit.com/r/newAIParadigms/comments/1k7uzlu/the\_concept\_of\_world\_models\_why\_its\_fundamental/](https://www.reddit.com/r/newAIParadigms/comments/1k7uzlu/the_concept_of_world_models_why_its_fundamental/)

(Btw, I really like your thread. I actually learned a lot. Thank you very much)
- A " world model " is really just some set of assumptions The model makes about the world reflected in how it makes decisions.

Mostly when we mean this we aren't actually referring to some kind of concretized design and instead speculating on a performance characteristic
- models are simplify version of a system, there is so many uncertainty in the real world I don't think it is possible that we use current machine learning training techniques to model the world (or maybe?)
- deserve coordinated quickest makeshift test air grandfather lunchroom simplistic telephone

 *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*
- The world model is the latent representation of the world that models, in case of a video diffusion transformer, the dynamics of the world. Look at open-oasis, in that case the model learns the Minecraft world, its physics, it's interactions from Minecraft video gameplay. Imagine training a model on real high def video of nature, not a video game. Maybe even impose the symplectic form of classical mechanics by using a Hamiltonian Graph Neural Network and symplectic integrators. And unitary from quantum mechanics by adding some kind of auxiliary loss or otherwise enforce self-adjointness of the model. Perhaps go as far as to try to put in general relativity from the start by adding proper time to every "voxel" in this latent space representation of the world, like a fiber bundle. the qs and ps would form a cotangent fiber with classical phase space, a proper time fiber and a U(1) phase fiber for quantum amplitude. All of this enforced by the architecture, more things could be added. Then learn the physics of the world on top of this scaffold you built from the architecture. The closer your architecture is to real physics, the easier it would be to learn the physics from just video.
- Let me give you the picture. You are building an AI for playing.. Go, yes the very same AI as AlphaGo. Now state is easy right, its just snapshot of the board and each tile has 3 kind of state: EMPTY, WHITE, BLACK. modelling the state is kind of trivial we can agree. But what about action? how would you model action of placing the piece on the board?

For each state, there are way many actions you can take, and each might be unallowed action like placing a piece on already placed one. So your naive solution is just, if the model make a mistake of taking unallowed action, punish it with negative reward. It can definately work, but lets see another solution. What if your model can simulate Go itself. Yes, you are now doing a search, traditional one, you are seeking the next state or next few state that give you the biggest advantage greedily. your model simulate say 3 turn and look at the advantage for each action. after some simulations, you agree by taking greedy advantage, this state is the best and your model can get to that state if it take certain action. 

Notice how we never have Policy model or Q model, just old plain V ro evaluate a state. and this is possible on finite action as infinite action may be impossible to simulate where we want to go. And this example is what i always want to think when i try to explain what the hell is World Model. It can be both explicit and implicit. Explicit one works like our Go example while implicit one works differently and mostly in latent space. And our model use its explicit knowledge build by human to plan whats the next best move.

Interestingly, i believe this is how human reason by planning. we dont just follow orders blindly, we reason whats best action to take and we can do that because we can predict by common sense whats gonna happen if i do this or this. Interestingly, Risk is also modelled in this. In our Greedy advantage following example, it might also has a chance to create the greetest disadvantage because it might be a risky move or something.
- I believe the definition of a world model varies slightly depending on the different context.   
  
In cognitive science, it is often referred to as "mental models", a nature human ability to construct internal representations of the world and predict future states of things, also known as the *human world model*. With the development of AI, this concept has evolved into what we now discuss as the *AI world model*. I think this is a good definition:  
  
**An AI world model is broadly any mechanism by which an agent captures or accesses approximate environment dynamics.**  
  
This definition comes from this paper, also inspired my understanding.  
[https://arxiv.org/pdf/2504.01990](https://arxiv.org/pdf/2504.01990)

The concept of a "world model" has long existed in research, independent of AI technologies like reinforcement learning or generative AI. It is the rise of AI that has brought renewed attention to the idea.
- Absolutely great post.

You are correct on what a world model is. The core difference between world model and expert systems is that the rules are automatically generated at an absolutely massive scale and that by the probabilistic nature of neutral networks, they can be adjusted to any kind of decision space. In expert systems, your 1000 rule system is extremely hard to adjust if a new data point does not match the current set of rules.

The main challenges also include being able to efficiently learn complex internal representations. Most successes are on video games, that are quite straightforward, and for many reasons, the methods don't scale to real world problems (sample efficiency, world model scale...)
- I think it partially is but he seems to understand his stuff enough to the point where I think he at least heavily edited this
- Part of, because I am more familiar with Chinese . I find that by telling ChatGPT the details and structure I want to say, the post would be more logic and clear for English speakers.
- Thanks for the thoughtful reply and I really appreciate the video recommendation, it helped clarify some things!



Interestingly, I saw others in the thread suggesting that a world model is really about judging whether a system can reason or infer causality, almost like a functional benchmark rather than a specific technique.



But then the video you shared seems to draw a distinction between world models and LLMs, emphasizing that world models don‚Äôt operate in language space, but instead in some kind of latent space, more aligned with mental simulation, like how humans think visually or intuitively without words.



That feels closer to what Yann LeCun has been proposing where world models are part of a new computation paradigm entirely, rather than just a behavior we can attribute to any model that generalizes well.



Curious how you see this, do you think world models are just a general, purpose concept applicable to many techniques, or are they shaping up to be a new class of architectures altogether?
- So if I understand you correctly, when we talk about a ‚Äúworld model,‚Äù we‚Äôre really evaluating whether a system can reason about causality and make informed decisions based on some internal assumptions about the environment, rather than referring to a specific architectural module.



That makes me wonder, do you think current LLMs or some forms of embodied intelligence are starting to meet that bar? If so, what kinds of techniques or improvements (architectural or training-wise) do you think are enabling that? I'd love to hear your take.
- Yeah, I think you're right that fully modeling the real world in all its complexity is probably infeasible with current ML techniques.

But like that other top-voted comment suggested, I see ‚Äúworld models‚Äù less as literal simulations of the world and more as a way for machines to reason about unseen conditions through internal causal logic. It's not about building a complete replica of the world, but rather giving the agent the ability to make informed predictions or plans beyond what it has directly experienced.
- That makes sense.  So if I understand correctly, you're saying that a world model is more about processing information through internal representations of space and time, rather than relying on semantics like LLMs do.



That‚Äôs quite different from some other views in the thread, where a ‚Äúworld model‚Äù is treated more like a conceptual benchmark for whether a system can reason or infer causality, regardless of how it processes information.



I'm really curious though if it's not grounded in semantics, then what kind of data does a world model actually process?  Are we talking about equations, physical states, or some other form of structured signal?
- Ah I see! Interesting use case.
- Objectively I wouldn't tie the concept of World Models to any specific technique. It would be a bit disingenuous

Personally though I don't think it makes sense to attribute it to a system that is primarily text-based. Intuitively, I would say it makes more sense to attribute it to systems that excel at vision first and language second (like humans and animals). If it's not vision, it probably needs to be based on continuous sensory input (like touch, audio). 

As you mentioned, I think a system with a real World Model should be able to mentally simulate scenarios both in the real world and in the more abstract world.

People often don't realize this but even when we do math in our head we still visualize things. Our mental images are just a bit fuzzier and harder to explain in words.
- More importantly, you don't have to build anything by hand (aside from basic feature engineering ) unlike rule based/expert systems. With enough unsupervised and supervised learning, eventually the ML system will have a statistical representation that's close to/similar enough to the real world for practical use.
- long dinosaurs history marble reminiscent money profit attempt distinct busy

 *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*
