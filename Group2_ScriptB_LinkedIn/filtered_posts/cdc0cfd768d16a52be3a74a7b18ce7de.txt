URL: https://www.linkedin.com/posts/ido-galil_iclr2023-uncertaintyestimation-selectiveprediction-activity-7054812679651115009-AJsy
Date: 2025-10-08
Author: Unknown

è·³åˆ°ä¸»è¦å†…å®¹
é¢†è‹±
çƒ­é—¨å†…å®¹
ä¼šå‘˜
é¢†è‹±å­¦ä¹ 
èŒä½
æ¸¸æˆ
ä¸‹è½½ APP
é©¬ä¸ŠåŠ å…¥
ç™»å½•
Ido Galilçš„åŠ¨æ€
Ido Galil

Deep Learning Researcher at NVIDIA & PhD Student in Computer Science at the Technion

2 å¹´  å·²ç¼–è¾‘

Iâ€™m excited to share another of my papers, to be published in ICLR 2023, made in collaboration with my friend Mohammed Dabbah and my supervisor Ran El-Yaniv:

ğ˜ğ˜©ğ˜¢ğ˜µ ğ˜Šğ˜¢ğ˜¯ ğ˜¸ğ˜¦ ğ˜“ğ˜¦ğ˜¢ğ˜³ğ˜¯ ğ˜ğ˜³ğ˜°ğ˜® ğ˜›ğ˜©ğ˜¦ ğ˜šğ˜¦ğ˜­ğ˜¦ğ˜¤ğ˜µğ˜ªğ˜·ğ˜¦ ğ˜—ğ˜³ğ˜¦ğ˜¥ğ˜ªğ˜¤ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜ˆğ˜¯ğ˜¥ ğ˜œğ˜¯ğ˜¤ğ˜¦ğ˜³ğ˜µğ˜¢ğ˜ªğ˜¯ğ˜µğ˜º ğ˜Œğ˜´ğ˜µğ˜ªğ˜®ğ˜¢ğ˜µğ˜ªğ˜°ğ˜¯ ğ˜—ğ˜¦ğ˜³ğ˜§ğ˜°ğ˜³ğ˜®ğ˜¢ğ˜¯ğ˜¤ğ˜¦ ğ˜–ğ˜§ 523 ğ˜ğ˜®ğ˜¢ğ˜¨ğ˜¦ğ˜¯ğ˜¦ğ˜µ ğ˜Šğ˜­ğ˜¢ğ˜´ğ˜´ğ˜ªğ˜§ğ˜ªğ˜¦ğ˜³ğ˜´?
https://lnkd.in/d9BZgdSx

ğ•ğ¢ğğğ¨ ğ­ğšğ¥ğ¤:
 https://lnkd.in/djpgA6Af

ğ“ğ‹ğƒğ‘: 
We present a novel and comprehensive study of selective prediction and the uncertainty estimation performance of 523 existing pretrained deep ImageNet classifiers that are available in popular repositories. We identify numerous and previously unknown factors that affect uncertainty estimation and examine the relationships between the different metrics. To name a few, we find that distillation-based training regimes consistently yield better uncertainty estimations than other training schemes such as vanilla training, pretraining on a larger dataset and adversarial training. Moreover, we find a subset of ViT models that outperform any other models in terms of uncertainty estimation performance.

ğ’ğ®ğ¦ğ¦ğšğ«ğ²:
We evaluate 500+ ImageNet-1k classifiers for their uncertainty estimation performance, i.e., ranking (AUROC), calibration (ECE) and selective performance (selective risk & SAC). We make numerous novel observations. To name a few:

1. Knowledge distillation is the training regime that consistently improves uncertainty estimation performance the most. This remains true even when the teacher itself is much worse at uncertainty estimation.

2. Several training regimes result in a subset of ViTs that outperforms all other architectures and training regimes. 

3. Contrary to previous work, the correlations between AUROC, ECE, accuracy and the number of parameters of the model are dependent on the architecture analyzed. We observe that while there is a strong correlation b