URL: https://www.linkedin.com/posts/judd-devermont-24296611a_intelligence-reform-should-include-an-updated-activity-7364682952326160387-29l2
Date: 2025-10-08
Author: Unknown

è·³åˆ°ä¸»è¦å†…å®¹
é¢†è‹±
çƒ­é—¨å†…å®¹
ä¼šå‘˜
é¢†è‹±å­¦ä¹ 
èŒä½
æ¸¸æˆ
ä¸‹è½½ APP
é©¬ä¸ŠåŠ å…¥
ç™»å½•
Judd Devermontçš„åŠ¨æ€
Judd Devermont

Operating Partner (Innovation), Kupanda Capital

1 ä¸ªæœˆ

This is a fantastic piece. Jerry offers a sensible approach to simplify what has become such a confusing and contentious issue: probabilistic language in the intelligence community.

Jerry Laurienti, Ph.D.

Leidos | Former CIA | Texas A&M Bush School DC Adjunct Professor | Korbel School of International Studies Alumni Council Member

1 ä¸ªæœˆ

Thanks to the #TheCipherBrief for publishing my thoughts about restarting a conversation around how the IC gauges and expresses probability. You may disagree with my suggestion, but perhaps a debate can help us get it right.

Intelligence Reform Should Include an Updated Probability Yardstick
thecipherbrief.com
14
2 æ¡è¯„è®º
èµ
è¯„è®º
åˆ†äº«
Blair Sondker

Intelligence Analyst | Political Risk Manager | Strategic Advisor

1 ä¸ªæœˆ

This is a blisteringly important topic/consideration, particularly when briefing customers outside of traditional national security roles. Terms of art within analytic, probability, absolutely fluster scientific or engineering audiences

èµ
å›å¤
1 æ¬¡å›åº”
Christopher Hoffman

National security professional with over 20 years of service. Cyber policy, translation, intelligence production, covering South America, Sub-Saharan Africa, and the Middle East. Conversation: serious or light.

1 ä¸ªæœˆ

it's certainly more realistic.

èµ
å›å¤
æŸ¥çœ‹æ›´å¤šè¯„è®º

è¦æŸ¥çœ‹æˆ–æ·»åŠ è¯„è®ºï¼Œè¯·ç™»å½•

æœ€ç›¸å…³çš„åŠ¨æ€
MD. REAZ UDDIN

CSE Student | Reverse-engineering AGI, Embedded Systems | Building Scalable Tools & Open Source Projects

3 å‘¨  å·²ç¼–è¾‘

ğŸ”¥Map top to bottom, how LLMs actually work

Stop 1: Text â†’ Tokens â†’ Embeddings
Stop 2: Positional Embeddings (absolute, RoPE, Alibi)
Stop 3: Attention (QKV, multihead)
Stop 4: Transformers (normalize, repeat, logits)
Stop 5: Sampling tricks (temperature, top-k, top-p)
Stop 6: KV cache + long context hacks
Stop 7: MoE & GQA
Stop 8: Normalizations & activations (PokÃ©mon icons?)
Stop 9: Training objectives (causal LM, ma