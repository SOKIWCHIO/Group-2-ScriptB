URL: https://www.linkedin.com/posts/ilia-ekhlakov_machinelearning-datascience-modelvalidation-activity-7333113796716060672-tYB-
Date: 2025-10-08
Author: Unknown

è·³åˆ°ä¸»è¦å†…å®¹
é¢†è‹±
çƒ­é—¨å†…å®¹
ä¼šå‘˜
é¢†è‹±å­¦ä¹ 
èŒä½
æ¸¸æˆ
ä¸‹è½½ APP
é©¬ä¸ŠåŠ å…¥
ç™»å½•
Ilia Ekhlakovçš„åŠ¨æ€
Ilia Ekhlakov

Senior Data Scientist | Growth & Retention with Advanced Analytics | Predictive ML & Causal Inference | Google Cloud Platform (GCP) | 10 Years of Experience | Considering New Challenges

4 ä¸ªæœˆ

ğ€ğ¬ğ¬ğğ¬ğ¬ğ¢ğ§ğ  ğ¦ğ¨ğğğ¥ ğ«ğğ¥ğ¢ğšğ›ğ¢ğ¥ğ¢ğ­ğ² ğ¯ğ¢ğš ğ­ğ¡ğ ğğ¢ğ¬ğ­ğ«ğ¢ğ›ğ®ğ­ğ¢ğ¨ğ§ ğ¨ğŸ ğªğ®ğšğ¥ğ¢ğ­ğ² ğ¦ğğ­ğ«ğ¢ğœğ¬

When we train model ensembles, we often use bootstrapping or bagging - some people without even realizing it. Both methods help improve generalization by creating multiple different samples from our training data. The main difference is that bagging samplesÂ withoutÂ replacement, while bootstrapping samplesÂ withÂ replacement. But their goal is the same: they treat our single training dataset as justÂ one possible realizationÂ of the true population (for example, all your company's clients or all leads over a period).

During training, we might generateÂ dozens or even hundredsÂ of such samples to build a more robust model that handles the randomness in our data.
In cross-validation, however, we normally use onlyÂ 3â€“5 splits. This does reduce the effect of one particular validation split, but not as much as bootstrapping or bagging.

And when it comes to the final hold-out test, we often useÂ just one hold-out set. Yet the same logic applies: the hold-out set is onlyÂ one possible future dataset. We often assume our hold-out data will closely resemble the data we see at inference time - but in reality, that similarity isn't guaranteed.

It's important to remember thatÂ any performance metric is itself a random variable, not a fixed constant. Each new hold-out split or bootstrap sample can lead to different metric's value. So rather than relying on a single point estimate, we can choose:

ğğšğ¬ğ¢ğœ ğ¨ğ©ğ­ğ¢ğ¨ğ§ğ¬:
ğŸ‘‰ Multiple hold-out splitsÂ (when data allows). For example it could be nested cross-validationÂ (outer loop for evaluation, inner loop for hyperparameter tuning) to obtain a more honest estimate of both performance and its uncertainty
ğŸ‘‰ Bootstrappin