URL: https://www.linkedin.com/posts/simonsee_this-an-very-important-piece-of-work-activity-7269681415020011520-WygP
Date: 2025-10-08
Author: Unknown

è·³åˆ°ä¸»è¦å†…å®¹
é¢†è‹±
çƒ­é—¨å†…å®¹
ä¼šå‘˜
é¢†è‹±å­¦ä¹ 
èŒä½
æ¸¸æˆ
ä¸‹è½½ APP
é©¬ä¸ŠåŠ å…¥
ç™»å½•
Simon Seeçš„åŠ¨æ€
Simon See
10 ä¸ªæœˆ

This an very important piece of work.    Ability to decompose aleatoric and epistemic uncertainty will allow us to have better insight to how we can qualify the uncertainty.  

Michael Kirchhof

Research Scientist at Apple on Uncertainty Quantification

10 ä¸ªæœˆ

Proud to announce our NeurIPS spotlight, which was in the works for over a year now :) We dig into why decomposing aleatoric and epistemic uncertainty is hard, and what this means for the future of uncertainty quantification.

ğŸ“– https://lnkd.in/eMfBaHxS 

The easy solution would of course be to use a decomposition formula like "predictive uncertainty = aleatoric uncertainty + epistemic uncertainty". However, we find that this does not work. In practice, the two components are literally the same, see the scatterplot.

And this happens for any second-order distribution we've implemented. From evidential deep learning to Laplace approximations to deep ensembles, the rank correlations between the two components are always within 0.8 and 0.999 (!)

So how could we estimate epistemic and aleatoric uncertainty then? For epistemic, we find that an uncertainty estimator trained explicitly on OOD data is the best in OOD detection. So maybe we should go away from general uncertainty methods, towards specialized ones.

For aleatoric uncertainty, no clear winner exists yet. This seems to be a more challenging task, and also to depend on how exactly the aleatoric uncertainty ground-truth is collected on each dataset.

We also test multiple predictive uncertainty metrics. AUROC, AUAC, rAULC, ECE. The best method always depends on what exactly the metric tests, and thus, what _kind_ of predictive uncertainty you need a solution for in your practical task.

So what did we learn in the past year of research? We need to rethink the aleatoric/epistemic dichotomy. Uncertainty estimation is a very rich and nuanced field with a whole spectrum of types of uncer