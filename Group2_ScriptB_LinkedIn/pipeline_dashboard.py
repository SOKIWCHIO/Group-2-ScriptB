"""
Group 2 - Uncertainty Estimation Clustering Dashboard
å¯è§†åŒ–æ§åˆ¶é¢æ¿ | Visual Control Panel

Usage:
    streamlit run pipeline_dashboard.py
"""

import streamlit as st
import os
import subprocess
import time
from datetime import datetime
import pandas as pd
import json
from pathlib import Path

# ============================================================================
# é¡µé¢é…ç½®
# ============================================================================
st.set_page_config(
    page_title="Group 2 Pipeline Dashboard",
    page_icon="ğŸ¯",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============================================================================
# è®¾ç½®å·¥ä½œç›®å½•ï¼ˆç¡®ä¿åœ¨æ­£ç¡®ä½ç½®ï¼‰
# ============================================================================
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
os.chdir(SCRIPT_DIR)

# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================
def count_files(folder, ext=None):
    """ç»Ÿè®¡æ–‡ä»¶æ•°é‡ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
    if not os.path.exists(folder):
        return 0
    
    try:
        files = os.listdir(folder)
        
        # å¦‚æœæŒ‡å®šäº†æ‰©å±•å
        if ext:
            return len([f for f in files if f.endswith(ext)])
        
        # ç»Ÿè®¡æ‰€æœ‰æ–‡ä»¶ï¼ˆæ’é™¤æ–‡ä»¶å¤¹å’Œéšè—æ–‡ä»¶ï¼‰
        return len([f for f in files 
                   if os.path.isfile(os.path.join(folder, f)) 
                   and not f.startswith('.')])
    except:
        return 0

def get_folder_status():
    """è·å–æ‰€æœ‰æ–‡ä»¶å¤¹çŠ¶æ€"""
    folders = {
        'LinkedIn Posts': ('linkedin_posts', '.txt'),
        'Reddit Posts': ('Reddit_posts', '.txt'),
        'Filtered Posts': ('filtered_posts', '.txt'),
        'Cluster Output': ('cluster_output', None),
        'Visualizations': ('cluster_visualizations', None),
        'Filter Stats': ('filter_stats', None)
    }
    
    status = {}
    for name, (folder, ext) in folders.items():
        count = count_files(folder, ext)
        exists = os.path.exists(folder)
        status[name] = {
            'folder': folder,
            'count': count,
            'exists': exists,
            'status': 'âœ…' if exists and count > 0 else 'âš ï¸' if exists else 'âŒ'
        }
    return status

def run_pipeline_step(script_path, step_name):
    """è¿è¡Œpipelineæ­¥éª¤"""
    if not os.path.exists(script_path):
        st.error(f"âŒ Script not found: {script_path}")
        return False
    
    with st.spinner(f'ğŸš€ Running {step_name}...'):
        start = time.time()
        result = subprocess.run(
            ['python', script_path], 
            capture_output=True, 
            text=True,
            cwd=SCRIPT_DIR
        )
        elapsed = time.time() - start
        
        if result.returncode == 0:
            st.success(f'âœ… {step_name} completed in {elapsed:.1f}s')
            if result.stdout:
                with st.expander("ğŸ“‹ View Output"):
                    st.code(result.stdout[-2000:], language='text')  # åªæ˜¾ç¤ºæœ€å2000å­—ç¬¦
            return True
        else:
            st.error(f'âŒ {step_name} failed')
            if result.stderr:
                with st.expander("ğŸ› View Error"):
                    st.code(result.stderr[-2000:], language='text')
            return False

# ============================================================================
# ä¸»ç•Œé¢
# ============================================================================
def main():
    # æ ‡é¢˜
    st.title("ğŸ¯ Group 2 - Uncertainty Estimation Clustering")
    st.markdown("**Course**: AAI 6610, Fall 2025 | **Institution**: Northeastern University")
    st.markdown("**Repository**: SOKIWCHIO/Group-2-ScriptB")
    
    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("ğŸ“Š Project Status")
        
        # å½“å‰å·¥ä½œç›®å½•
        st.caption(f"ğŸ“ Working Dir: `{os.getcwd()}`")
        
        # åˆ·æ–°æŒ‰é’®
        if st.button("ğŸ”„ Refresh Status", width='stretch'):
            st.rerun()
        
        st.divider()
        
        # æ˜¾ç¤ºæ–‡ä»¶å¤¹çŠ¶æ€
        status = get_folder_status()
        for name, info in status.items():
            col1, col2 = st.columns([3, 1])
            with col1:
                st.write(f"{info['status']} **{name}**")
            with col2:
                st.write(f"`{info['count']}`")
        
        st.divider()
        
        # å¿«æ·æ“ä½œ
        st.subheader("âš¡ Quick Actions")
        
        if st.button("ğŸ“‚ Open Results", width='stretch'):
            folder = os.path.abspath('cluster_output')
            if os.path.exists(folder):
                os.startfile(folder)
            else:
                st.warning("Folder not found")
        
        if st.button("ğŸ“Š Open Visualizations", width='stretch'):
            folder = os.path.abspath('cluster_visualizations')
            if os.path.exists(folder):
                os.startfile(folder)
            else:
                st.warning("Folder not found")
        
        st.divider()
        
        # æ•°æ®ç»Ÿè®¡
        st.subheader("ğŸ“ˆ Data Statistics")
        total_raw = status['LinkedIn Posts']['count'] + status['Reddit Posts']['count']
        total_filtered = status['Filtered Posts']['count']
        
        if total_raw > 0:
            retention = (total_filtered / total_raw * 100) if total_raw > 0 else 0
            st.metric("Total Raw", f"{total_raw}")
            st.metric("Filtered", f"{total_filtered}")
            st.metric("Retention", f"{retention:.1f}%")
    
    # ============================================================================
    # ä¸»å†…å®¹åŒº - Tabs
    # ============================================================================
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ® Run Pipeline", "ğŸ“Š Results", "ğŸ“ˆ Visualizations", "ğŸ“ Info"])
    
    # ============================================================================
    # Tab 1: è¿è¡ŒPipeline
    # ============================================================================
    with tab1:
        st.header("ğŸ® Pipeline Control Panel")
        
        st.info("ğŸ’¡ Tip: Click any button to run that step. Steps already completed will be skipped automatically.")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ğŸ“¥ Data Collection")
            
            if st.button("ğŸ” Step 0: LinkedIn Auth", 
                        key="step0", width='stretch',
                        help="Get LinkedIn cookies for authentication"):
                run_pipeline_step('pipeline_0_linkedin_cookie.py', 'LinkedIn Auth')
            
            if st.button("ğŸ“± Step 1: LinkedIn Crawler", 
                        key="step1", width='stretch',
                        help="Collect ~450 posts from LinkedIn"):
                run_pipeline_step('pipeline_1_crawler_linkedin.py', 'LinkedIn Crawler')
            
            if st.button("ğŸŒ Step 2: Reddit Crawler", 
                        key="step2", width='stretch',
                        help="Collect ~1000 posts from Reddit/StackExchange"):
                run_pipeline_step('pipeline_1_crawler_Reddit.py', 'Reddit Crawler')
        
        with col2:
            st.subheader("ğŸ”¬ Analysis & Visualization")
            
            if st.button("ğŸ” Step 3: Semantic Filter", 
                        key="step3", width='stretch',
                        help="Filter relevant posts (threshold=30)"):
                run_pipeline_step('pipeline_2_semantic_filter.py', 'Semantic Filter')
            
            if st.button("ğŸ¯ Step 4: Clustering", 
                        key="step4", width='stretch',
                        help="Cluster posts into topics (HDBSCAN + KMeans)"):
                run_pipeline_step('pipeline_3_cluster.py', 'Clustering')
            
            if st.button("ğŸ“Š Step 5: Visualization", 
                        key="step5", width='stretch',
                        help="Generate PCA, t-SNE, UMAP plots"):
                run_pipeline_step('pipeline_3_cluster_print.py', 'Visualization')
        
        st.divider()
        
        # æ‰¹é‡æ“ä½œ
        st.subheader("âš¡ Batch Operations")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("ğŸš€ Run ALL Steps", type="primary", width='stretch'):
                steps = [
                    ('pipeline_1_crawler_linkedin.py', 'LinkedIn Crawler'),
                    ('pipeline_1_crawler_Reddit.py', 'Reddit Crawler'),
                    ('pipeline_2_semantic_filter.py', 'Semantic Filter'),
                    ('pipeline_3_cluster.py', 'Clustering'),
                    ('pipeline_3_cluster_print.py', 'Visualization')
                ]
                
                progress_bar = st.progress(0)
                for i, (script, name) in enumerate(steps):
                    progress_bar.progress((i) / len(steps), f"Running {name}...")
                    if not run_pipeline_step(script, name):
                        st.error(f"Pipeline stopped at {name}")
                        break
                else:
                    progress_bar.progress(1.0, "All steps completed!")
                    st.balloons()
                    st.success("ğŸ‰ All steps completed successfully!")
        
        with col2:
            if st.button("ğŸ”„ Re-run Analysis", width='stretch',
                        help="Re-run filtering, clustering, visualization"):
                steps = [
                    ('pipeline_2_semantic_filter.py', 'Semantic Filter'),
                    ('pipeline_3_cluster.py', 'Clustering'),
                    ('pipeline_3_cluster_print.py', 'Visualization')
                ]
                for script, name in steps:
                    run_pipeline_step(script, name)
                st.rerun()
        
        with col3:
            if st.button("ğŸ“Š Viz Only", width='stretch',
                        help="Re-generate visualizations only"):
                if run_pipeline_step('pipeline_3_cluster_print.py', 'Visualization'):
                    st.rerun()
    
    # ============================================================================
    # Tab 2: ç»“æœå±•ç¤º
    # ============================================================================
    with tab2:
        st.header("ğŸ“Š Clustering Results")
        
        # è¯»å–èšç±»ç»“æœ
        clusters_file = 'cluster_output/HDBSCAN_clusters.csv'
        reps_file = 'cluster_output/HDBSCAN_representatives.txt'
        
        if os.path.exists(clusters_file):
            df = pd.read_csv(clusters_file)
            
            # ç»Ÿè®¡ä¿¡æ¯
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("ğŸ“„ Total Documents", len(df))
            
            with col2:
                n_clusters = len(df['cluster'].unique()) - (1 if -1 in df['cluster'].values else 0)
                st.metric("ğŸ¯ Clusters", n_clusters)
            
            with col3:
                noise = len(df[df['cluster'] == -1]) if -1 in df['cluster'].values else 0
                st.metric("ğŸ”‡ Noise Points", f"{noise} ({noise/len(df)*100:.1f}%)")
            
            with col4:
                if len(df) > 0:
                    largest = df[df['cluster'] != -1]['cluster'].value_counts().iloc[0] if len(df[df['cluster'] != -1]) > 0 else 0
                    st.metric("ğŸ“¦ Largest Cluster", largest)
            
            st.divider()
            
            # ç°‡åˆ†å¸ƒå›¾
            st.subheader("ğŸ“ˆ Cluster Distribution")
            cluster_counts = df[df['cluster'] != -1]['cluster'].value_counts().sort_index()
            st.bar_chart(cluster_counts)
            
            # æ•°æ®è¡¨æ ¼
            st.subheader("ğŸ“‹ Cluster Breakdown")
            cluster_summary = df.groupby('cluster').size().reset_index(name='count')
            cluster_summary = cluster_summary[cluster_summary['cluster'] != -1]
            cluster_summary['percentage'] = (cluster_summary['count'] / len(df) * 100).round(1)
            st.dataframe(cluster_summary, hide_index=True, width=600)
            
            st.divider()
            
            # æ˜¾ç¤ºä»£è¡¨æ–‡æœ¬
            if os.path.exists(reps_file):
                st.subheader("ğŸ“ Cluster Summaries")
                
                with open(reps_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # æŒ‰ç°‡åˆ†å‰²
                clusters_text = content.split('\n\n')
                
                # æ˜¾ç¤ºå‰15ä¸ªç°‡
                for cluster_text in clusters_text[:15]:
                    if cluster_text.strip():
                        lines = cluster_text.split('\n')
                        if lines:
                            header = lines[0]  # Cluster X (Size: Y texts)
                            with st.expander(f"ğŸ” {header}"):
                                st.text(cluster_text)
            else:
                st.info("âš ï¸ Representatives file not found. Run Step 4 (Clustering) first.")
        
        else:
            st.info("âš ï¸ No clustering results yet. Please run Step 4 (Clustering) first.")
            st.markdown("""
            **To get started**:
            1. If you don't have raw data, run Steps 1-2 (Crawlers)
            2. Run Step 3 (Semantic Filter)
            3. Run Step 4 (Clustering)
            4. Results will appear here!
            """)
    
    # ============================================================================
    # Tab 3: å¯è§†åŒ–å±•ç¤º
    # ============================================================================
    with tab3:
        st.header("ğŸ“ˆ Cluster Visualizations")
        
        viz_folder = 'cluster_visualizations'
        
        if os.path.exists(viz_folder):
            images = sorted([f for f in os.listdir(viz_folder) if f.endswith('.png')])
            
            if images:
                # æŒ‰èšç±»æ–¹æ³•åˆ†ç»„
                clustering_methods = {}
                for img in images:
                    if 'HDBSCAN' in img:
                        method = 'HDBSCAN'
                    elif 'KMeans' in img:
                        method = 'KMeans'
                    else:
                        method = 'Other'
                    
                    if method not in clustering_methods:
                        clustering_methods[method] = []
                    clustering_methods[method].append(img)
                
                # æ˜¾ç¤ºæ¯ç§èšç±»æ–¹æ³•çš„å¯è§†åŒ–
                for method, imgs in clustering_methods.items():
                    st.subheader(f"{method} Visualizations")
                    
                    cols = st.columns(min(3, len(imgs)))
                    for i, img in enumerate(imgs):
                        with cols[i % 3]:
                            img_path = os.path.join(viz_folder, img)
                            caption = img.replace('.png', '').replace('_', ' ').upper()
                            st.image(img_path, caption=caption, width=400)
                    
                    st.divider()
                
            else:
                st.info("âš ï¸ No visualization images found. Run Step 5 first.")
        else:
            st.info("âš ï¸ Visualization folder not found. Run Step 5 first.")
        
        # è¿‡æ»¤ç»Ÿè®¡å›¾
        st.subheader("ğŸ” Filtering Statistics")
        filter_viz = 'filter_stats/filtering_analysis.png'
        if os.path.exists(filter_viz):
            st.image(filter_viz, width=800)
        else:
            st.info("âš ï¸ Filtering statistics not found. Run Step 3 first.")
    
    # ============================================================================
    # Tab 4: é¡¹ç›®ä¿¡æ¯
    # ============================================================================
    with tab4:
        st.header("ğŸ“ Project Information")
        
        # é¡¹ç›®æ¦‚è§ˆ
        st.subheader("ğŸ¯ Project Overview")
        st.markdown("""
        **Topic**: Uncertainty Estimation in Machine Learning
        
        **Objective**: Collect and cluster social media discussions about ML uncertainty from LinkedIn and Reddit
        
        **Pipeline**:
        1. Data Collection (LinkedIn + Reddit) â†’ 1,519 posts
        2. Semantic Filtering (threshold=30) â†’ 532 posts (35% retention)
        3. Clustering (HDBSCAN + KMeans) â†’ 13-16 topics
        4. Multi-dimensional Visualization (PCA, t-SNE, UMAP)
        """)
        
        st.divider()
        
        # é…ç½®ä¿¡æ¯
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("âš™ï¸ Filter Configuration")
            st.code("""
THRESHOLD = 30.0
MODEL = 'all-MiniLM-L6-v2'
BATCH_SIZE = 32

Queries:
- uncertainty estimation
- Bayesian neural networks
- epistemic/aleatoric uncertainty
- confidence calibration
- etc. (10 queries total)
            """, language='python')
        
        with col2:
            st.subheader("âš™ï¸ Clustering Configuration")
            st.code("""
HDBSCAN:
  min_cluster_size = 10
  min_samples = 3
  metric = 'euclidean'

KMeans:
  K range = 2-10 (auto-select)
  metric = silhouette score

Embeddings:
  OpenAI text-embedding-3-large
  Dimensions: 3072 â†’ 50 (UMAP)
            """, language='python')
        
        st.divider()
        
        # è¯»å–filter summary
        st.subheader("ğŸ“Š Detailed Statistics")
        summary_file = 'filter_stats/filter_summary.json'
        if os.path.exists(summary_file):
            with open(summary_file, 'r', encoding='utf-8') as f:
                summary = json.load(f)
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("Total Input", summary.get('total_input', 0))
            with col2:
                st.metric("Filtered Output", summary.get('total_filtered', 0))
            with col3:
                st.metric("Retention Rate", f"{summary.get('filter_rate', 0):.1f}%")
            
            with st.expander("ğŸ“„ View Full Summary JSON"):
                st.json(summary)
        else:
            st.info("âš ï¸ Run Step 3 (Semantic Filter) to generate statistics")
        
        st.divider()
        
        # è„šæœ¬åˆ—è¡¨
        st.subheader("ğŸ“œ Available Scripts")
        scripts = {
            'pipeline_0_linkedin_cookie.py': 'LinkedIn Authentication',
            'pipeline_1_crawler_linkedin.py': 'LinkedIn Crawler',
            'pipeline_1_crawler_Reddit.py': 'Reddit Crawler',
            'pipeline_2_semantic_filter.py': 'Semantic Filter',
            'pipeline_3_cluster.py': 'Clustering Analysis',
            'pipeline_3_cluster_print.py': 'Visualization'
        }
        
        for script, desc in scripts.items():
            exists = os.path.exists(script)
            icon = "âœ…" if exists else "âŒ"
            size = f"{os.path.getsize(script)/1024:.1f} KB" if exists else "N/A"
            st.write(f"{icon} **{script}** - {desc} ({size})")

# ============================================================================
# è¿è¡Œ
# ============================================================================
if __name__ == "__main__":
    main()
