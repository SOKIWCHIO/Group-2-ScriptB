[
  {
    "filename": "2509.16953v1.txt",
    "text": "This paper proposes a numerical method using neural networks to solve the path integral problem in quantum mechanics for arbitrary potentials. The method is based on a radial basis function expansion of the interaction term that appears in the Euclidean path integral formalism. By constructing a corresponding multi-layered perceptron-type neural network with exponential nonlinearities in the hidden layer, the original path integral can be approximated by a linear combination of Gaussian path integrals that can be solved analytically. The method has been tested for the double-well potential that includes a quadratic and a quartic term, giving very good, within a few percent agreement between the true and estimated bound state wave functions that are extracted from the propagator at large Euclidean times. The proposed method can also be used to describe potentials that have imaginary parts, which is tested for a simple Gaussian path integral with complex frequencies, where the model uncertainty stays below one percent for both the real and imaginary parts of the propagator."
  },
  {
    "filename": "2509.16958v1.txt",
    "text": "Abductive reasoning - the search for plausible explanations - has long been central to human inquiry, from forensics to medicine and scientific discovery. Yet formal approaches in AI have largely reduced abduction to eliminative search: hypotheses are treated as mutually exclusive, evaluated against consistency constraints or probability updates, and pruned until a single \"best\" explanation remains. This reductionist framing overlooks the way human reasoners sustain multiple explanatory lines in suspension, navigate contradictions, and generate novel syntheses. This paper introduces quantum abduction, a non-classical paradigm that models hypotheses in superposition, allows them to interfere constructively or destructively, and collapses only when coherence with evidence is reached. Grounded in quantum cognition and implemented with modern NLP embeddings and generative AI, the framework supports dynamic synthesis rather than premature elimination. Case studies span historical mysteries (Ludwig II of Bavaria, the \"Monster of Florence\"), literary demonstrations (\"Murder on the Orient Express\"), medical diagnosis, and scientific theory change. Across these domains, quantum abduction proves more faithful to the constructive and multifaceted nature of human reasoning, while offering a pathway toward expressive and transparent AI reasoning systems."
  },
  {
    "filename": "2509.16963v1.txt",
    "text": "Humans and animals can make real-time adjustments to movements by imagining their action outcomes to prevent unanticipated or even catastrophic motion failures in unknown unstructured environments. Action imagination, as a refined sensorimotor strategy, leverages perception-action loops to handle physical interaction-induced uncertainties in perception and system modeling within complex systems. Inspired by the action-awareness capability of animal intelligence, this study proposes an imagination-inspired motion planner (I-MP) framework that specifically enhances robots' action reliability by imagining plausible spatial states for approaching. After topologizing the workspace, I-MP build perception-action loop enabling robots autonomously build contact models. Leveraging fixed-point theory and Hausdorff distance, the planner computes convergent spatial states under interaction characteristics and mission constraints. By homogenously representing multi-dimensional environmental characteristics through work, the robot can approach the imagined spatial states via real-time computation of energy gradients. Consequently, experimental results demonstrate the practicality and robustness of I-MP in complex cluttered environments."
  },
  {
    "filename": "2509.16971v1.txt",
    "text": "Audio deep reasoning is a challenging task that requires expert-level perception, multi-step logical inference, and the integration of contextual knowledge. However, existing models suffer from a gap between audio perception and reasoning abilities due to the lack of training data with explicit reasoning chains and the absence of mechanisms for active exploration and iterative refinement. To address these challenges, we propose AudioGenie-Reasoner (AGR), the first unified training-free multi-agent system that coordinates perception and reasoning over an evolving chain of textual evidence. Our key idea is a paradigm shift that transforms audio deep reasoning into complex text understanding task from a new perspective, thereby unlocking the full potential of large language models. Specifically, the design of AGR mimics the human coarse-to-fine cognitive process. It first transforms the input audio into a coarse text-based document. Then, we design a novel proactive iterative document refinement loop, featuring tool-augmented routes and specialized agents, to continuously search for missing information and augment the evidence chain in a coarse-to-fine manner until sufficient question-related information is gathered for making final predictions. Experimental results show that AGR achieves state-of-the-art (SOTA) performance over existing open-source audio deep reasoning models across various benchmarks. The code will be made publicly available."
  },
  {
    "filename": "2509.16975v1.txt",
    "text": "Automatic mean opinion score (MOS) prediction provides a more perceptual alternative to objective metrics, offering deeper insights into the evaluated models. With the rapid progress of multimodal large language models (MLLMs), their enhanced perceptual and reasoning abilities enable more comprehensive and interpretable audio quality assessment. In this work, we tackle the challenging task of audio editing evaluation and propose the first natural language-based automated evaluation framework built on MLLMs. Our approach introduces two fine-tuning tasks to boost multi-audio understanding, combined with Chain-of-Thought prompting, and lightweight instruction tuning, to enhance step-by-step reasoning. Experiment demonstrate that our framework delivers accurate, interpretable, and text-based editing evaluation, closely aligning with human judgments and objective metrics while substantially improving over baselines. The code and demo are available at https://github.com/NKU-HLT/Eval_Reasoning."
  },
  {
    "filename": "2509.16979v1.txt",
    "text": "Speech intelligibility evaluation for hearing-impaired (HI) listeners is essential for assessing hearing aid performance, traditionally relying on listening tests or intrusive methods like HASPI. However, these methods require clean reference signals, which are often unavailable in real-world conditions, creating a gap between lab-based and real-world assessments. To address this, we propose a non-intrusive intelligibility prediction framework that leverages speech enhancers to provide a parallel enhanced-signal pathway, enabling robust predictions without reference signals. We evaluate three state-of-the-art enhancers and demonstrate that prediction performance depends on the choice of enhancer, with ensembles of strong enhancers yielding the best results. To improve cross-dataset generalization, we introduce a 2-clips augmentation strategy that enhances listener-specific variability, boosting robustness on unseen datasets. Our approach consistently outperforms the non-intrusive baseline, CPC2 Champion across multiple datasets, highlighting the potential of enhancer-guided non-intrusive intelligibility prediction for real-world applications."
  },
  {
    "filename": "2509.16984v1.txt",
    "text": "Prevailing network control strategies, which rely on static shortest-path logic, suffer from catastrophic \"stress concentration\" on critical nodes. This paper introduces the System Relaxation Algorithm (SRA), a new control paradigm inspired by physical relaxation that guides a network toward an emergent equilibrium of load balance. SRA is an interpretable, 'white-box' dynamical system whose behavior is profoundly topology-dependent: in heterogeneous networks, it acts as a proactive performance optimizer, reducing peak centrality by over 80\\% and increasing high-load throughput by more than 45\\%; in homogeneous topologies, its objective intelligently shifts to resilience enhancement. We rigorously prove its global convergence and practical stability using the theory of non-smooth dynamical systems, establishing a predictable paradigm for network governance that intelligently trades off performance and resilience."
  },
  {
    "filename": "2509.16994v1.txt",
    "text": "We introduce a novel deep learning-based audio-visual quality (AVQ) prediction model that leverages internal features from state-of-the-art unimodal predictors. Unlike prior approaches that rely on simple fusion strategies, our model employs a hybrid representation that combines learned Generative Machine Listener (GML) audio features with hand-crafted Video Multimethod Assessment Fusion (VMAF) video features. Attention mechanisms capture cross-modal interactions and intra-modal relationships, yielding context-aware quality representations. A modality relevance estimator quantifies each modality's contribution per content, potentially enabling adaptive bitrate allocation. Experiments demonstrate improved AVQ prediction accuracy and robustness across diverse content types."
  },
  {
    "filename": "2509.16996v1.txt",
    "text": "We numerically study fast Newtonian radiation mediated shocks (RMS - v/c~0.2) in two simplified problems in the context of supernova shock breakout; (1) An RMS traveling in a uniform medium, and (2) an RMS escaping a powerlaw density profile in planar geometry (\\rho~x^n). Both problems were previously solved in the literature assuming a fully ionized plasma medium emitting Bremstrahllung. It was shown that at high shock velocities photons can deviate from local thermal equilibrium (LTE) and reach distributions peaked at many keV.   In this study we incorporate, for the first time, opacity from bound species of heavy elements (solar-like composition) into these two problems, at times drastically augmenting the photon production due to bound-free and bound-bound radiative processes. We use a previously developed hydrodynamically coupled multi-group radiative diffusion code, including inelastic Compton scattering and frequency-dependent opacity from the publicly available TOPS table. Adding a more realistic opacity leads the radiation to maintain LTE at higher velocities in comparison to the fully ionized problem.   In the planar SBO problem this opacity can reduce the emission temperature by half and even an order of magnitude. This result is important for the observation of supernova shock breakout emission. The SED of SN envelope breakout will very likely remain in LTE for explosions in red super giant stars without stellar wind (and part of blue super giant star explosions), making X-Ray observations less likely in these cases by orders of magnitude relative to previous predictions. We provide a semi-analytic description for the SED in the case where LTE is maintained. A correct shock-breakout calculation requires opacity tables that include bound yet highly ionized species, ruling out the use of certain line tables (such as the commonly used Kurucz table)."
  },
  {
    "filename": "2509.17000v1.txt",
    "text": "Large Reasoning Models (LRMs) often suffer from computational inefficiency due to overthinking, where a fixed reasoning budget fails to match the varying complexity of tasks. To address this issue, we propose Adaptive Overclocking, a method that makes the overclocking hyperparameter $\\alpha$ dynamic and context-aware. Our method adjusts reasoning speed in real time through two complementary signals: (1) token-level model uncertainty for fine-grained step-wise control, and (2) input complexity estimation for informed initialization. We implement this approach with three strategies: Uncertainty-Aware Alpha Scheduling (UA-$\\alpha$S), Complexity-Guided Alpha Initialization (CG-$\\alpha$I), and a Hybrid Adaptive Control (HAC) that combines both. Experiments on GSM8K, MATH, and SVAMP show that HAC achieves superior accuracy-latency trade-offs, reducing unnecessary computation on simple problems while allocating more resources to challenging ones. By mitigating overthinking, Adaptive Overclocking enhances both efficiency and overall reasoning performance."
  },
  {
    "filename": "2509.17003v1.txt",
    "text": "In astrophysics, line opacity is a primary source of uncertainty in theoretical calculations of radiative transfer. Much of this uncertainty is dominated by the inability to resolve the width and separation in frequency of sharp atomic transition lines, leading to the common use of approximate frequency-averaged treatments for the lines. In a previous paper, we calculated shock-cooling emission following explosions in core-collapse supernovae using a mult-group radiative transfer code, and compared the results to those of the similar and often used STELLA code from the literature. We found important differences in the spectral energy distribution (SED) resulting from different choices of line opacity treatment. In our code, we used in the emissivity a frequency-binned average of a high-resolution opacity, while in STELLA the often-used Eastman Pinto 1993 (EP93) prescription was employed. In this short letter we revisit this comparison, essentially reproducing STELLA bound-free (photoionization) and bound-bound (line transition) opacities. We show the importance of introducing micro-plasma electron excitation level cutoffs in the equation of state (EOS). We also argue that EP93 is useful for estimating photon mean free-path in the presence of a forest of lines, but that it can underestimate photon production and reprocessing rates (emissivity) by orders of magnitude. To our knowledge, no fully-consistent coarse-frequency solution currently exists for line modeling in these systems. Finally, we describe new features in our updated publicly available high-resolution frequency-dependent opacity table."
  },
  {
    "filename": "2509.17010v1.txt",
    "text": "This paper presents a novel Koopman operator formulation for Euler Lagrangian dynamics that employs an implicit generalized momentum-based state space representation, which decouples a known linear actuation channel from state dependent dynamics and makes the system more amenable to linear Koopman modeling. By leveraging this structural separation, the proposed formulation only requires to learn the unactuated dynamics rather than the complete actuation dependent system, thereby significantly reducing the number of learnable parameters, improving data efficiency, and lowering overall model complexity. In contrast, conventional explicit formulations inherently couple inputs with the state dependent terms in a nonlinear manner, making them more suitable for bilinear Koopman models, which are more computationally expensive to train and deploy. Notably, the proposed scheme enables the formulation of linear models that achieve superior prediction performance compared to conventional bilinear models while remaining substantially more efficient. To realize this framework, we present two neural network architectures that construct Koopman embeddings from actuated or unactuated data, enabling flexible and efficient modeling across different tasks. Robustness is ensured through the integration of a linear Generalized Extended State Observer (GESO), which explicitly estimates disturbances and compensates for them in real time. The combined momentum-based Koopman and GESO framework is validated through comprehensive trajectory tracking simulations and experiments on robotic manipulators, demonstrating superior accuracy, robustness, and learning efficiency relative to state of the art alternatives."
  },
  {
    "filename": "2509.17012v1.txt",
    "text": "Document image quality assessment (DIQA) is an important component for various applications, including optical character recognition (OCR), document restoration, and the evaluation of document image processing systems. In this paper, we introduce a subjective DIQA dataset DIQA-5000. The DIQA-5000 dataset comprises 5,000 document images, generated by applying multiple document enhancement techniques to 500 real-world images with diverse distortions. Each enhanced image was rated by 15 subjects across three rating dimensions: overall quality, sharpness, and color fidelity. Furthermore, we propose a specialized no-reference DIQA model that exploits document layout features to maintain quality perception at reduced resolutions to lower computational cost. Recognizing that image quality is influenced by both low-level and high-level visual features, we designed a feature fusion module to extract and integrate multi-level features from document images. To generate multi-dimensional scores, our model employs independent quality heads for each dimension to predict score distributions, allowing it to learn distinct aspects of document image quality. Experimental results demonstrate that our method outperforms current state-of-the-art general-purpose IQA models on both DIQA-5000 and an additional document image dataset focused on OCR accuracy."
  },
  {
    "filename": "2509.17015v1.txt",
    "text": "High-fidelity ocean forecasting at high spatial and temporal resolution is essential for capturing fine-scale dynamical features, with profound implications for hazard prediction, maritime navigation, and sustainable ocean management. While conventional numerical models can generate sub-daily, eddy-resolving forecasts, they demand substantial computational resources and often struggle to maintain predictive skill at such fine scales. Data-driven models offer a promising alternative with significantly higher computational efficiency; however, most are constrained to daily outputs and show a rapid decay in accuracy when extended to sub-daily timescales. Here, we introduce TianHai, the first-of-its-kind global data-driven 6-hour forecasting model, which delivers predictions at 1/12{\\deg} eddy-resolving resolution with a vertical extent down to 1,500 m. A key feature of TianHai is the integration of atmospheric forcings through FuXi-Atmosphere, a data-driven atmospheric forecasting system, which enables the explicit representation of air-sea coupling effects. Unlike conventional approaches, TianHai does not rely on numerical atmospheric models or external meteorological forecasts, making it a fully data-driven framework for coupled prediction. Benchmark experiments demonstrate that TianHai delivers state-of-the-art performance in forecasting temperature and salinity profiles, zonal and meridional currents, sea surface temperature, and sea level anomalies for lead times ranging from 1 to 10 days."
  },
  {
    "filename": "2509.17018v1.txt",
    "text": "We propose a machine learning (ML) architecture to better capture the dependency of thermodynamic properties on the independent states. When predicting state-dependent thermodynamic properties, ML models need to account for both molecular structure and the thermodynamic state, described by independent variables, typically temperature, pressure, and composition. Modern molecular ML models typically include state information by adding it to molecular fingerprint vectors or by embedding explicit (semi-empirical) thermodynamic relations. Here, we propose to rather split the information processing on the molecular structure and the dependency on states into two separate network channels: a graph neural network and a multilayer perceptron, whose output is combined by a dot product. We refer to our approach as DeepEOSNet, as this idea is based on the DeepONet architecture [Lu et al. (2021), Nat. Mach. Intell.]: instead of operators, we learn state dependencies, with the possibility to predict equation of states (EOS). We investigate the predictive performance of DeepEOSNet by means of three case studies, which include the prediction of vapor pressure as a function of temperature, and mixture molar volume as a function of composition, temperature, and pressure. Our results show superior performance of DeepEOSNet for predicting vapor pressure and comparable performance for predicting mixture molar volume compared to state-of-research graph-based thermodynamic prediction models from our earlier works. In fact, we see large potential of DeepEOSNet in cases where data is sparse in the state domain and the output function is structurally similar across different molecules. The concept of DeepEOSNet can easily be transferred to other ML architectures in molecular context, and thus provides a viable option for property prediction."
  },
  {
    "filename": "2509.17021v1.txt",
    "text": "Recent advancements in text-to-speech (TTS) have shown that language model (LM) based systems offer competitive performance compared to traditional approaches. However, in training, TTS models use ground-truth (GT) tokens as prefixes to predict the next token, while in inference these tokens are not available, a gap between training and inference that is often neglected. In this study, we propose a prompt-guided hybrid training scheme to mitigate exposure bias in popular LM-based TTS systems. Our core idea is to adopt a hybrid training paradigm that combines teacher forcing with free running, thereby introducing self-generated tokens into the training process. This makes the training mode more consistent with inference, reducing the training-inference gap. In addition, we incorporate an EOS prediction mechanism during training to detect incorrect sequence termination and adaptively control the free running process. Experimental results provide a comprehensive evaluation of the impact of exposure bias on LM-based TTS, and demonstrate that our method effectively narrows the training-inference gap, thereby improving the quality of synthesized long-form speech."
  },
  {
    "filename": "2509.17033v1.txt",
    "text": "Recent studies suggest that dark matter could take the form of a Bose-Einstein condensate (BEC), a possibility motivated by anomalies in galactic rotation curves and the missing mass problem in galaxy clusters. We investigate the astrophysical properties of BEC dark matter halos and their potential observational signatures distinguishing them from alternative models. In this framework, dark matter behaves as a self-gravitating Newtonian fluid with a polytropic equation of state of index $n=1$. We derive analytic expressions for the mass distribution, gravitational potential, and dynamical profiles such as the density slope and tangential velocity. The lensing behavior of BEC halos is analyzed, yielding a general series representation of the projected surface density that enables precise predictions for deflection angles, lensing potentials, and magnifications. Finally, halo equilibrium and stability are examined via the scalar and tensor virial theorems, leading to perturbation equations that describe their response to small disturbances. Together, these results provide a unified framework linking the microscopic physics of condensate dark matter to macroscopic halo observables."
  },
  {
    "filename": "2509.17041v1.txt",
    "text": "Behavioural differences across organisms, whether healthy or pathological, are closely tied to the structure of their neural circuits. Yet, the fine-scale synaptic changes that give rise to these variations remain poorly understood, in part due to persistent challenges in detecting synapses reliably and at scale. Volume electron microscopy (EM) offers the resolution required to capture synaptic architecture, but automated detection remains difficult due to sparse annotations, morphological variability, and cross-dataset domain shifts. To address this, we make three key contributions. First, we curate a diverse EM benchmark spanning four datasets across two invertebrate species: adult and larval Drosophila melanogaster, and Megaphragma viggianii (micro-WASP). Second, we propose SimpSyn, a single-stage Residual U-Net trained to predict dual-channel spherical masks around pre- and post-synaptic sites, designed to prioritize training and inference speeds and annotation efficiency over architectural complexity. Third, we benchmark SimpSyn against Buhmann et al.'s Synful [1], a state-of-the-art multi-task model that jointly infers synaptic pairs. Despite its simplicity, SimpSyn consistently outperforms Synful in F1-score across all volumes for synaptic site detection. While generalization across datasets remains limited, SimpSyn achieves competitive performance when trained on the combined cohort. Finally, ablations reveal that simple post-processing strategies - such as local peak detection and distance-based filtering - yield strong performance without complex test-time heuristics. Taken together, our results suggest that lightweight models, when aligned with task structure, offer a practical and scalable solution for synapse detection in large-scale connectomic pipelines."
  },
  {
    "filename": "2509.17043v1.txt",
    "text": "The quantum geometric tensor (QGT) fundamentally encodes the geometry and topology of quantum states in both Hermitian and non-Hermitian regimes. While adiabatic perturbation theory links its real part (quantum metric) and imaginary part (Berry curvature) to energy fluctuations and generalized forces, respectively, in Hermitian systems, direct measurement of the QGT, which defined using both left and right eigenstates of non-Hermitian Hamiltonian, remains challenging. Here we develop two quantum simulation schemes to directly extract all components of the QGT in pseudo-Hermitian systems with real spectra. Each scheme independently determines the complete QGT using generalized expectation values of either the energy fluctuation operator or the generalized force operator with respect to two time-evolved states prepared through distinct nonadiabatic evolutions, thereby establishing two self-contained measurement protocols. We illustrate the validity of these schemes on two $q$-deformed 2-band models: one with nontrivial topology, and the other with a nonvanishing off-diagonal quantum metric. Numerical simulations show that both schemes achieve high-fidelity agreement with theoretical predictions for measuring the QGT of both models, and successfully capture the topological phase transition of the first model using Chern numbers calculated from Berry curvatures. This work provides a framework for extending dynamical measurement schemes from Hermitian to pseudo-Hermitian systems with real spectra."
  },
  {
    "filename": "2509.17047v1.txt",
    "text": "Contemporary theories model language processing as integrating both top-down expectations and bottom-up inputs. One major prediction of such models is that the quality of the bottom-up inputs modulates ease of processing -- noisy inputs should lead to difficult and effortful comprehension. We test this prediction in the domain of reading. First, we propose an information-theoretic operationalization for the \"quality\" of bottom-up information as the mutual information (MI) between visual information and word identity. We formalize this prediction in a mathematical model of reading as a Bayesian update. Second, we test our operationalization by comparing participants' reading times in conditions where words' information quality has been reduced, either by occluding their top or bottom half, with full words. We collect data in English and Chinese. We then use multimodal language models to estimate the mutual information between visual inputs and words. We use these data to estimate the specific effect of reduced information quality on reading times. Finally, we compare how information is distributed across visual forms. In English and Chinese, the upper half contains more information about word identity than the lower half. However, the asymmetry is more pronounced in English, a pattern which is reflected in the reading times."
  },
  {
    "filename": "2509.17053v1.txt",
    "text": "Contact-rich manipulation is crucial for robots to perform tasks requiring precise force control, such as insertion, assembly, and in-hand manipulation. However, most imitation learning (IL) policies remain position-centric and lack explicit force awareness, and adding force/torque sensors to collaborative robot arms is often costly and requires additional hardware design. To overcome these issues, we propose FILIC, a Force-guided Imitation Learning framework with impedance torque control. FILIC integrates a Transformer-based IL policy with an impedance controller in a dual-loop structure, enabling compliant force-informed, force-executed manipulation. For robots without force/torque sensors, we introduce a cost-effective end-effector force estimator using joint torque measurements through analytical Jacobian-based inversion while compensating with model-predicted torques from a digital twin. We also design complementary force feedback frameworks via handheld haptics and VR visualization to improve demonstration quality. Experiments show that FILIC significantly outperforms vision-only and joint-torque-based methods, achieving safer, more compliant, and adaptable contact-rich manipulation. Our code can be found in https://github.com/TATP-233/FILIC."
  },
  {
    "filename": "2509.17058v1.txt",
    "text": "Reachability analysis is a key formal verification technique for ensuring the safety of modern cyber physical systems subject to uncertainties in measurements, system models (parameters), and inputs. Classical model-based approaches rely on accurate prior knowledge of system dynamics, which may not always be available or reliable. To address this, we present a data-driven reachability analysis framework that computes over-approximations of reachable sets directly from online state measurements. The method estimates time-varying unknown models using an Exponentially Forgetting Zonotopic Recursive Least Squares (EF ZRLS) method, which processes data corrupted by bounded noise. Specifically, a time-varying set of models that contains the true model of the system is estimated recursively, and then used to compute the forward reachable sets under process noise and uncertain inputs. Our approach applies to both discrete-time Linear Time Varying (LTV) and nonlinear Lipschitz systems. Compared to existing techniques, it produces less conservative reachable set over approximations, remains robust under slowly varying dynamics, and operates solely on real-time data without requiring any pre-recorded offline experiments. Numerical simulations and real-world experiments validate the effectiveness and practical applicability of the proposed algorithms."
  },
  {
    "filename": "2509.17061v1.txt",
    "text": "The nature of turbulence at sub-electron scales has remained an open question, central to understanding how electrons are heated in the solar wind. This is primarily because spacecraft measurements have been limited to magnetic field fluctuations alone. We resolve this by deriving new high-resolution density fluctuations from spacecraft potential measurements of Parker Solar Probe resolving scales smaller than the electron gyro-radius ($\\rho_e$). A systematic comparison of the density and magnetic spectra shows that both steepen near the electron scales. Notably, the density spectrum exhibits slopes close to $-10/3$, while the magnetic spectrum becomes consistently steeper than the density spectrum at scales smaller than $\\rho_e$, indicating that the turbulence becomes electrostatic. These results are consistent with theoretical predictions of an electron entropy cascade, which may explain the irreversible dissipation of turbulent energy at sub-$\\rho_e$ scales."
  },
  {
    "filename": "2509.17065v1.txt",
    "text": "Echocardiography is a vital non-invasive modality for cardiac assessment, with left ventricular ejection fraction (LVEF) serving as a key indicator of heart function. Existing LVEF estimation methods depend on large-scale annotated video datasets, which are costly and limit adaptability across various clinical settings. Recent vision-language models for echocardiography, such as EchoCLIP, apply image-to-text pretraining but fail to capture crucial temporal dynamics and localized cardiac structures essential for accurate diagnosis. To address these challenges, we propose CardiacCLIP, a video-based framework that enhances LVEF prediction through attention-based frame aggregation and multi-resolution input scaling. Specifically, we introduce MFL (Multi Frame Learning), a novel attention-based mechanism for selectively fusing informative frames, and EchoZoom, a multi-scale feature extraction strategy that refines spatial representations of cardiac structures. As a novel adaptation of CLIP models for few-shot echocardiogram video analysis, our approach significantly improves diagnostic accuracy, reducing MAE by 2.07 on the EchoNet-Dynamic dataset under 1-shot setting. The code is available at https://github.com/xmed-lab/CardiacCLIP."
  },
  {
    "filename": "2509.17066v1.txt",
    "text": "Next point-of-interest (POI) recommendation predicts a user's next destination from historical movements. Traditional models require intensive training, while LLMs offer flexible and generalizable zero-shot solutions but often generate generic or geographically irrelevant results due to missing trajectory and spatial context. To address these issues, we propose RALLM-POI, a framework that couples LLMs with retrieval-augmented generation and self-rectification. We first propose a Historical Trajectory Retriever (HTR) that retrieves relevant past trajectories to serve as contextual references, which are then reranked by a Geographical Distance Reranker (GDR) for prioritizing spatially relevant trajectories. Lastly, an Agentic LLM Rectifier (ALR) is designed to refine outputs through self-reflection. Without additional training, RALLM-POI achieves substantial accuracy gains across three real-world Foursquare datasets, outperforming both conventional and LLM-based baselines. Code is released at https://github.com/LKRcrocodile/RALLM-POI."
  },
  {
    "filename": "2509.17068v1.txt",
    "text": "Long-term trajectory anomaly detection is a challenging problem due to the diversity and complex spatiotemporal dependencies in trajectory data. Existing trajectory anomaly detection methods fail to simultaneously consider both the high-level intentions of agents as well as the low-level details of the agent's navigation when analysing an agent's trajectories. This limits their ability to capture the full diversity of normal trajectories. In this paper, we propose an unsupervised trajectory anomaly detection method named Intention-aware Hierarchical Diffusion model (IHiD), which detects anomalies through both high-level intent evaluation and low-level sub-trajectory analysis. Our approach leverages Inverse Q Learning as the high-level model to assess whether a selected subgoal aligns with an agent's intention based on predicted Q-values. Meanwhile, a diffusion model serves as the low-level model to generate sub-trajectories conditioned on subgoal information, with anomaly detection based on reconstruction error. By integrating both models, IHiD effectively utilises subgoal transition knowledge and is designed to capture the diverse distribution of normal trajectories. Our experiments show that the proposed method IHiD achieves up to 30.2% improvement in anomaly detection performance in terms of F1 score over state-of-the-art baselines."
  },
  {
    "filename": "2509.17080v1.txt",
    "text": "Accurate trajectory prediction and motion planning are crucial for autonomous driving systems to navigate safely in complex, interactive environments characterized by multimodal uncertainties. However, current generation-then-evaluation frameworks typically construct multiple plausible trajectory hypotheses but ultimately adopt a single most likely outcome, leading to overconfident decisions and a lack of fallback strategies that are vital for safety in rare but critical scenarios. Moreover, the usual decoupling of prediction and planning modules could result in socially inconsistent or unrealistic joint trajectories, especially in highly interactive traffic. To address these challenges, we propose a contingency-aware diffusion planner (CoPlanner), a unified framework that jointly models multi-agent interactive trajectory generation and contingency-aware motion planning. Specifically, the pivot-conditioned diffusion mechanism anchors trajectory sampling on a validated, shared short-term segment to preserve temporal consistency, while stochastically generating diverse long-horizon branches that capture multimodal motion evolutions. In parallel, we design a contingency-aware multi-scenario scoring strategy that evaluates candidate ego trajectories across multiple plausible long-horizon evolution scenarios, balancing safety, progress, and comfort. This integrated design preserves feasible fallback options and enhances robustness under uncertainty, leading to more realistic interaction-aware planning. Extensive closed-loop experiments on the nuPlan benchmark demonstrate that CoPlanner consistently surpasses state-of-the-art methods on both Val14 and Test14 datasets, achieving significant improvements in safety and comfort under both reactive and non-reactive settings. Code and model will be made publicly available upon acceptance."
  },
  {
    "filename": "2509.17082v1.txt",
    "text": "The gyromagnetic ratio and Pauli form factor are important quantities that characterize the electric and magnetic moment distribution of a particle. The experimentally measured value for the gyromagnetic ratio or the spin g-factor of an electron in a vacuum agrees remarkably well with theoretical predictions, making it one of the biggest successes of quantum field theory. However, these factors may get modified compared to their vacuum counterparts due to the interaction present in the system. Motivating from that, in this article, we investigate the effects of interactions within the framework of $2+1$ dimensional Proca quantum electrodynamics. We demonstrate how the g-factor and Pauli form factors change with the electron's Fermi velocity and the mass of the vector fields within Proca quantum electrodynamics."
  },
  {
    "filename": "2509.17083v1.txt",
    "text": "Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative to NeRF-based approaches, enabling real-time, high-quality novel view synthesis through explicit, optimizable 3D Gaussians. However, 3DGS suffers from significant memory overhead due to its reliance on per-Gaussian parameters to model view-dependent effects and anisotropic shapes. While recent works propose compressing 3DGS with neural fields, these methods struggle to capture high-frequency spatial variations in Gaussian properties, leading to degraded reconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a novel scene representation that combines the strengths of explicit Gaussians and neural fields. HyRF decomposes the scene into (1) a compact set of explicit Gaussians storing only critical high-frequency parameters and (2) grid-based neural fields that predict remaining properties. To enhance representational capacity, we introduce a decoupled neural field architecture, separately modeling geometry (scale, opacity, rotation) and view-dependent color. Additionally, we propose a hybrid rendering scheme that composites Gaussian splatting with a neural field-predicted background, addressing limitations in distant scene representation. Experiments demonstrate that HyRF achieves state-of-the-art rendering quality while reducing model size by over 20 times compared to 3DGS and maintaining real-time performance. Our project page is available at https://wzpscott.github.io/hyrf/."
  },
  {
    "filename": "2509.17089v1.txt",
    "text": "Subsurface lithological heterogeneity presents challenges for traditional geophysical methods, particularly in resolving nonlinear electrical resistivity and induced polarization (IP) relationships. This study introduces a data-driven machine learning and deep learning (ML/DL) framework for predicting 2D IP chargeability models from resistivity, depth, and station distance, reducing reliance on field IP surveys. The framework integrates ensemble regressors with a one-dimensional convolutional neural network (1D CNN) enhanced by global average pooling. Among the tested models, CatBoost achieved the highest prediction accuracy (R^2 = 0.942 training, 0.945 testing), closely followed by random forest, while the stacked ML/DL ensemble further improved performance, particularly for complex resistivity-IP behaviors. Overall accuracy ranged from R^2 = 0.882 to 0.947 with RMSE < 0.04. Integration with k-means clustering enhanced lithological discrimination, effectively delineating sandy silt, silty sand, and weathered granite influenced by saturation, clay content, and fracturing. This scalable approach provides a rapid solution for subsurface modeling in exploration, geotechnical, and environmental applications."
  },
  {
    "filename": "2509.17093v1.txt",
    "text": "We build a deep neural network model to predict meson widths from quantum numbers and masses based on the Transformer architecture. A Gaussian Monte-Carlo data enhancement method is adopted to enhance the meson data by considering the experimental errors, which significantly increase the data samples and improve the robustness and generalization performance of the model. With the meson widths ranging from $\\sim10^{-14}$ to 625 MeV, the relative errors of the predictions behave $0.07\\%$, $1.0\\%$, and $0.14\\%$ in the training set, the test set, and all the data, respectively. The width predictions are presented for the currently discovered mesons and some theoretically predicted states. We also use the model as a probe to study the quantum numbers and inner structures for some undetermined states. Furthermore, this data-driven model is investigated to show well charge conjugation symmetry and approximate isospin symmetry, which is consistent with the physical phenomena. The results indicate that the deep neural network has powerful learning and inference abilities to describe and explore the hadron structures and the complicated interactions in particle physics."
  },
  {
    "filename": "2509.17095v1.txt",
    "text": "The integration of solar power has been increasing as the green energy transition rolls out. The penetration of solar power challenges the grid stability and energy scheduling, due to its intermittent energy generation. Accurate and near real-time solar power prediction is of critical importance to tolerant and support the permeation of distributed and volatile solar power production in the energy system. In this paper, we propose a deep-learning based ultra-short-term solar power prediction with data reconstruction. We decompose the data for the prediction to facilitate extensive exploration of the spatial and temporal dependencies within the data. Particularly, we reconstruct the data into low- and high-frequency components, using ensemble empirical model decomposition with adaptive noise (CEEMDAN). We integrate meteorological data with those two components, and employ deep-learning models to capture long- and short-term dependencies towards the target prediction period. In this way, we excessively exploit the features in historical data in predicting a ultra-short-term solar power production. Furthermore, as ultra-short-term prediction is vulnerable to local optima, we modify the optimization in our deep-learning training by penalizing long prediction intervals. Numerical experiments with diverse settings demonstrate that, compared to baseline models, the proposed method achieves improved generalization in data reconstruction and higher prediction accuracy for ultra-short-term solar power production."
  },
  {
    "filename": "2509.17098v1.txt",
    "text": "Uncertainty estimation has been widely studied in medical image segmentation as a tool to provide reliability, particularly in deep learning approaches. However, previous methods generally lack effective supervision in uncertainty estimation, leading to low interpretability and robustness of the predictions. In this work, we propose a self-supervised approach to guide the learning of uncertainty. Specifically, we introduce three principles about the relationships between the uncertainty and the image gradients around boundaries and noise. Based on these principles, two uncertainty supervision losses are designed. These losses enhance the alignment between model predictions and human interpretation. Accordingly, we introduce novel quantitative metrics for evaluating the interpretability and robustness of uncertainty. Experimental results demonstrate that compared to state-of-the-art approaches, the proposed method can achieve competitive segmentation performance and superior results in out-of-distribution (OOD) scenarios while significantly improving the interpretability and robustness of uncertainty estimation. Code is available via https://github.com/suiannaius/SURE."
  },
  {
    "filename": "2509.17100v1.txt",
    "text": "Advances in artificial intelligence (AI) for surgical quality assessment promise to democratize access to expertise, with applications in training, guidance, and accreditation. This study presents the SAGES Critical View of Safety (CVS) Challenge, the first AI competition organized by a surgical society, using the CVS in laparoscopic cholecystectomy, a universally recommended yet inconsistently performed safety step, as an exemplar of surgical quality assessment. A global collaboration across 54 institutions in 24 countries engaged hundreds of clinicians and engineers to curate 1,000 videos annotated by 20 surgical experts according to a consensus-validated protocol. The challenge addressed key barriers to real-world deployment in surgery, including achieving high performance, capturing uncertainty in subjective assessment, and ensuring robustness to clinical variability. To enable this scale of effort, we developed EndoGlacier, a framework for managing large, heterogeneous surgical video and multi-annotator workflows. Thirteen international teams participated, achieving up to a 17\\% relative gain in assessment performance, over 80\\% reduction in calibration error, and a 17\\% relative improvement in robustness over the state-of-the-art. Analysis of results highlighted methodological trends linked to model performance, providing guidance for future research toward robust, clinically deployable AI for surgical quality assessment."
  },
  {
    "filename": "2509.17102v1.txt",
    "text": "The $^{12}\\mathrm{C}(\\alpha,\\gamma)^{16}\\mathrm{O}$ reaction governs the carbon-to-oxygen ratio set during helium burning, shaping white-dwarf structure and Type~Ia supernova yields. At the astrophysical energy $E \\approx 300~\\mathrm{keV}$, the cross section is controlled by the subthreshold $1^{-}$ (7.12~MeV) and $2^{+}$ (6.92~MeV) states, whose contributions depend on their asymptotic normalization coefficients (ANCs) $C_{1}$ and $C_{2}$, respectively. We perform a Bayesian analysis of the $S_{E1}(300~\\mathrm{keV})$ and $S_{E2}(300~\\mathrm{keV})$ factors using calibrated $R$-matrix mappings and experimental ANC constraints for the $1^{-}$, $2^{+}$, and $0^{+}$ ground state. For $S_{E1}(300~\\mathrm{keV})$, flat prior on the $1^{-}$ ANC lead to broad posterior with $68\\%$ credible interval spanning $ [71.4,\\,93.4]$~keV\\,b, while Gaussian priors concentrate weight near the reported ANC values and yield narrower posteriors. For $S_{E2}(300~\\mathrm{keV})$, the analysis includes the interference of the radiative transition through the subthreshold resonance with the direct capture to the ground-state, which depends on the ground-state ANC $C_{0}$, giving broad posterior with $68\\%$ credible interval spanning $[30.7,\\,50.5]$~keV\\,b. The Gaussian priors centered near anchor values. The resulting posteriors quantify both correlations and uncertainties: despite incorporating the published ANC constraints, the $68\\%$ intervals remain broad, showing that present ANC determinations do not yet reduce the astrophysical uncertainty. Overall, the Bayesian framework provides statistically robust posteriors for $S_{E1}(300~\\mathrm{keV})$ and $S_{E2}(300~\\mathrm{keV})$, improving the reliability of extrapolations for stellar modeling and nucleosynthesis."
  },
  {
    "filename": "2509.17119v1.txt",
    "text": "To address the intermittency of renewable energy source (RES) generation, scenario forecasting offers a series of stochastic realizations for predictive objects with superior flexibility and direct views. Based on a long time-series perspective, this paper explores uncertainties in the realms of renewable power and deep learning. Then, an uncertainty-aware model is meticulously designed for renewable scenario forecasting, which leverages an attention mechanism and generative adversarial networks (GANs) to precisely capture complex spatial-temporal dynamics. To improve the interpretability of uncertain behavior in RES generation, Bayesian deep learning and adaptive instance normalization (AdaIN) are incorporated to simulate typical patterns and variations. Additionally, the integration of meteorological information, forecasts, and historical trajectories in the processing layer improves the synergistic forecasting capability for multiscale periodic regularities. Numerical experiments and case analyses demonstrate that the proposed approach provides an appropriate interpretation for renewable uncertainty representation, including both aleatoric and epistemic uncertainties, and shows superior performance over state-of-the-art methods."
  },
  {
    "filename": "2509.17125v1.txt",
    "text": "Relational object rearrangement (ROR) tasks (e.g., insert flower to vase) require a robot to manipulate objects with precise semantic and geometric reasoning. Existing approaches either rely on pre-collected demonstrations that struggle to capture complex geometric constraints or generate goal-state observations to capture semantic and geometric knowledge, but fail to explicitly couple object transformation with action prediction, resulting in errors due to generative noise. To address these limitations, we propose Imagine2Act, a 3D imitation-learning framework that incorporates semantic and geometric constraints of objects into policy learning to tackle high-precision manipulation tasks. We first generate imagined goal images conditioned on language instructions and reconstruct corresponding 3D point clouds to provide robust semantic and geometric priors. These imagined goal point clouds serve as additional inputs to the policy model, while an object-action consistency strategy with soft pose supervision explicitly aligns predicted end-effector motion with generated object transformation. This design enables Imagine2Act to reason about semantic and geometric relationships between objects and predict accurate actions across diverse tasks. Experiments in both simulation and the real world demonstrate that Imagine2Act outperforms previous state-of-the-art policies. More visualizations can be found at https://sites.google.com/view/imagine2act."
  },
  {
    "filename": "2509.17128v1.txt",
    "text": "Identifying multivariate dependencies in high-dimensional data is an important problem in large-scale inference. This problem has motivated recent advances in mining (partial) correlations, which focus on the challenging ultra-high dimensional setting where the sample size, n, is fixed, while the number of features, p, grows without bound. The state-of-the-art method for partial correlation screening can lead to undesirable results. This paper introduces a novel principled framework for partial correlation screening with error control (PARSEC), which leverages the connection between partial correlations and regression coefficients. We establish the inferential properties of PARSEC when n is fixed and p grows super-exponentially. First, we provide \"fixed-n-large-p\" asymptotic expressions for the familywise error rate (FWER) and k-FWER. Equally importantly, our analysis leads to a novel discovery which permits the calculation of exact marginal p-values for controlling the false discovery rate (FDR), and also the positive FDR (pFDR). To our knowledge, no other competing approach in the \"fixed-n large-p\" setting allows for error control across the spectrum of multiple hypothesis testing metrics. We establish the computational complexity of PARSEC and rigorously demonstrate its scalability to the large p setting. The theory and methods are successfully validated on simulated and real data, and PARSEC is shown to outperform the current state-of-the-art."
  },
  {
    "filename": "2509.17145v1.txt",
    "text": "Predictive Process Monitoring (PPM) aims to forecast the future behavior of ongoing process instances using historical event data, enabling proactive decision-making. While recent advances rely heavily on deep learning models such as LSTMs and Transformers, their high computational cost hinders practical adoption. Prior work has explored data reduction techniques and alternative feature encodings, but the effect of simplifying model architectures themselves remains underexplored. In this paper, we analyze how reducing model complexity, both in terms of parameter count and architectural depth, impacts predictive performance, using two established PPM approaches. Across five diverse event logs, we show that shrinking the Transformer model by 85% results in only a 2-3% drop in performance across various PPM tasks, while the LSTM proves slightly more sensitive, particularly for waiting time prediction. Overall, our findings suggest that substantial model simplification can preserve predictive accuracy, paving the way for more efficient and scalable PPM solutions."
  },
  {
    "filename": "2509.17146v1.txt",
    "text": "One prominent feature of solar cycle is its irregular variation in its cycle strength, making it challenging to predict the amplitude of the next cycle. Studies show that fluctuations and nonlinearity in generating poloidal field throughout the decay and dispersal of tilted sunspots produce variation in the solar cycle. The flux, latitudinal position, and tilt angle of sunspots are the primary parameters that determine the polar field and, thus, the next solar cycle strength. By analysing the observed sunspots and polar field proxy, we show that the nonlinearity in the poloidal field generation becomes important for strong cycles. Except for strong cycles, we can reasonably predict the polar field at the end of the cycle (and thus the next cycle strength) using the total sunspot area alone. Combining the mean tilt angle and latitude positions with the sunspot area, we can predict the polar field of Cycles 15 -- 24 (or the amplitude of sunspot Cycles 16-25) with reasonable accuracy except for Cycle 23 for which the average tilt angle cannot predict the polar field. For Cycles 15--22, we show that the average tilt angle variation dominates over the latitude variation in determining the polar field of a cycle. In particular, the reduction of tilt in Cycle 19 was the primary cause of the following weak cycle (Cycle 20). Thus, we conclude that tilt quenching is essential in regulating the solar cycle strength in the solar dynamo."
  },
  {
    "filename": "2509.17147v1.txt",
    "text": "Navigation through narrow passages during colony relocation by the tandem-running ants, $\\textit{Diacamma}$ $\\textit{indicum}$, is a tour de force of biological traffic coordination. Even on one-lane paths, the ants tactfully manage a bidirectional flow: Informed individuals (termed leaders) guide nest-mates (termed followers) from a suboptimal nest to a new optimal nest, and then return to recruit additional followers. We propose that encounters between the ants moving in opposite directions can be modelled within the framework of game theory leading to an understanding of the mechanism behind observed behaviours. Our experiments reveal that, upon encountering a tandem pair (a leader and its follower) on a narrow path, the returning leader reverses her direction and proceeds toward the new nest again as if she becomes the leader guiding a follower. This observed behaviour is consistent with game-theoretic predictions, provided the assumption of perfect rationality is relaxed in favour of bounded rationality -- specifically, procedural rationality. In other words, the experimental outcomes are consistent with sampling equilibrium but not with Nash equilibrium. Our work, which strives to induct the essence of behavioural game theory into the world of ants, is first ever report of realizing sampling equilibrium in scenarios not involving human players."
  },
  {
    "filename": "2509.17153v1.txt",
    "text": "We present Flow-Induced Diagonal Gaussian Processes (FiD-GP), a compression framework that incorporates a compact inducing weight matrix to project a neural network's weight uncertainty into a lower-dimensional subspace. Critically, FiD-GP relies on normalising-flow priors and spectral regularisations to augment its expressiveness and align the inducing subspace with feature-gradient geometry through a numerically stable projection mechanism objective. Furthermore, we demonstrate how the prediction framework in FiD-GP can help to design a single-pass projection for Out-of-Distribution (OoD) detection. Our analysis shows that FiD-GP improves uncertainty estimation ability on various tasks compared with SVGP-based baselines, satisfies tight spectral residual bounds with theoretically guaranteed OoD detection, and significantly compresses the neural network's storage requirements at the cost of increased inference computation dependent on the number of inducing weights employed. Specifically, in a comprehensive empirical study spanning regression, image classification, semantic segmentation, and out-of-distribution detection benchmarks, it cuts Bayesian training cost by several orders of magnitude, compresses parameters by roughly 51%, reduces model size by about 75%, and matches state-of-the-art accuracy and uncertainty estimation."
  },
  {
    "filename": "2509.17154v1.txt",
    "text": "Hamiltonian dynamics describe a wide range of physical systems. As such, data-driven simulations of Hamiltonian systems are important for many scientific and engineering problems. In this work, we propose kernel-based methods for identifying and forecasting Hamiltonian systems directly from data. We present two approaches: a two-step method that reconstructs trajectories before learning the Hamiltonian, and a one-step method that jointly infers both. Across several benchmark systems, including mass-spring dynamics, a nonlinear pendulum, and the Henon-Heiles system, we demonstrate that our framework achieves accurate, data-efficient predictions and outperforms two-step kernel-based baselines, particularly in scarce-data regimes, while preserving the conservation properties of Hamiltonian dynamics. Moreover, our methodology provides theoretical a priori error estimates, ensuring reliability of the learned models. We also provide a more general, problem-agnostic numerical framework that goes beyond Hamiltonian systems and can be used for data-driven learning of arbitrary dynamical systems."
  },
  {
    "filename": "2509.17163v1.txt",
    "text": "Quantum mechanics predicts deviations from exponential decay at short and long times, yet experimental evidence is limited. We report a power-law tail after $\\sim$10 lifetimes in erythrosine~B fluorescence, confirmed by two detectors probing distinct bands but yielding different power coefficients. The data match a divergent but normalizable spectral density, and theory predicts oscillations as a future test. A novel and general result is that in multichannel QM (and QFT) decay the lifetime is universal, but the late-time deviations exhibit a sizeable time window in which they are channel (or band) dependent, a feature consistent with our data."
  },
  {
    "filename": "2509.17165v1.txt",
    "text": "Time series data is a prevalent form of data found in various fields. It consists of a series of measurements taken over time. Forecasting is a crucial application of time series models, where future values are predicted based on historical data. Accurate forecasting is essential for making well-informed decisions across industries. When it comes to electric vehicles (EVs), precise predictions play a key role in planning infrastructure development, load balancing, and energy management. This study introduces a BI-LSTM embedding denoising autoencoder model (BDM) designed to address time series problems, focusing on short-term EV charging load prediction. The performance of the proposed model is evaluated by comparing it with benchmark models like Transformer, CNN, RNN, LSTM, and GRU. Based on the results of the study, the proposed model outperforms the benchmark models in four of the five-time steps, demonstrating its effectiveness for time series forecasting. This research makes a significant contribution to enhancing time series forecasting, thereby improving decision-making processes."
  },
  {
    "filename": "2509.17169v1.txt",
    "text": "Many potential direct imaging candidates suffer from large orbital period uncertainties, leading to challenges in accurate predictions of future orbital positions and imprecise direct imaging measurements of planetary parameters. To improve the precision in orbital properties, precursor radial velocity (RV) follow-up observations for selected candidates are essential. This study examines the impact of three variables on the orbital period uncertainties of long-period giant planets: the number of future observations, the temporal gap between past and future data, and the temporal coverage of upcoming observations. Our simulations indicate that the orbital phases at which future RV observations are acquired play a significant role in reducing period uncertainties. Additionally, observing too frequently within a given time frame adds limited value to the program once a certain number of observations has been achieved. The temporal gap proves to be the most important factor when there is no strict end time to the observing campaign. However, if a strict end time is set, starting observations earlier yields improved reductions in orbital period uncertainty. These insights offer practical guidance for planning efficient RV follow-up campaigns to maximize the science yield of future space-based direct imaging missions."
  },
  {
    "filename": "2509.17172v1.txt",
    "text": "The automated prediction of facial beauty is a benchmark task in affective computing that requires a sophisticated understanding of both local aesthetic details (e.g., skin texture) and global facial harmony (e.g., symmetry, proportions). Existing models, based on either Convolutional Neural Networks (CNNs) or Vision Transformers (ViTs), exhibit inherent architectural biases that limit their performance; CNNs excel at local feature extraction but struggle with long-range dependencies, while ViTs model global relationships at a significant computational cost. This paper introduces the \\textbf{Mamba-Diffusion Network (MD-Net)}, a novel dual-stream architecture that resolves this trade-off by delegating specialized roles to state-of-the-art models. The first stream leverages a frozen U-Net encoder from a pre-trained latent diffusion model, providing a powerful generative prior for fine-grained aesthetic qualities. The second stream employs a Vision Mamba (Vim), a modern state-space model, to efficiently capture global facial structure with linear-time complexity. By synergistically integrating these complementary representations through a cross-attention mechanism, MD-Net creates a holistic and nuanced feature space for prediction. Evaluated on the SCUT-FBP5500 benchmark, MD-Net sets a new state-of-the-art, achieving a Pearson Correlation of \\textbf{0.9235} and demonstrating the significant potential of hybrid architectures that fuse generative and sequential modeling paradigms for complex visual assessment tasks."
  },
  {
    "filename": "2509.17174v1.txt",
    "text": "Inferring synaptic connectivity from neural population activity is a fundamental challenge in computational neuroscience, complicated by partial observability and mismatches between inference models and true circuit dynamics. In this study, we propose a graph-based neural inference model that simultaneously predicts neural activity and infers latent connectivity by modeling neurons as interacting nodes in a graph. The architecture features two distinct modules: one for learning structural connectivity and another for predicting future spiking activity via a graph neural network (GNN). Our model accommodates unobserved neurons through auxiliary nodes, allowing for inference in partially observed circuits. We evaluate this approach using synthetic data from ring attractor networks and real spike recordings from head direction cells in mice. Across a wide range of conditions, including varying recurrent connectivity, external inputs, and incomplete observations, our model consistently outperforms standard baselines, resolving spurious correlations more effectively and recovering accurate weight profiles. When applied to real data, the inferred connectivity aligns with theoretical predictions of continuous attractor models. These results highlight the potential of GNN-based models to infer latent neural circuitry through self-supervised structure learning, while leveraging the spike prediction task to flexibly link connectivity and dynamics across both simulated and biological neural systems."
  },
  {
    "filename": "2509.17180v1.txt",
    "text": "Many common estimators in machine learning and causal inference are linear smoothers, where the prediction is a weighted average of the training outcomes. Some estimators, such as ordinary least squares and kernel ridge regression, allow for arbitrarily negative weights, which improve feature imbalance but often at the cost of increased dependence on parametric modeling assumptions and higher variance. By contrast, estimators like importance weighting and random forests (sometimes implicitly) restrict weights to be non-negative, reducing dependence on parametric modeling and variance at the cost of worse imbalance. In this paper, we propose a unified framework that directly penalizes the level of extrapolation, replacing the current practice of a hard non-negativity constraint with a soft constraint and corresponding hyperparameter. We derive a worst-case extrapolation error bound and introduce a novel \"bias-bias-variance\" tradeoff, encompassing biases due to feature imbalance, model misspecification, and estimator variance; this tradeoff is especially pronounced in high dimensions, particularly when positivity is poor. We then develop an optimization procedure that regularizes this bound while minimizing imbalance and outline how to use this approach as a sensitivity analysis for dependence on parametric modeling assumptions. We demonstrate the effectiveness of our approach through synthetic experiments and a real-world application, involving the generalization of randomized controlled trial estimates to a target population of interest."
  },
  {
    "filename": "2509.17182v1.txt",
    "text": "The aerodynamic optimization of cars requires close collaboration between aerodynamicists and stylists, while slow, expensive simulations remain a bottleneck. Surrogate models have been shown to accurately predict aerodynamics within the design space for which they were trained. However, many of these models struggle to scale to higher resolutions because of the 3D nature of the problem and data scarcity. We propose Progressive Multi-Resolution Training (PMRT), a probabilistic multi-resolution training schedule that enables training a U-Net to predict the drag coefficient ($c_d$) and high-resolution velocity fields (512 x 128 x 128) in 24 hours on a single NVIDIA H100 GPU, 7x cheaper than the high-resolution-only baseline, with similar accuracy. PMRT samples batches from three resolutions based on probabilities that change during training, starting with an emphasis on lower resolutions and gradually shifting toward higher resolutions. Since this is a training methodology, it can be adapted to other high-resolution-focused backbones. We also show that a single model can be trained across five datasets from different solvers, including a real-world dataset, by conditioning on the simulation parameters. In the DrivAerML dataset, our models achieve a $c_d$ $R^2$ of 0.975, matching literature baselines at a fraction of the training cost."
  },
  {
    "filename": "2509.17189v1.txt",
    "text": "Turbulence is a central challenge in classical physics and a critical barrier to accurate flow prediction in climate, aerospace, and energy systems. Despite the widespread reliance on Reynolds-averaged Navier-Stokes (RANS) solvers in industrial simulations, existing turbulence models lack the generalizability to handle diverse regimes, such as separation, secondary flows, and free-shear flows, without manual tuning or switching. We propose a unified data-driven turbulence modeling framework based on multi-objective learning. The goal is to achieve Pareto-optimal performance across heterogeneous flow datasets, each representing distinct mechanisms and quantities of interest. The resulting unified foundation model employs a parallel tensor basis neural network with automatic balancing and internal branching to adapt across flow regimes without explicit switching. The parallel architecture enables explicit regularization to promote model parsimony, while the tensor-basis formulation preserves physical symmetries. Trained on five representative flows, the model is evaluated on 27 test cases spanning attached, separated, and secondary flows, as well as two realistic three-dimensional flows of industrial relevance. It improves or matches the performance of the baseline $k$-$\\omega$ model in all cases. For specific applications, we show that specialist models trained on tailored datasets can further improve accuracy in challenging configurations, such as three-dimensional diffuser flows common in gas turbine aerodynamics, which exhibit simultaneous separation and secondary flows. These results demonstrate that a generalized, deployable turbulence model unifying multiple flow mechanisms within a single architecture is achievable. This work marks significant progress toward unified turbulence modeling for scientific and industrial applications."
  },
  {
    "filename": "2509.17202v1.txt",
    "text": "Learning underlies nearly all human behavior and is central to education and education reform. Although recent advances in neuroscience have revealed the fundamental structure of learning processes, these insights have yet to be integrated into research and practice. Specifically, neuroscience has found that decision-making is governed by a structured process of perception, action-selection, and execution, supported by multiple neural systems with distinct memory stores and learning mechanisms. These systems extract different types of information (categorical, predictive, structural, and sequential) challenging canonical models of memory used in learning and behavioral science research by providing a mechanistic account of how humans acquire and use knowledge. Because each system learns differently, effective teaching requires alignment with system-specific processes. We propose a unified model that integrates these neuroscientific insights, bridging basic mechanisms with outcomes in education, identity, belonging, and wellbeing. By translating first principles of neural information processing into a generalizable framework, this work advances theories of skill acquisition and transfer while establishing a foundation for interdisciplinary research to refine how learning is understood and supported across domains of human behavior."
  },
  {
    "filename": "2509.17207v1.txt",
    "text": "Pre-training strategies play a critical role in advancing the performance of transformer-based models for 3D point cloud tasks. In this paper, we introduce Point-RTD (Replaced Token Denoising), a novel pretraining strategy designed to improve token robustness through a corruption-reconstruction framework. Unlike traditional mask-based reconstruction tasks that hide data segments for later prediction, Point-RTD corrupts point cloud tokens and leverages a discriminator-generator architecture for denoising. This shift enables more effective learning of structural priors and significantly enhances model performance and efficiency. On the ShapeNet dataset, Point-RTD reduces reconstruction error by over 93% compared to PointMAE, and achieves more than 14x lower Chamfer Distance on the test set. Our method also converges faster and yields higher classification accuracy on ShapeNet, ModelNet10, and ModelNet40 benchmarks, clearly outperforming the baseline Point-MAE framework in every case."
  },
  {
    "filename": "2509.17213v1.txt",
    "text": "Self-driving cars operate in constantly changing environments and are exposed to a variety of uncertainties and disturbances. These factors render classical controllers ineffective, especially for lateral control. Therefore, an adaptive MPC controller is designed in this paper for the path tracking task, tuned by an improved particle swarm optimization algorithm. Online parameter adaptation is performed using Neural Networks and ANFIS. The designed controller showed promising results compared to standard MPC in triple lane change and trajectory tracking scenarios. Code can be found here: https://github.com/yassinekebbati/NN_MPC-vs-ANFIS_MPC"
  },
  {
    "filename": "2509.17215v1.txt",
    "text": "Autonomous vehicles are the upcoming solution to most transportation problems such as safety, comfort and efficiency. The steering control is one of the main important tasks in achieving autonomous driving. Model predictive control (MPC) is among the fittest controllers for this task due to its optimal performance and ability to handle constraints. This paper proposes an adaptive MPC controller (AMPC) for the path tracking task, and an improved PSO algorithm for optimising the AMPC parameters. Parameter adaption is realised online using a lookup table approach. The propose AMPC performance is assessed and compared with the classic MPC and the Pure Pursuit controller through simulations. Code can be found here: https://github.com/yassinekebbati/Optimized_adaptive_MPC"
  },
  {
    "filename": "2509.17216v1.txt",
    "text": "Between 40$\\%$ and 80$\\%$ of cataclysmic variables (CVs) are expected to have evolved past the period-minimum and contain a degenerate donor. However, observational surveys for CVs have only been able to detect a few of these highly evolved \"period-bouncers\", most likely due to the intrinsic faintness associated with their predicted low mass accretion rates. We have produced an initial selection of 137 high-likelihood period-bounce candidates from WD catalog based on our multiwavelength period-bouncer scorecard and selection cuts including X-ray data from the extended ROentgen Survey with an Imaging Telescope Array (eROSITA) on board the Spektrum-Roentgen-Gamma spacecraft (SRG). We have laid out three main requirements (classification as a CV, determination of an orbital period and detection of a very late-type donor) that should result in the confirmation of several of these candidates. Our path for confirming these candidates has already produced its first successful result with the confirmation of GALEX J125751.4-283015 as a new period-bouncer. Several other candidates have already fulfilled at least one of our three requirements making their future confirmation likely. Our search for period-bouncers using the X-ray eROSITA emission of objects in optical WD catalogs has led to the confirmation of six new period-bouncers identified from the Gaia DR3 WD catalog (five previously known CVs and one WD candidate), a 18$\\%$ increase that brings the present population to 39 systems. Both the selection method for period-bounce candidates and the confirmation path that we outlined will aid in future searches for new period-bounce candidates, contributing to the goal of resolving the discrepancy between the predicted high number of period-bouncers and the low number of these systems successfully observed to date."
  },
  {
    "filename": "2509.17224v1.txt",
    "text": "Advances in deep learning have opened an era of abundant and accurate predicted protein structures; however, similar progress in protein ensembles has remained elusive. This review highlights several recent research directions towards AI-based predictions of protein ensembles, including coarse-grained force fields, generative models, multiple sequence alignment perturbation methods, and modeling of ensemble descriptors. An emphasis is placed on realistic assessments of the technological maturity of current methods, the strengths and weaknesses of broad families of techniques, and promising machine learning frameworks at an early stage of development. We advocate for \"closing the loop\" between model training, simulation, and inference to overcome challenges in training data availability and to enable the next generation of models."
  },
  {
    "filename": "2509.17228v1.txt",
    "text": "Clinical notes contain rich patient information, such as diagnoses or medications, making them valuable for patient representation learning. Recent advances in large language models have further improved the ability to extract meaningful representations from clinical texts. However, clinical notes are often missing. For example, in our analysis of the MIMIC-IV dataset, 24.5% of patients have no available discharge summaries. In such cases, representations can be learned from other modalities such as structured data, chest X-rays, or radiology reports. Yet the availability of these modalities is influenced by clinical decision-making and varies across patients, resulting in modality missing-not-at-random (MMNAR) patterns. We propose a causal representation learning framework that leverages observed data and informative missingness in multimodal clinical records. It consists of: (1) an MMNAR-aware modality fusion component that integrates structured data, imaging, and text while conditioning on missingness patterns to capture patient health and clinician-driven assignment; (2) a modality reconstruction component with contrastive learning to ensure semantic sufficiency in representation learning; and (3) a multitask outcome prediction model with a rectifier that corrects for residual bias from specific modality observation patterns. Comprehensive evaluations across MIMIC-IV and eICU show consistent gains over the strongest baselines, achieving up to 13.8% AUC improvement for hospital readmission and 13.1% for ICU admission."
  },
  {
    "filename": "2509.17231v1.txt",
    "text": "Ultra-short period (USP) exoplanets -- with $R_p \\leq 2~$R$_{\\oplus}$ and periods $\\leq$1 day -- are expected to be stripped of volatile atmospheres by intense host star irradiation, which is corroborated by their nominal bulk densities and previous eclipse observations consistent with bare rock surfaces. However, a few USP planets appear anomalously under-dense relative to an Earth-like composition, suggesting an exotic interior structure (e.g., core-less) or a volatile-rich secondary atmosphere increasing their apparent radius. Here we present the first dayside emission spectrum of the low density (4.3$\\pm$0.4 g~cm$^{-3}$) ultra-short period planet TOI-561 b, which orbits an iron-poor, alpha-rich, $\\sim$10 Gyr old thick disk star. Our 3-5 $\\mu$m JWST/NIRSpec observations demonstrate the dayside of TOI-561 b is inconsistent with a bare-rock surface at high statistical significance, suggesting instead a thick volatile envelope that is cooling the dayside to well below the $\\sim$3000 K expected in the bare rock or thin atmosphere case. These results reject the popular hypothesis of complete atmospheric desiccation for highly irradiated exoplanets and support predictions that planetary-scale magma oceans can retain substantial reservoirs of volatiles, opening the geophysical study of ultra-hot super-Earths through the lens of their atmospheres."
  },
  {
    "filename": "2509.17237v1.txt",
    "text": "Autonomous underwater vehicles (AUVs) are subject to various sources of faults during their missions, which challenges AUV control and operation in real environments. This paper addresses fault-tolerant trajectory tracking of autonomous underwater vehicles (AUVs) under thruster failures. We propose an adaptive Lyapunov-constrained model predictive control (LMPC) that guarantees stable trajectory tracking when the AUV switches between fault and normal modes. Particularly, we model different AUV thruster faults and build online failure identification based on Bayesian approach. This facilitates a soft switch between AUV status, and the identified and updated AUV failure model feeds LMPC controller for the control law derivation. The Lyapunov constrain in LMPC ensures that the trajectory tracking control remains stable during AUV status shifts, thus mitigating severe and fatal fluctuations when an AUV thruster occurs or recovers. We conduct numerical simulations on a four-thruster planar AUV using the proposed approach. The results demonstrate smooth transitions between thruster failure types and low trajectory tracking errors compared with the benchmark adaptive MPC and backstepping control with rapid failure identification and failure accommodation during the trajectory tracking."
  },
  {
    "filename": "2509.17238v1.txt",
    "text": "The generation quality of large language models (LLMs) is often improved by utilizing inference-time sequence-level scaling methods (e.g., Chain-of-Thought). We introduce hyper-parallel scaling, a complementary framework that improves prediction quality at the token level. Hyper-parallel scaling computes and aggregates multiple output proposals for a single token from the model. We implement this concept in Mixture-of-Experts (MoE) models, which we refer to as Roster of Experts (RoE). RoE is a training-free inference algorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects controlled stochasticity into the expert routing mechanism, enabling it to sample multiple diverse experts for each token and aggregate their outputs for a more accurate final prediction.To overcome the computational cost, we introduce an efficient batching strategy and a specialized KV-caching mechanism that minimizes compute and memory overhead. For example, RoE enables a 7B MoE model to match the performance of a 10.5B MoE model while using 30% less compute for inference. These gains are achieved without any fine-tuning of model parameters."
  },
  {
    "filename": "2509.17243v1.txt",
    "text": "Strong light-matter coupling in Fabry-Perot cavities can modify ground-state molecular reactivity, charge and energy transport, while modifications to single-molecule properties have not been observed experimentally. The mechanisms and reproducibility of such effects remain contested, with conflicting theoretical predictions driven by differences in Hamiltonian choice and quantum state representation. Here, we resolve these ambiguities with numerically exact quantum simulations of cavity-coupled molecular ensembles based on the ab initio light-matter Hamiltonian, treating electrons, nuclei, and cavity photons on equal footing. We investigate ensembles of the rotational-vibrational-electronic Shin-Metiu model using variational tree-tensor-network quantum dynamics, capturing rovibronic couplings and anharmonicity. Embedding ensembles in a cavity induces local modifications of rotational, nuclear, and --more weakly-- electronic observables in individual molecules. The extent of these modifications depends only on the per-molecule coupling strength up until each molecule reaches the ultrastrong coupling regime, which remains unattainable in practical Fabry-Perot setups. Increasing ensemble size toward the thermodynamic limit causes these local modifications to vanish regardless of dipole self-energy inclusion. Nonetheless, global ground-state observables, such as light-matter coupling energy contributions (affecting overall polarizability) and cavity field displacement fluctuations, depend crucially on proper treatment of intermolecular and light-matter correlations, whereas intramolecular observables remain largely insensitive. These insights are crucial for guiding future investigations using approximate quantum treatments, and for the interpretation of experimental results in polaritonic chemistry."
  },
  {
    "filename": "2509.17246v1.txt",
    "text": "We introduce SPFSplatV2, an efficient feed-forward framework for 3D Gaussian splatting from sparse multi-view images, requiring no ground-truth poses during training and inference. It employs a shared feature extraction backbone, enabling simultaneous prediction of 3D Gaussian primitives and camera poses in a canonical space from unposed inputs. A masked attention mechanism is introduced to efficiently estimate target poses during training, while a reprojection loss enforces pixel-aligned Gaussian primitives, providing stronger geometric constraints. We further demonstrate the compatibility of our training framework with different reconstruction architectures, resulting in two model variants. Remarkably, despite the absence of pose supervision, our method achieves state-of-the-art performance in both in-domain and out-of-domain novel view synthesis, even under extreme viewpoint changes and limited image overlap, and surpasses recent methods that rely on geometric supervision for relative pose estimation. By eliminating dependence on ground-truth poses, our method offers the scalability to leverage larger and more diverse datasets. Code and pretrained models will be available on our project page: https://ranrhuang.github.io/spfsplatv2/."
  },
  {
    "filename": "2509.17250v1.txt",
    "text": "We introduce U-shaped encoder-decoder graph neural networks (U-GNNs) for stochastic graph signal generation using denoising diffusion processes. The architecture learns node features at different resolutions with skip connections between the encoder and decoder paths, analogous to the convolutional U-Net for image generation. The U-GNN is prominent for a pooling operation that leverages zero-padding and avoids arbitrary graph coarsening, with graph convolutions layered on top to capture local dependencies. This technique permits learning feature embeddings for sampled nodes at deeper levels of the architecture that remain convolutional with respect to the original graph. Applied to stock price prediction -- where deterministic forecasts struggle to capture uncertainties and tail events that are paramount -- we demonstrate the effectiveness of the diffusion model in probabilistic forecasting of stock prices."
  },
  {
    "filename": "2509.17252v1.txt",
    "text": "Understanding the role of human behavior in shaping environmental outcomes is crucial for addressing global challenges such as climate change. Environmental systems are influenced not only by natural factors like temperature, but also by human decisions regarding mitigation efforts, which are often based on forecasts or predictions about future environmental conditions. Over time, different outcomes can emerge, including scenarios where the environment deteriorates despite efforts to mitigate, or where successful mitigation leads to environmental resilience. Additionally, fluctuations in the level of human participation in mitigation can occur, reflecting shifts in collective behavior. In this study, we consider a variety of human mitigation decisions, in addition to the feedback loop that is created by changes in human behavior because of environmental changes. While these outcomes are based on simplified models, they offer important insights into the dynamics of human decision-making and the factors that influence effective action in the context of environmental sustainability. This study aims to examine key social dynamics influencing society's response to a worsening climate. While others conclude that homophily prompts greater warming unconditionally, this model finds that homophily can prevent catastrophic effects given a poor initial environmental state. Assuming that poor countries have the resources to do so, a consensus in that class group to defect from the strategy of the rich group (who are generally incentivized to continue ``business as usual'') can frequently prevent the vegetation proportion from converging to 0."
  },
  {
    "filename": "2509.17254v1.txt",
    "text": "The Direct Simulation Monte Carlo (DSMC) method remains the gold standard for simulating rarefied gas flows but is prohibitively expensive for parametric and many-query applications. To address this limitation, we introduce a Deep Operator Network (DeepONet) surrogate framework featuring an innovative, physics-guided zonal loss function. The zonal loss prioritizes physical fidelity in critical flow regions over global error metrics, leading to predictions with greater engineering relevance. It explicitly emphasizes accuracy in the recirculation zone, ensuring faithful reconstruction of separated flow features that are often under-resolved by conventional, globally averaged error metrics. Two operator-learning tasks are demonstrated: mapping the Knudsen number to the velocity field and mapping the step-height ratio to the flow solution. The results show excellent agreement with high-fidelity DSMC data. An ablation study highlights that, while global error metrics may suggest only marginal improvements, localized error analysis reveals the superior fidelity of the zonal loss in capturing vortex dynamics -- an aspect central to engineering relevance. The proposed surrogate reproduces detailed velocity fields with high physical fidelity and achieves predictions for unseen parameters in milliseconds, representing speedups of several orders of magnitude relative to DSMC. This capability enables the quantification of uncertainty, optimization, and design-space exploration that would otherwise be computationally intractable."
  },
  {
    "filename": "2509.17270v1.txt",
    "text": "Intrusive speech-intelligibility predictors that exploit explicit reference signals are now widespread, yet they have not consistently surpassed non-intrusive systems. We argue that a primary cause is the limited exploitation of speech foundation models (SFMs). This work revisits intrusive prediction by combining reference conditioning with multi-layer SFM representations. Our final system achieves RMSE 22.36 on the development set and 24.98 on the evaluation set, ranking 1st on CPC3. These findings provide practical guidance for constructing SFM-based intrusive intelligibility predictors."
  },
  {
    "filename": "2509.17272v1.txt",
    "text": "The terahertz (THz) properties of ZrN thin films grown with CMOS-techniques on industry-standard 300 mm silicon wafers are investigated in order to explore their superconducting behavior. The films have thicknesses ranging from 18 to 48 nm, and their critical temperatures Tc are between 5 and 7.3 K. We probe the real and imaginary parts of the complex dynamical conductivity sigma in the frequency range from 100 - 540 GHz (0.4 - 2.2 meV) and as a function of temperature. The experiments provide direct access to the low-energy electrodynamics and key materials parameters such as superconducting energy gap and superfluid density. Our findings indicate that ZrN is a weakly coupled BCS-type superconductor with a gap-to-Tc ratio of approximately 3.4 in the thick film limit. For thinner films, this coupling ratio increases up to 4.0, departing from the BCS prediction. The results establish large-scale ZrN thin films as promising material for high-frequency superconducting applications."
  },
  {
    "filename": "2509.17280v1.txt",
    "text": "Generative pretraining (the \"GPT\" in ChatGPT) enables language models to learn from vast amounts of internet text without human supervision. This approach has driven breakthroughs across AI by allowing deep neural networks to learn from massive, unstructured datasets. We use the term foundation models to refer to large pretrained systems that can be adapted to a wide range of tasks within and across domains, and these models are increasingly applied beyond language to the brain sciences. These models achieve strong predictive accuracy, raising hopes that they might illuminate computational principles. But predictive success alone does not guarantee scientific understanding. Here, we outline how foundation models can be productively integrated into the brain sciences, highlighting both their promise and their limitations. The central challenge is to move from prediction to explanation: linking model computations to mechanisms underlying neural activity and cognition."
  },
  {
    "filename": "2509.17284v1.txt",
    "text": "We present a covariant framework for the quantization of the electromagnetic field in the presence of magnetic monopoles. Building on the two-potential formalism of Cabibbo and Ferrari, which treats electric and magnetic sources on equal footing and reveals a $U(1) \\times U(1)$ gauge symmetry, we extend the theory into the quantum domain. Using the Gupta-Bleuler procedure in the Feynman-'t Hooft gauge, we construct the physical Hilbert space and eliminate negative-norm states. The resulting theory predicts, in addition to conventional photons, the existence of dual photons associated with magnetic charges. We discuss the role of these dual excitations and their possible relevance in the broader context of electromagnetic duality and gauge theories."
  },
  {
    "filename": "2509.17291v1.txt",
    "text": "Given a set of graphs from some unknown family, we want to generate new graphs from that family. Recent methods use diffusion on either graph embeddings or the discrete space of nodes and edges. However, simple changes to embeddings (say, adding noise) can mean uninterpretable changes in the graph. In discrete-space diffusion, each step may add or remove many nodes/edges. It is hard to predict what graph patterns we will observe after many diffusion steps. Our proposed method, called GraphWeave, takes a different approach. We separate pattern generation and graph construction. To find patterns in the training graphs, we see how they transform vectors during random walks. We then generate new graphs in two steps. First, we generate realistic random walk \"trajectories\" which match the learned patterns. Then, we find the optimal graph that fits these trajectories. The optimization infers all edges jointly, which improves robustness to errors. On four simulated and five real-world benchmark datasets, GraphWeave outperforms existing methods. The most significant differences are on large-scale graph structures such as PageRank, cuts, communities, degree distributions, and flows. GraphWeave is also 10x faster than its closest competitor. Finally, GraphWeave is simple, needing only a transformer and standard optimizers."
  },
  {
    "filename": "2509.17292v1.txt",
    "text": "Cognitive distortions have been closely linked to mental health disorders, yet their automatic detection remained challenging due to contextual ambiguity, co-occurrence, and semantic overlap. We proposed a novel framework that combines Large Language Models (LLMs) with Multiple-Instance Learning (MIL) architecture to enhance interpretability and expression-level reasoning. Each utterance was decomposed into Emotion, Logic, and Behavior (ELB) components, which were processed by LLMs to infer multiple distortion instances, each with a predicted type, expression, and model-assigned salience score. These instances were integrated via a Multi-View Gated Attention mechanism for final classification. Experiments on Korean (KoACD) and English (Therapist QA) datasets demonstrate that incorporating ELB and LLM-inferred salience scores improves classification performance, especially for distortions with high interpretive ambiguity. Our results suggested a psychologically grounded and generalizable approach for fine-grained reasoning in mental health NLP."
  },
  {
    "filename": "2509.17297v1.txt",
    "text": "Machine learning (ML) models are often constrained by their limitations in extrapolation, which restricts their applicability in engineering contexts. Conversely, while exhibiting broad generality, many established scientific models seem to lack the necessary accuracy. This study addresses these challenges by introducing JPResUnet (Joint PDF Residual U-net), a novel model that integrates the strengths of both ML and traditional scientific approaches to predict sub-grid joint probability density functions (PDFs) in partially premixed flames. JPResUnet employs a residual U-Net architecture to translate classic $\\beta$-PDFs to sub-grid PDFs. The model is trained using direct numerical simulation (DNS) data from methane-air moderate or intense low-oxygen dilution (MILD) combustion and is initially tested through a priori assessments on out-of-sample data. Comparative analyses against an artificial neural network (ANN) and the $\\beta$-PDF approach demonstrate that JPResUnet consistently outperforms these methods in capturing complex sub-grid features with greater accuracy and robustness for both box and Gaussian kernels of varying widths, and for more extrapolated cases. Subsequent a posteriori assessment involves two versions of JPResUnet with different output PDF resolutions, which are deployed for large eddy simulation (LES) of a multi-regime burner through the look-up table (LUT) approach. The higher resolution model yields improvements in temperature estimates compared to the conventional LUT method. This highlights the potential of the JPResUnet model for robust and accurate LES of reacting flows with ML."
  },
  {
    "filename": "2509.17304v1.txt",
    "text": "Performative prediction (PP) is an algorithmic framework for optimizing machine learning (ML) models where the model's deployment affects the distribution of the data it is trained on. Compared to traditional ML with fixed data, designing algorithms in PP converging to a stable point -- known as a stationary performative stable (SPS) solution -- is more challenging than the counterpart in conventional ML tasks due to the model-induced distribution shifts. While considerable efforts have been made to find SPS solutions using methods such as repeated gradient descent (RGD) and greedy stochastic gradient descent (SGD-GD), most prior studies assumed a strongly convex loss until a recent work established $\\mathcal{O}(1/\\sqrt{T})$ convergence of SGD-GD to SPS solutions under smooth, non-convex losses. However, this latest progress is still based on the restricted bounded variance assumption in stochastic gradient estimates and yields convergence bounds with a non-vanishing error neighborhood that scales with the variance. This limitation motivates us to improve convergence rates and reduce error in stochastic optimization for PP, particularly in non-convex settings. Thus, we propose a new algorithm called stochastic performative prediction with variance reduction (SPRINT) and establish its convergence to an SPS solution at a rate of $\\mathcal{O}(1/T)$. Notably, the resulting error neighborhood is **independent** of the variance of the stochastic gradients. Experiments on multiple real datasets with non-convex models demonstrate that SPRINT outperforms SGD-GD in both convergence rate and stability."
  },
  {
    "filename": "2509.17305v1.txt",
    "text": "T cell receptor (TCR) recognition of peptide-MHC (pMHC) complexes is fundamental to adaptive immunity and central to the development of T cell-based immunotherapies. While transformer-based models have shown promise in predicting TCR-pMHC interactions, most lack a systematic and explainable approach to architecture design. We present an approach that uses a new post-hoc explainability method to inform the construction of a novel encoder-decoder transformer model. By identifying the most informative combinations of TCR and epitope sequence inputs, we optimize cross-attention strategies, incorporate auxiliary training objectives, and introduce a novel early-stopping criterion based on explanation quality. Our framework achieves state-of-the-art predictive performance while simultaneously improving explainability, robustness, and generalization. This work establishes a principled, explanation-driven strategy for modeling TCR-pMHC binding and offers mechanistic insights into sequence-level binding behavior through the lens of deep learning."
  },
  {
    "filename": "2509.17308v1.txt",
    "text": "Cable-driven serpentine manipulators hold great potential in unstructured environments, offering obstacle avoidance, multi-directional force application, and a lightweight design. By placing all motors and sensors at the base and employing plastic links, we can further reduce the arm's weight. To demonstrate this concept, we developed a 9-degree-of-freedom cable-driven serpentine manipulator with an arm length of 545 mm and a total mass of only 308 g. However, this design introduces flexibility-induced variations, such as cable slack, elongation, and link deformation. These variations result in discrepancies between analytical predictions and actual link positions, making pose estimation more challenging. To address this challenge, we propose a physical reservoir computing based pose estimation method that exploits the manipulator's intrinsic nonlinear dynamics as a high-dimensional reservoir. Experimental results show a mean pose error of 4.3 mm using our method, compared to 4.4 mm with a baseline long short-term memory network and 39.5 mm with an analytical approach. This work provides a new direction for control and perception strategies in lightweight cable-driven serpentine manipulators leveraging their intrinsic dynamics."
  },
  {
    "filename": "2509.17314v1.txt",
    "text": "Software increasingly relies on the emergent capabilities of Large Language Models (LLMs), from natural language understanding to program analysis and generation. Yet testing them on specific tasks remains difficult and costly: many prompts lack ground truth, forcing reliance on human judgment, while existing uncertainty and adequacy measures typically require full inference. A key challenge is to assess input adequacy in a way that reflects the demands of the task, ideally before even generating any output. We introduce CLOTHO, a task-specific, pre-generation adequacy measure that estimates input difficulty directly from hidden LLM states. Given a large pool of unlabelled inputs for a specific task, CLOTHO uses a Gaussian Mixture Model (GMM) to adaptively sample the most informative cases for human labelling. Based on this reference set the GMM can then rank unseen inputs by their likelihood of failure. In our empirical evaluation across eight benchmark tasks and three open-weight LLMs, CLOTHO can predict failures with a ROC-AUC of 0.716, after labelling reference sets that are on average only 5.4% of inputs. It does so without generating any outputs, thereby reducing costs compared to existing uncertainty measures. Comparison of CLOTHO and post-generation uncertainty measures shows that the two approaches complement each other. Crucially, we show that adequacy scores learnt from open-weight LLMs transfer effectively to proprietary models, extending the applicability of the approach. When prioritising test inputs for proprietary models, CLOTHO increases the average number of failing inputs from 18.7 to 42.5 out of 100, compared to random prioritisation."
  },
  {
    "filename": "2509.17316v1.txt",
    "text": "In this work, a theoretical scheme on quantum molecular dynamics in curved Schwarzschild spacetime is developed. To this end, the gravitational field is introduced by revising the kinetics energy operator (KEO) rather than by adding Newtonian gravitational interaction in the potential energy surface (PES) due to the important role of the metric tensor in deriving the KEO. To test the present Schr{\\\"o}dinger-type framework, spherically symmetric Schwarzschild spacetime is chosen to explore (1) the H + H$_2$ reaction dynamics, (2) the H$_2$ + H$_2$ scattering dynamics, (3) dynamics of dissociative chemsorption of H$_2$O on Cu(111), and (4) the spectrum band of anthracene cation. Extensive numerical calculations in curved space predict that reaction or scattering probability and spectrum band decrease abruptly to zero as the gravitational strength increases, indicating the remarkable role of gravitational field in chemical dynamics and naturally leading to effects of gravitational time dilation of the molecular systems. However, it is noteworthy that moderate gravitational field can significantly enhance dynamical resonance in the low-energy region, if such resonance exists in flat space. Finally, based on the present numerical results discussions on the quantum molecular dynamics in curved space are given."
  },
  {
    "filename": "2509.17321v1.txt",
    "text": "Data scarcity remains one of the most limiting factors in driving progress in robotics. However, the amount of available robotics data in the wild is growing exponentially, creating new opportunities for large-scale data utilization. Reliable temporal task completion prediction could help automatically annotate and curate this data at scale. The Generative Value Learning (GVL) approach was recently proposed, leveraging the knowledge embedded in vision-language models (VLMs) to predict task progress from visual observations. Building upon GVL, we propose OpenGVL, a comprehensive benchmark for estimating task progress across diverse challenging manipulation tasks involving both robotic and human embodiments. We evaluate the capabilities of publicly available open-source foundation models, showing that open-source model families significantly underperform closed-source counterparts, achieving only approximately $70\\%$ of their performance on temporal progress prediction tasks. Furthermore, we demonstrate how OpenGVL can serve as a practical tool for automated data curation and filtering, enabling efficient quality assessment of large-scale robotics datasets. We release the benchmark along with the complete codebase at \\href{github.com/budzianowski/opengvl}{OpenGVL}."
  },
  {
    "filename": "2509.17323v1.txt",
    "text": "Visual Multi-Object Tracking (MOT) is a crucial component of robotic perception, yet existing Tracking-By-Detection (TBD) methods often rely on 2D cues, such as bounding boxes and motion modeling, which struggle under occlusions and close-proximity interactions. Trackers relying on these 2D cues are particularly unreliable in robotic environments, where dense targets and frequent occlusions are common. While depth information has the potential to alleviate these issues, most existing MOT datasets lack depth annotations, leading to its underexploited role in the domain. To unveil the potential of depth-informed trajectory refinement, we introduce DepTR-MOT, a DETR-based detector enhanced with instance-level depth information. Specifically, we propose two key innovations: (i) foundation model-based instance-level soft depth label supervision, which refines depth prediction, and (ii) the distillation of dense depth maps to maintain global depth consistency. These strategies enable DepTR-MOT to output instance-level depth during inference, without requiring foundation models and without additional computational cost. By incorporating depth cues, our method enhances the robustness of the TBD paradigm, effectively resolving occlusion and close-proximity challenges. Experiments on both the QuadTrack and DanceTrack datasets demonstrate the effectiveness of our approach, achieving HOTA scores of 27.59 and 44.47, respectively. In particular, results on QuadTrack, a robotic platform MOT dataset, highlight the advantages of our method in handling occlusion and close-proximity challenges in robotic tracking. The source code will be made publicly available at https://github.com/warriordby/DepTR-MOT."
  },
  {
    "filename": "2509.17328v1.txt",
    "text": "Building autonomous agents that perceive and operate graphical user interfaces (GUIs) like humans has long been a vision in the field of artificial intelligence. Central to these agents is the capability for GUI interaction, which involves GUI understanding and planning capabilities. Existing methods have tried developing GUI agents based on the multi-modal comprehension ability of vision-language models (VLMs). However, the limited scenario, insufficient size, and heterogeneous action spaces hinder the progress of building generalist GUI agents. To resolve these issues, this paper proposes \\textbf{UIPro}, a novel generalist GUI agent trained with extensive multi-platform and multi-task GUI interaction data, coupled with a unified action space. We first curate a comprehensive dataset encompassing 20.6 million GUI understanding tasks to pre-train UIPro, granting it a strong GUI grounding capability, which is key to downstream GUI agent tasks. Subsequently, we establish a unified action space to harmonize heterogeneous GUI agent task datasets and produce a merged dataset to foster the action prediction ability of UIPro via continued fine-tuning. Experimental results demonstrate UIPro's superior performance across multiple GUI task benchmarks on various platforms, highlighting the effectiveness of our approach."
  },
  {
    "filename": "2509.17338v1.txt",
    "text": "Static program slicing is a fundamental technique in software engineering. Traditional static slicing tools rely on parsing complete source code, which limits their applicability to real-world scenarios where code snippets are incomplete or unparsable. While recent research developed learning-based approaches to predict slices, they face critical challenges: (1) Inaccurate dependency identification, where models fail to precisely capture data and control dependencies between code elements; and (2) Unconstrained generation, where models produce slices with extraneous or hallucinated tokens not present in the input, violating the structural integrity of slices. To address these challenges, we propose \\ourtool, a novel slicing framework that reformulates static program slicing as a sequence-to-sequence task using lightweight language models (e.g., CodeT5+). Our approach incorporates two key innovations. First, we introduce a copy mechanism that enables the model to more accurately capture inter-element dependencies and directly copy relevant tokens from the input, improving both dependency reasoning and generation constraint. Second, we design a constrained decoding process with (a) lexical constraint, restricting outputs to input tokens only, and (b) syntactic constraint, leveraging Tree Similarity of Edit Distance (TSED) monotonicity to detect structurally invalid outputs and discard them. We evaluate \\ourtool on CodeNet and LeetCode datasets and show it consistently outperforms state-of-the-art baselines, improving ExactMatch scores by up to 27\\%. Furthermore, \\ourtool demonstrates strong performance on incomplete code, highlighting its robustness and practical utility in real-world development environments."
  },
  {
    "filename": "2509.17340v1.txt",
    "text": "Agile mapless navigation in cluttered 3D environments poses significant challenges for autonomous drones. Conventional mapping-planning-control pipelines incur high computational cost and propagate estimation errors. We present AERO-MPPI, a fully GPU-accelerated framework that unifies perception and planning through an anchor-guided ensemble of Model Predictive Path Integral (MPPI) optimizers. Specifically, we design a multi-resolution LiDAR point-cloud representation that rapidly extracts spatially distributed \"anchors\" as look-ahead intermediate endpoints, from which we construct polynomial trajectory guides to explore distinct homotopy path classes. At each planning step, we run multiple MPPI instances in parallel and evaluate them with a two-stage multi-objective cost that balances collision avoidance and goal reaching. Implemented entirely with NVIDIA Warp GPU kernels, AERO-MPPI achieves real-time onboard operation and mitigates the local-minima failures of single-MPPI approaches. Extensive simulations in forests, verticals, and inclines demonstrate sustained reliable flight above 7 m/s, with success rates above 80% and smoother trajectories compared to state-of-the-art baselines. Real-world experiments on a LiDAR-equipped quadrotor with NVIDIA Jetson Orin NX 16G confirm that AERO-MPPI runs in real time onboard and consistently achieves safe, agile, and robust flight in complex cluttered environments. The code will be open-sourced upon acceptance of the paper."
  },
  {
    "filename": "2509.17341v1.txt",
    "text": "This paper introduces the concept of trajectory encryption in cooperative simultaneous target interception, wherein heterogeneity in guidance principles across a team of unmanned autonomous systems is leveraged as a strategic design feature. By employing a mix of heterogeneous time-to-go formulations leading to a cooperative guidance strategy, the swarm of vehicles is able to generate diverse trajectory families. This diversity expands the feasible solution space for simultaneous target interception, enhances robustness under disturbances, and enables flexible time-to-go adjustments without predictable detouring. From an adversarial perspective, heterogeneity obscures the collective interception intent by preventing straightforward prediction of swarm dynamics, effectively acting as an encryption layer in the trajectory domain. Simulations demonstrate that the swarm of heterogeneous vehicles is able to intercept a moving target simultaneously from a diverse set of initial engagement configurations."
  },
  {
    "filename": "2509.17354v1.txt",
    "text": "Lane-change maneuvers are a leading cause of highway accidents, underscoring the need for accurate intention prediction to improve the safety and decision-making of autonomous driving systems. While prior studies using machine learning and deep learning methods (e.g., SVM, CNN, LSTM, Transformers) have shown promise, most approaches remain limited by binary classification, lack of scenario diversity, and degraded performance under longer prediction horizons. In this study, we propose a physics-informed AI framework that explicitly integrates vehicle kinematics, interaction feasibility, and traffic-safety metrics (e.g., distance headway, time headway, time-to-collision, closing gap time) into the learning process. lane-change prediction is formulated as a three-class problem that distinguishes left change, right change, and no change, and is evaluated across both straight highway segments (highD) and complex ramp scenarios (exiD). By integrating vehicle kinematics with interaction features, our machine learning models, particularly LightGBM, achieve state-of-the-art accuracy and strong generalization. Results show up to 99.8% accuracy and 93.6% macro F1 on highD, and 96.1% accuracy and 88.7% macro F1 on exiD at a 1-second horizon, outperforming a two-layer stacked LSTM baseline. These findings demonstrate the practical advantages of a physics-informed and feature-rich machine learning framework for real-time lane-change intention prediction in autonomous driving systems."
  },
  {
    "filename": "2509.17366v1.txt",
    "text": "We formulate two methods to facilitate the calculation of perturbative corrections to quantum few-body observables. Both techniques are designed for a numerical realization in combination with any tool that obtains either the entire spectrum or solely the eigenvalues of an operator corresponding to the observable of interest. We exemplify these methods in the context of the nuclear contact theory without pions (Pionless EFT) and benchmark them in the deuteron channel with available analytical, field-theoretical calculations, as well as in the triton and 3-helium channels through earlier extractions within the dibaryon formalism, where in all three systems the point-proton root-mean-square charge radius (rms) was the perturbed observable of choice. Beyond these $A\\leq3$ consistency and accuracy checks, we employ the numerical methods to predict the rms of the 4-helium nuclear ground state to assess three different ways of integrating the Coulomb interaction into Pionless EFT. By comparing the respective results at leading and next-to-leading order for 3- and 4-helium, we find that the uncertainty due to the strong, short-range interaction is significantly larger compared with that due to the long-range Coulomb interaction for both bound states with their different binding momenta. Thereby, we provide strong support for simplifying extractions of bound-state observables by shutting off any Coulomb interaction if the strong part of the potential is considered only up to first order in the effective range expansion."
  },
  {
    "filename": "2509.17375v1.txt",
    "text": "Estimating the fundamental frequency, or melody, is a core task in Music Information Retrieval (MIR). Various studies have explored signal processing, machine learning, and deep-learning-based approaches, with a very recent focus on utilizing uncertainty in active learning settings for melody estimation. However, these approaches do not investigate the relative effectiveness of different uncertainties. In this work, we follow a framework that disentangles aleatoric and epistemic uncertainties to guide active learning for melody estimation. Trained on a source dataset, our model adapts to new domains using only a small number of labeled samples. Experimental results demonstrate that epistemic uncertainty is more reliable for domain adaptation with reduced labeling effort as compared to aleatoric uncertainty."
  },
  {
    "filename": "2509.17383v1.txt",
    "text": "In this paper, we study equations with nonlinearity in the form of a double-well potential, randomised by a velocity-switching (telegraph) stochastic process. If the speed parameters of the randomisation are small, then this dynamics has one metastable uncertainty interval and two invariant attractors. The probabilities of leaving the metastable interval through the upper boundary are determined, as well as characteristics of the first crossing times. Invariant measures are also found. When and if the direction of the telegraph process velocity coincides with the direction of the periodic change in potential, the system can go into a metastable state, having received a time window for the interwell transition. The obtained results can be used as an alternative to stochastic resonance models."
  },
  {
    "filename": "2509.17384v1.txt",
    "text": "In the momentarily comoving frame of a cosmological fluid, the determinant of the energy-momentum tensor (EMT) is highly sensitive to its pressure. This component is significant during radiation-dominated epochs, and becomes naturally negligible as the universe transitions to the matter-dominated era. Here, we investigate the cosmological consequences of gravity sourced by the determinant of the EMT. Unlike Azri and Nasri, Phys. Lett. B 836, 137626 (2023), we consider the most general case in which the second derivative of the perfect-fluid Lagrangian does not vanish. We derive the gravitational field equations for the general power-law case and examine the cosmological implications of the scale-independent model characterized by dimensionless couplings to photons and neutrinos. We show that, unlike various theories based on the EMT, the present setup, which leads to an enhanced gravitational effects of radiation, does not alter the time evolution of the energy density of particle species. Furthermore, we confront the model with the predictions of primordial nucleosynthesis, and discuss its potential to alleviate the Hubble tension by reducing the sound horizon. The radiation-gravity couplings we propose here are expected to yield testable cosmological and astrophysical signatures, probing whether gravity distinguishes between relativistic and nonrelativistic species in the early universe."
  },
  {
    "filename": "2509.17393v1.txt",
    "text": "We introduce transductive program synthesis, a new formulation of the program synthesis task that explicitly leverages test inputs during synthesis. While prior approaches to program synthesis--whether based on natural language descriptions or input-output examples--typically aim to generalize from training examples, they often struggle with robustness, especially in real-world settings where training examples are limited and test inputs involve various edge cases. To address this, we propose a novel framework that improves robustness by treating synthesis as an active learning over a finite hypothesis class defined by programs' outputs. We use an LLM to predict outputs for selected test inputs and eliminate inconsistent hypotheses, where the inputs are chosen via a greedy maximin algorithm to minimize the number of LLM queries required. We evaluate our approach on two real-world datasets: Playgol, a string transformation benchmark, and MBPP+, a Python code generation benchmark. We demonstrate that our method significantly improves program synthesis in both accuracy and efficiency. We release our code at https://github.com/klee972/SYNTRA."
  },
  {
    "filename": "2509.17397v1.txt",
    "text": "Global Navigation Satellite Systems (GNSS) are vital for reliable urban positioning. However, multipath and non-line-of-sight reception often introduce large measurement errors that degrade accuracy. Learning-based methods for predicting and compensating pseudorange errors have gained traction, but their performance is limited by complex error distributions. To address this challenge, we propose Diff-GNSS, a coarse-to-fine GNSS measurement (pseudorange) error estimation framework that leverages a conditional diffusion model to capture such complex distributions. Firstly, a Mamba-based module performs coarse estimation to provide an initial prediction with appropriate scale and trend. Then, a conditional denoising diffusion layer refines the estimate, enabling fine-grained modeling of pseudorange errors. To suppress uncontrolled generative diversity and achieve controllable synthesis, three key features related to GNSS measurement quality are used as conditions to precisely guide the reverse denoising process. We further incorporate per-satellite uncertainty modeling within the diffusion stage to assess the reliability of the predicted errors. We have collected and publicly released a real-world dataset covering various scenes. Experiments on public and self-collected datasets show that DiffGNSS consistently outperforms state-of-the-art baselines across multiple metrics. To the best of our knowledge, this is the first application of diffusion models to pseudorange error estimation. The proposed diffusion-based refinement module is plug-and-play and can be readily integrated into existing networks to markedly improve estimation accuracy."
  },
  {
    "filename": "2509.17410v1.txt",
    "text": "Room Impulse Response (RIR) prediction at arbitrary receiver positions is essential for practical applications such as spatial audio rendering. We propose Neural Acoustic Multipole Splatting (NAMS), which synthesizes RIRs at unseen receiver positions by learning the positions of neural acoustic multipoles and predicting their emitted signals and directivities using a neural network. Representing sound fields through a combination of multipoles offers sufficient flexibility to express complex acoustic scenes while adhering to physical constraints such as the Helmholtz equation. We also introduce a pruning strategy that starts from a dense splatting of neural acoustic multipoles and progressively eliminates redundant ones during training. Experiments conducted on both real and synthetic datasets indicate that the proposed method surpasses previous approaches on most metrics while maintaining rapid inference. Ablation studies reveal that multipole splatting with pruning achieves better performance than the monopole model with just 20% of the poles."
  },
  {
    "filename": "2509.17413v1.txt",
    "text": "Ensuring the safety of neural networks under input uncertainty is a fundamental challenge in safety-critical applications. This paper builds on and expands Fazlyab's quadratic-constraint (QC) and semidefinite-programming (SDP) framework for neural network verification to a distributionally robust and tail-risk-aware setting by integrating worst-case Conditional Value-at-Risk (WC-CVaR) over a moment-based ambiguity set with fixed mean and covariance. The resulting conditions remain SDP-checkable and explicitly account for tail risk. This integration broadens input-uncertainty geometry-covering ellipsoids, polytopes, and hyperplanes-and extends applicability to safety-critical domains where tail-event severity matters. Applications to closed-loop reachability of control systems and classification are demonstrated through numerical experiments, illustrating how the risk level $\\varepsilon$ trades conservatism for tolerance to tail events-while preserving the computational structure of prior QC/SDP methods for neural network verification and robustness analysis."
  },
  {
    "filename": "2509.17423v1.txt",
    "text": "Integrating unmanned aerial vehicles into daily use requires controllers that ensure stable flight, efficient energy use, and reduced noise. Proportional integral derivative controllers remain standard but are highly sensitive to gain selection, with manual tuning often yielding suboptimal trade-offs. This paper studies different optimization techniques for the automated tuning of quadrotor proportional integral derivative gains under a unified simulation that couples a blade element momentum based aerodynamic model with a fast deep neural network surrogate, six degrees of freedom rigid body dynamics, turbulence, and a data driven acoustic surrogate model that predicts third octave spectra and propagates them to ground receivers. We compare three families of gradient-free optimizers: metaheuristics, Bayesian optimization, and deep reinforcement learning. Candidate controllers are evaluated using a composite cost function that incorporates multiple metrics, such as noise footprint and power consumption, simultaneously. Metaheuristics improve performance consistently, with Grey Wolf Optimization producing optimal results. Bayesian optimization is sample efficient but carries higher per iteration overhead and depends on the design domain. The reinforcement learning agents do not surpass the baseline in the current setup, suggesting the problem formulation requires further refinement. On unseen missions the best tuned controller maintains accurate tracking while reducing oscillations, power demand, and acoustic emissions. These results show that noise aware proportional integral derivative tuning through black box search can deliver quieter and more efficient flight without hardware changes."
  },
  {
    "filename": "2509.17424v1.txt",
    "text": "A considerable proportion of young stars belong to multiple star systems. Constraining the planet formation processes in multiple stellar systems is then key to understand the global exoplanet population. This study focuses on investigating the dust reservoir within the triple system V892 Tau. Our objective is to establish constraints on the properties and characteristics of the dust present in the system's circumbinary ring. Based on archival ALMA and VLA data from 0.9 mm to 9.8 mm, we present a multi-wavelength analysis of the ring of V892 Tau. We first studied the spatial variation of the spectral index, before employing 3D full radiative transfer calculations to constrain the ring's geometry and the radial dependence of the dust grain properties. Spectral indices are consistent with non-dust emission in the vicinity of the central binary, and with dust emission in the ring likely remaining optically thick up to 3.0 mm. Our radiative transfer analysis supports these interpretations, yielding a model that reproduces the observed intensities within the 1-sigma uncertainties across all wavelengths. The resulting dust surface density and temperature profiles both decrease with increasing radius, and are in agreement with values reported in the literature. Maximum grain sizes are constrained to 0.2 cm, with a size distribution power-law index -3.5. These results imply that the dust grain fragmentation velocity does not exceed 8 m/s. Whilst our results suggest dust trapping at the cavity edge, they also suggest that tidal perturbations triggered by the central binary limit grain growth within the ring. This highlights the need to further constrain planet formation efficiency in multiple stellar systems, a goal that may be advanced by applying the methodology of this work to a wider sample of systems."
  },
  {
    "filename": "2509.17429v1.txt",
    "text": "Accurate temporal prediction is the bridge between comprehensive scene understanding and embodied artificial intelligence. However, predicting multiple fine-grained states of a scene at multiple temporal scales is difficult for vision-language models. We formalize the Multi-Scale Temporal Prediction (MSTP) task in general and surgical scenes by decomposing multi-scale into two orthogonal dimensions: the temporal scale, forecasting states of humans and surgery at varying look-ahead intervals, and the state scale, modeling a hierarchy of states in general and surgical scenes. For example, in general scenes, states of contact relationships are finer-grained than states of spatial relationships. In surgical scenes, medium-level steps are finer-grained than high-level phases yet remain constrained by their encompassing phase. To support this unified task, we introduce the first MSTP Benchmark, featuring synchronized annotations across multiple state scales and temporal scales. We further propose a method, Incremental Generation and Multi-agent Collaboration (IG-MC), which integrates two key innovations. First, we present a plug-and-play incremental generation module that continuously synthesizes up-to-date visual previews at expanding temporal scales to inform multiple decision-making agents, keeping decisions and generated visuals synchronized and preventing performance degradation as look-ahead intervals lengthen. Second, we present a decision-driven multi-agent collaboration framework for multi-state prediction, comprising generation, initiation, and multi-state assessment agents that dynamically trigger and evaluate prediction cycles to balance global coherence and local fidelity."
  },
  {
    "filename": "2509.17430v1.txt",
    "text": "The field of Embodied AI predominantly relies on simulation for training and evaluation, often using either fully synthetic environments that lack photorealism or high-fidelity real-world reconstructions captured with expensive hardware. As a result, sim-to-real transfer remains a major challenge. In this paper, we introduce EmbodiedSplat, a novel approach that personalizes policy training by efficiently capturing the deployment environment and fine-tuning policies within the reconstructed scenes. Our method leverages 3D Gaussian Splatting (GS) and the Habitat-Sim simulator to bridge the gap between realistic scene capture and effective training environments. Using iPhone-captured deployment scenes, we reconstruct meshes via GS, enabling training in settings that closely approximate real-world conditions. We conduct a comprehensive analysis of training strategies, pre-training datasets, and mesh reconstruction techniques, evaluating their impact on sim-to-real predictivity in real-world scenarios. Experimental results demonstrate that agents fine-tuned with EmbodiedSplat outperform both zero-shot baselines pre-trained on large-scale real-world datasets (HM3D) and synthetically generated datasets (HSSD), achieving absolute success rate improvements of 20\\% and 40\\% on real-world Image Navigation task. Moreover, our approach yields a high sim-vs-real correlation (0.87--0.97) for the reconstructed meshes, underscoring its effectiveness in adapting policies to diverse environments with minimal effort. Project page: https://gchhablani.github.io/embodied-splat"
  },
  {
    "filename": "2509.17432v1.txt",
    "text": "Purpose: Pencil beam scanning proton therapy is sensitive to respiratory motion, leading to potential dose inhomogeneities due to interplay effects. We developed and validated a predictive framework to assess interplay and motion robustness in lung and esophageal cancer patients treated under free-breathing conditions. Methods: A synthetic-breathing-based predictive model was implemented in the RayStation treatment planning system (TPS) and validated against an in vivo approach using patient-specific respiratory traces and machine log files. Both were benchmarked against the Monte Carlo engine FRED. To demonstrate the methodology, the framework was applied to two clinical cases treated on the Mevion S250i system without rescanning. Dose accumulation incorporated respiratory phase, range ($\\pm$3 %), and setup ($\\pm$5 mm) uncertainties. Results: Excellent agreement was observed between TPS and FRED ($<$1 % mean dose difference) and between predictive and in vivo models ($<$2 % across DVH metrics). Cumulative dose distributions for the primary CTV converged after five fractions, confirming robust delivery. Conclusions: This automated, clinically integrated framework enables pre-treatment prediction and in vivo validation of interplay and motion robustness in PBS plans. Preliminary results support its clinical utility, especially for hypofractionation and targets with large motion ($>$2 cm). A larger cohort study is ongoing and will be reported separately."
  },
  {
    "filename": "2509.17433v1.txt",
    "text": "Background: Accurate and fast dose calculation is essential for optimizing carbon ion therapy. Existing machine learning (ML) models have been developed for other radiotherapy modalities. They use patient data with uniform CT imaging properties. Purpose: This study investigates the application of several ML models for physical dose calculation in carbon ion therapy and compares their ability to generalize to CT data with varying resolutions. Among the models examined is a Diffusion Model, which is tested for the first time for the calculation of physical dose distributions. Methods: A dataset was generated using publicly available CT images of the head and neck region. Monoenergetic carbon ion beams were simulated at various initial energies using Geant4 simulation software. A U-Net architecture was developed for dose prediction based on distributions of material density in patients and of absorbed dose in water. It was trained as a Generative Adversarial Network (GAN) generator, a Diffusion Model noise estimator, and as a standalone network. Their performances were compared with two models from literature. Results: All models produced dose distributions deviating by less than 2% from that obtained by a full Monte Carlo simulation, even for a patient not seen during training. Dose calculation time on a GPU was in the range of 3 ms to 15 s. The resource-efficient U-Net appears to perform comparably to the more computationally intensive GAN and Diffusion Model. Conclusion: This study demonstrates that ML models can effectively balance accuracy and speed for physical dose calculation in carbon ion therapy. Using the computationally efficient U-Net can help conserve resources. The generalizability of the models to different CT image resolutions enables the use for different patients without extensive retraining."
  },
  {
    "filename": "2509.17445v1.txt",
    "text": "Reliable question answering with large language models (LLMs) is challenged by hallucinations, fluent but factually incorrect outputs arising from epistemic uncertainty. Existing entropy-based semantic-level uncertainty estimation methods are limited by sampling noise and unstable clustering of variable-length answers. We propose Semantic Reformulation Entropy (SRE), which improves uncertainty estimation in two ways. First, input-side semantic reformulations produce faithful paraphrases, expand the estimation space, and reduce biases from superficial decoder tendencies. Second, progressive, energy-based hybrid clustering stabilizes semantic grouping. Experiments on SQuAD and TriviaQA show that SRE outperforms strong baselines, providing more robust and generalizable hallucination detection. These results demonstrate that combining input diversification with multi-signal clustering substantially enhances semantic-level uncertainty estimation."
  },
  {
    "filename": "2509.17447v1.txt",
    "text": "Black hole population studies are currently performed either using astrophysically motivated models (informed but rigid in their functional forms) or via non-parametric methods (flexible but not directly interpretable). In this paper, we present a statistical framework to complement the predictive power of astrophysically motivated models with the flexibility of non-parametric methods. Our method makes use of the Dirichlet distribution to robustly infer the relative weights of different models as well as of the Gibbs sampling approach to efficiently explore the parameter space. After having validated our approach using simulated data, we apply this method to the BBH mergers observed during the first three Observing Runs of the LIGO-Virgo-KAGRA collaboration using both phenomenological and astrophysical models as parametric models, finding results in agreement with the currently available literature."
  },
  {
    "filename": "2509.17450v1.txt",
    "text": "Dexterous robotic hands enable robots to perform complex manipulations that require fine-grained control and adaptability. Achieving such manipulation is challenging because the high degrees of freedom tightly couple hand and arm motions, making learning and control difficult. Successful dexterous manipulation relies not only on precise hand motions, but also on accurate spatial positioning of the arm and coordinated arm-hand dynamics. However, most existing visuomotor policies represent arm and hand actions in a single combined space, which often causes high-dimensional hand actions to dominate the coupled action space and compromise arm control. To address this, we propose DQ-RISE, which quantizes hand states to simplify hand motion prediction while preserving essential patterns, and applies a continuous relaxation that allows arm actions to diffuse jointly with these compact hand states. This design enables the policy to learn arm-hand coordination from data while preventing hand actions from overwhelming the action space. Experiments show that DQ-RISE achieves more balanced and efficient learning, paving the way toward structured and generalizable dexterous manipulation. Project website: http://rise-policy.github.io/DQ-RISE/"
  },
  {
    "filename": "2509.17462v1.txt",
    "text": "The goal of multi-task learning is to learn to conduct multiple tasks simultaneously based on a shared data representation. While this approach can improve learning efficiency, it may also cause performance degradation due to task conflicts that arise when optimizing the model for different objectives. To address this challenge, we introduce MAESTRO, a structured framework designed to generate task-specific features and mitigate feature interference in multi-task 3D perception, including 3D object detection, bird's-eye view (BEV) map segmentation, and 3D occupancy prediction. MAESTRO comprises three components: the Class-wise Prototype Generator (CPG), the Task-Specific Feature Generator (TSFG), and the Scene Prototype Aggregator (SPA). CPG groups class categories into foreground and background groups and generates group-wise prototypes. The foreground and background prototypes are assigned to the 3D object detection task and the map segmentation task, respectively, while both are assigned to the 3D occupancy prediction task. TSFG leverages these prototype groups to retain task-relevant features while suppressing irrelevant features, thereby enhancing the performance for each task. SPA enhances the prototype groups assigned for 3D occupancy prediction by utilizing the information produced by the 3D object detection head and the map segmentation head. Extensive experiments on the nuScenes and Occ3D benchmarks demonstrate that MAESTRO consistently outperforms existing methods across 3D object detection, BEV map segmentation, and 3D occupancy prediction tasks."
  },
  {
    "filename": "2509.17464v1.txt",
    "text": "Predicting the Curie temperature ($T_\\mathrm{C}$) of magnetic materials is crucial for advancing applications in data storage, spintronics, and sensors. We present a machine learning (ML) framework to predict $T_{\\mathrm{C}}$ using a curated dataset of 2,500 ferromagnetic compounds, employing two types of elemental descriptor-based features: one based on stoichiometry-weighted descriptors, and the other leveraging Graph Neural Networks (GNNs). CatBoost trained on the stoichiometry-weighted descriptors achieved an $R^2$ score of 0.87, while the use of GNN-based representations led to a further improvement, with CatBoost reaching an $R^2$ of 0.91, highlighting the effectiveness of graph-based feature learning. We also demonstrated that using an uncurated dataset available online leads to poor predictions, resulting in a low $R^2$ score of 0.66 for the CatBoost model. We analyzed feature importance using tools such as Recursive Feature Elimination (RFE), which revealed that ionization energies are a key physicochemical factor influencing $T_\\mathrm{C}$. Notably, the use of only the first 10 ionization energies as input features resulted in high predictive accuracy, with $R^2$ scores of up to 0.85 for statistical models and 0.89 for the GNN-based approach. These results highlight that combining robust ML models with thoughtful feature engineering and high-quality data, can accelerate the discovery of magnetic materials. Our curated dataset is publicly available on GitHub."
  },
  {
    "filename": "2509.17491v1.txt",
    "text": "Integrated Gradients (IG) is a widely used attribution method in explainable artificial intelligence (XAI). In this paper, we introduce Path-Weighted Integrated Gradients (PWIG), a generalization of IG that incorporates a customizable weighting function into the attribution integral. This modification allows for targeted emphasis along different segments of the path between a baseline and the input, enabling improved interpretability, noise mitigation, and the detection of path-dependent feature relevance. We establish its theoretical properties and illustrate its utility through experiments on a dementia classification task using the OASIS-1 MRI dataset. Attribution maps generated by PWIG highlight clinically meaningful brain regions associated with various stages of dementia, providing users with sharp and stable explanations. The results suggest that PWIG offers a flexible and theoretically grounded approach for enhancing attribution quality in complex predictive models."
  },
  {
    "filename": "2509.17492v1.txt",
    "text": "Multimodal pathological images are usually in clinical diagnosis, but computer vision-based multimodal image-assisted diagnosis faces challenges with modality fusion, especially in the absence of expert-annotated data. To achieve the modality fusion in multimodal images with label scarcity, we propose a novel ``pretraining + fine-tuning\" framework for multimodal semi-supervised medical image classification. Specifically, we propose a synergistic learning pretraining framework of consistency, reconstructive, and aligned learning. By treating one modality as an augmented sample of another modality, we implement a self-supervised learning pre-train, enhancing the baseline model's feature representation capability. Then, we design a fine-tuning method for multimodal fusion. During the fine-tuning stage, we set different encoders to extract features from the original modalities and provide a multimodal fusion encoder for fusion modality. In addition, we propose a distribution shift method for multimodal fusion features, which alleviates the prediction uncertainty and overfitting risks caused by the lack of labeled samples. We conduct extensive experiments on the publicly available gastroscopy image datasets Kvasir and Kvasirv2. Quantitative and qualitative results demonstrate that the proposed method outperforms the current state-of-the-art classification methods. The code will be released at: https://github.com/LQH89757/MICS."
  },
  {
    "filename": "2509.17507v1.txt",
    "text": "In Nigeria, electoral behavior is often interpreted through ethno-religious views and regional allegiances, without empirically assessing the influence of socioeconomic indicators such as health, income, education, and other deprivations on voter behavior. This study investigates the voting pattern in the 2023 Nigerian presidential election and previous cycles using spatio-temporal and multivariate analysis. It examines whether support for some candidates was more substantial in states with higher Human Development Index (HDI) and whether this alignment impacts governance quality and macroeconomic performance post-election. Socioeconomic data were obtained from the Global Data Lab and the Nigerian Bureau of Statistics (NBS) for the year preceding each election, while presidential vote results were sourced from the Independent National Electoral Commission (INEC). The results show that the Labour Party (LP) dominated states with high socioeconomic indices, accounting for about 30-53% of the variance in voting patterns of LP and People's Democratic Party (PDP), while the All Progressive Congress (APC) variance is less explained by these factors. Multinomial logits based on HDI is used to predict party win probabilities; the model predicted about 60% wins accurately. Comparative analysis of four presidential cycles revealed that in 2011, the winner had an HDI-vote correlation of 0.44, with improved macroeconomic indices post-election. In contrast, 2015, 2019, and 2023 saw negative correlations of -0.38, -0.43, and -0.34, respectively, alongside macroeconomic decline. The findings suggest that socioeconomic development shapes political preferences, promotes issue-based politics, and supports quality leadership; therefore, strengthening education, healthcare, and poverty reduction should be prioritized to enhance citizens' well-being and build an informed electorate."
  },
  {
    "filename": "2509.17520v1.txt",
    "text": "Brain tumor segmentation requires accurate identification of hierarchical regions including whole tumor (WT), tumor core (TC), and enhancing tumor (ET) from multi-sequence magnetic resonance imaging (MRI) images. Due to tumor tissue heterogeneity, ambiguous boundaries, and contrast variations across MRI sequences, methods relying solely on visual information or post-hoc loss constraints show unstable performance in boundary delineation and hierarchy preservation. To address this challenge, we propose the Unified Multimodal Coherent Field (UMCF) method. This method achieves synchronous interactive fusion of visual, semantic, and spatial information within a unified 3D latent space, adaptively adjusting modal contributions through parameter-free uncertainty gating, with medical prior knowledge directly participating in attention computation, avoiding the traditional \"process-then-concatenate\" separated architecture. On Brain Tumor Segmentation (BraTS) 2020 and 2021 datasets, UMCF+nnU-Net achieves average Dice coefficients of 0.8579 and 0.8977 respectively, with an average 4.18% improvement across mainstream architectures. By deeply integrating clinical knowledge with imaging features, UMCF provides a new technical pathway for multimodal information fusion in precision medicine."
  },
  {
    "filename": "2509.17522v1.txt",
    "text": "Concept Bottleneck Models (CBMs) provide inherent interpretability by first predicting a set of human-understandable concepts and then mapping them to labels through a simple classifier. While users can intervene in the concept space to improve predictions, traditional CBMs typically employ a fixed linear classifier over concept scores, which restricts interventions to manual value adjustments and prevents the incorporation of new concepts or domain knowledge at test time. These limitations are particularly severe in unsupervised CBMs, where concept activations are often noisy and densely activated, making user interventions ineffective. We introduce Chat-CBM, which replaces score-based classifiers with a language-based classifier that reasons directly over concept semantics. By grounding prediction in the semantic space of concepts, Chat-CBM preserves the interpretability of CBMs while enabling richer and more intuitive interventions, such as concept correction, addition or removal of concepts, incorporation of external knowledge, and high-level reasoning guidance. Leveraging the language understanding and few-shot capabilities of frozen large language models, Chat-CBM extends the intervention interface of CBMs beyond numerical editing and remains effective even in unsupervised settings. Experiments on nine datasets demonstrate that Chat-CBM achieves higher predictive performance and substantially improves user interactivity while maintaining the concept-based interpretability of CBMs."
  },
  {
    "filename": "2509.17525v1.txt",
    "text": "Analysis of radio emission from tidal disruption events allows for detailed constraints on the properties of ejected outflows and the host environment surrounding the black hole. However, the late-time radio behaviour of tidal disruption events is not well-studied due to a lack of observations. In this work we present long-term radio monitoring observations of the tidal disruption event AT2019azh spanning 1167-2159 days post disruption. We fit a physically motivated synchrotron model to the radio spectra at each epoch, and model the decay of the light curve under the assumption that the outflow transitions into the non-relativistic Sedov-Taylor regime at late times. Through this modelling we obtain strong constraints on the density profile of the circumnuclear medium, finding an unusually flat density profile proportional to $r^{0.24^{+0.11}_{-0.15}}$. Overall we find that unlike some tidal disruption events, AT2019azh does not show late time re-brightening up to 6 years post-disruption. The Sedov-Taylor light curve decay model provides a good fit to the data, motivating the assumption that the outflow was produced by a single ejection of material close to the time of stellar disruption. From forward modelling the evolution of the radio light curve decay we predict that the radio afterglow from AT2019azh should be detectable for decades at its current rate of evolution."
  },
  {
    "filename": "2509.17526v1.txt",
    "text": "Humid heat stress and heatwaves pose significant risks for living organisms, from humans and wildlife to insects, with wide-ranging health, ecological, and socio-economic impacts that are expected to worsen with climate change. How large-scale climate modes drive the week-to-month variability of humid heat remains poorly understood at the global scale, hindering accurate forecasts necessary for risk-management measures, notably in the heavily populated and ecologically fragile regions of the tropics and subtropics. With forecast lead times up to several weeks, the Madden-Julian Oscillation (MJO), a global-scale intraseasonal tropical atmospheric wave circumnavigating Earth in around 30-60 days, provides considerable predictability for weather conditions, and meteorological and oceanic extremes. Here we show that the MJO, and the associated boreal summer intraseasonal oscillation (BSISO), have a significant influence on humid heatwaves over much of the tropics and subtropics across all seasons, both over terrestrial and marine regions. Humid heatwave likelihood can double or halve, depending on the MJO phase, in large areas of the Earth. The MJO/BSISO's influence on wet-bulb temperature is primarily via specific humidity rather than dry-bulb temperature anomalies. We find that specific humidity anomalies are influenced by horizontal advection of moisture in the planetary boundary layer, particularly in the subtropics where advection of the climatological moisture gradient by MJO-related anomalous winds is the dominant term."
  },
  {
    "filename": "2509.17533v1.txt",
    "text": "The deployment of machine learning (ML) models on microcontrollers (MCUs) is constrained by strict energy, latency, and memory requirements, particularly in battery-operated and real-time edge devices. While software-level optimizations such as quantization and pruning reduce model size and computation, hardware acceleration has emerged as a decisive enabler for efficient embedded inference. This paper evaluates the impact of Neural Processing Units (NPUs) on MCU-based ML execution, using the ARM Cortex-M55 core combined with the Ethos-U55 NPU on the Alif Semiconductor Ensemble E7 development board as a representative platform. A rigorous measurement methodology was employed, incorporating per-inference net energy accounting via GPIO-triggered high-resolution digital multimeter synchronization and idle-state subtraction, ensuring accurate attribution of energy costs. Experimental results across six representative ML models -including MiniResNet, MobileNetV2, FD-MobileNet, MNIST, TinyYolo, and SSD-MobileNet- demonstrate substantial efficiency gains when inference is offloaded to the NPU. For moderate to large networks, latency improvements ranged from 7x to over 125x, with per-inference net energy reductions up to 143x. Notably, the NPU enabled execution of models unsupported on CPU-only paths, such as SSD-MobileNet, highlighting its functional as well as efficiency advantages. These findings establish NPUs as a cornerstone of energy-aware embedded AI, enabling real-time, power-constrained ML inference at the MCU level."
  },
  {
    "filename": "2509.17550v1.txt",
    "text": "As generative models are advancing in quality and quantity for creating synthetic content, deepfakes begin to cause online mistrust. Deepfake detectors are proposed to counter this effect, however, misuse of detectors claiming fake content as real or vice versa further fuels this misinformation problem. We present the first comprehensive uncertainty analysis of deepfake detectors, systematically investigating how generative artifacts influence prediction confidence. As reflected in detectors' responses, deepfake generators also contribute to this uncertainty as their generative residues vary, so we cross the uncertainty analysis of deepfake detectors and generators. Based on our observations, the uncertainty manifold holds enough consistent information to leverage uncertainty for deepfake source detection. Our approach leverages Bayesian Neural Networks and Monte Carlo dropout to quantify both aleatoric and epistemic uncertainties across diverse detector architectures. We evaluate uncertainty on two datasets with nine generators, with four blind and two biological detectors, compare different uncertainty methods, explore region- and pixel-based uncertainty, and conduct ablation studies. We conduct and analyze binary real/fake, multi-class real/fake, source detection, and leave-one-out experiments between the generator/detector combinations to share their generalization capability, model calibration, uncertainty, and robustness against adversarial attacks. We further introduce uncertainty maps that localize prediction confidence at the pixel level, revealing distinct patterns correlated with generator-specific artifacts. Our analysis provides critical insights for deploying reliable deepfake detection systems and establishes uncertainty quantification as a fundamental requirement for trustworthy synthetic media detection."
  },
  {
    "filename": "2509.17560v1.txt",
    "text": "We present a multiphase, resolved study of the galactic wind extending from the nearby starburst galaxy NGC 4666. For this we use VLT/MUSE observations from the GECKOS program and HI data from the WALLABY survey. We identify both ionised and HI gas in a biconical structure extending to at least $z\\sim$8 kpc from the galaxy disk, with increasing velocity offsets above the midplane in both phases, consistent with a multiphase wind. The measured electron density, using [SII], differs significantly from standard expectations of galactic winds. We find electron density declines from the galaxy centre to $\\sim2$ kpc, then rises again, remaining high ($\\sim100-300$ cm$^{-3}$) out to $\\sim$5 kpc. We find that HI dominates the mass loading. The total HI mass outflow rate (above $z~>2$ kpc) is between $5-13~M_{\\odot}~\\rm yr^{-1}$, accounting for uncertainties from disk-blurring and group interactions. The total ionised mass outflow rate (traced by H$\\alpha$) is between $0.5~M_{\\odot}~\\rm yr^{-1}$ and $5~M_{\\odot}~\\rm yr^{-1}$, depending on $n_e(z)$ assumptions. From ALMA/ACA observations, we place an upper-limit on CO flux in the outflow which correlates to $\\lesssim2.9~M_{\\odot}~\\rm yr^{-1}$. We also show that the entire outflow is not limited to the bicone, but a secondary starburst at the edge generates a more widespread outflow, which should be included in simulations. The cool gas in NGC 4666 wind has insufficient velocity to escape the halo of a galaxy of its mass, especially because most of the mass is present in the slower atomic phase. This strong biconical wind contributes to gas cycling around the galaxy."
  },
  {
    "filename": "2509.17568v1.txt",
    "text": "We present the first next-to-next-to-next-to-leading order (N$^3$LO) calculation of the twist-2 matching coefficients for transverse momentum dependent (TMD) quark transversity parton distribution and fragmentation functions in QCD. This matching relates the TMD quark transversity functions to their collinear counterparts in the large-transverse-momentum regime, and provides essential ingredients for precision TMD phenomenology involving transversely polarized beams. As part of our analysis, we also compute the complete set of next-to-next-to-leading order (NNLO) DGLAP splitting functions governing the QCD evolution of collinear transversity distributions. These results extend the perturbative toolkit for spin-dependent observables and establish the transversity sector on the same theoretical footing as unpolarized and helicity distributions. Our findings enable high-precision extractions of transversity PDFs and facilitate improved theoretical predictions for azimuthal asymmetries in semi-inclusive deep inelastic scattering (SIDIS), especially in light of forthcoming data from the Electron-Ion Collider (EIC)."
  },
  {
    "filename": "2509.17592v1.txt",
    "text": "Gravitational waves from compact binary coalescences provide unique opportunities to test general relativity (GR) in the strong-field regime. In particular, the merger phase, during which two compact objects finally coalesce, corresponds to the regime of the strongest gravitational fields accessible by direct observation and thus serves as a probe of the nonlinear nature of gravity. In this work, we test GR in the merger phase by analyzing GW150914 using a modified waveform proposed in [Watarai et al. 2024], which parametrizes possible deviations from GR during this stage. Within this framework, the inferred deviation parameters can be translated into model-independent constraints on physically meaningful quantities. For GW150914, we find that the additional energy radiated in the merger phase is constrained to be $0.26^{+0.75}_{-0.62}~\\%$ of the total energy emitted over the entire coalescence predicted by GR, and the deviation in the coalescence time is $2.17^{+9.56}_{-9.90}~\\mathrm{ms}$, both within the $90\\%$ credible interval. These two constraints serve as observational benchmarks for deviations in the nonlinear gravity regime, offering guidance for theoretical investigations of beyond-GR models."
  },
  {
    "filename": "2509.17594v1.txt",
    "text": "In this study, we introduce a sensitivity analysis methodology for stochastic systems in chemistry, where dynamics are often governed by random processes. Our approach is based on gradient estimation via finite differences, averaging simulation outcomes, and analyzing variability under intrinsic noise. We characterize gradient uncertainty as an angular range within which all plausible gradient directions are expected to lie. This uncertainty measure adaptively guides the number of simulations performed for each nominal-perturbation pair of points in order to minimize unnecessary computations while maintaining robustness.   Systematically exploring a range of parameter values across the parameter space, rather than focusing on a single value, allows us to identify not only sensitive parameters but also regions of parameter space associated with different levels of sensitivity. These results are visualized through vector field plots to offer an intuitive representation of local sensitivity across parameter space. Additionally, global sensitivity coefficients are computed to capture overall trends.   Flexibility regarding the choice of output observable measures is another key feature of our method: while traditional sensitivity analyses often focus on species concentrations, our framework allows for the definition of a large range of problem-specific observables. This makes it broadly applicable in diverse chemical and biochemical scenarios. We demonstrate our approach on two systems: classical Michaelis-Menten kinetics and a rule-based model of the formose reaction, using the cheminformatics software M{\\O}D for Gillespie-based stochastic simulations."
  },
  {
    "filename": "2509.17596v1.txt",
    "text": "We investigate the partonic origin of the proton longitudinal spin using spin-dependent energy correlator measured in lepton-hadron collisions with longitudinally polarized proton beams. These observables encode angular correlations in energy flow and are sensitive to the spin-momentum structure of confined partons. Using soft-collinear effective theory (SCET), we analyze the correlation patterns in both nearly back-to-back and forward limits, which establishes a direct correspondence with longitudinally polarized transverse momentum dependent distributions (TMDs) and nucleon energy correlators (NECs). The TMDs and NECs allow consistent matching onto hard radiation region, and provide a comprehensive description of the transition from perturbative parton branching to nonperturbative confinement. Using renormalization group evolution, we obtain joint $\\text{N}^{3}$LL/NNLL quantitative predictions for spin-dependent energy correlation patterns in the current and target fragmentation regions (CFR and TFR). The framework provides new theoretical insight into how the internal motion and spin of partons contribute to the formation of the proton longitudinal spin, and offers an experimental paradigm for probing the interplay between color confinement and spin dynamics at forthcoming Electron-Ion Collider (EIC)."
  },
  {
    "filename": "2509.17599v1.txt",
    "text": "Hawking's groundbreaking prediction that black holes emit thermal radiation and ultimately evaporate remains unverified due to the extreme faintness of this radiation for stellar-mass or larger black holes. In this study, we explore a novel observational strategy to search for Hawking radiation from asteroid-mass black hole morsels -- hypothetical small black holes that may form and be ejected during catastrophic events such as binary black hole mergers. These black hole morsels are expected to emit gamma rays in the GeV-TeV range on observable timescales. We analyze data from the Fermi Large Area Telescope coinciding with the well-localized binary black hole merger GW170814, searching for delayed gamma-ray signatures associated with morsel evaporation. While we find no evidence for such emission, we place exclusion limits on morsel masses, ruling out the 4 x 10^8 kg scenario at the 95 percent confidence level for a total emitted mass of one solar mass. We also outline future directions, including the incorporation of late-time evaporation spikes, systematic application across the growing gravitational wave catalog, and the enhanced discovery potential of next-generation facilities such as the Cherenkov Telescope Array Observatory."
  },
  {
    "filename": "2509.17601v1.txt",
    "text": "Machine learning weather prediction (MLWP) models have demonstrated remarkable potential in delivering accurate forecasts at significantly reduced computational cost compared to traditional numerical weather prediction (NWP) systems. However, challenges remain in ensuring the physical consistency of MLWP outputs, particularly in deterministic settings. This study presents FastNet, a graph neural network (GNN)-based global prediction model, and investigates the impact of alternative loss function designs on improving the physical realism of its forecasts. We explore three key modifications to the standard mean squared error (MSE) loss: (1) a modified spherical harmonic (MSH) loss that penalises spectral amplitude errors to reduce blurring and enhance small-scale structure retention; (2) inclusion of horizontal gradient terms in the loss to suppress non-physical artefacts; and (3) an alternative wind representation that decouples speed and direction to better capture extreme wind events. Results show that while the MSH and gradient-based losses \\textit{alone} may slightly degrade RMSE scores, when trained in combination the model exhibits very similar MSE performance to an MSE-trained model while at the same time significantly improving spectral fidelity and physical consistency. The alternative wind representation further improves wind speed accuracy and reduces directional bias. Collectively, these findings highlight the importance of loss function design as a mechanism for embedding domain knowledge into MLWP models and advancing their operational readiness."
  },
  {
    "filename": "2509.17602v1.txt",
    "text": "Quadrat images are essential for ecological studies, as they enable standardized sampling, the assessment of plant biodiversity, long-term monitoring, and large-scale field campaigns. These images typically cover an area of fifty centimetres or one square meter, and botanists carefully identify all the species present. Integrating AI could help specialists accelerate their inventories and expand the spatial coverage of ecological studies. To assess progress in this area, the PlantCLEF 2025 challenge relies on a new test set of 2,105 high-resolution multi-label images annotated by experts and covering around 400 species. It also provides a large training set of 1.4 million individual plant images, along with vision transformer models pre-trained on this data. The task is formulated as a (weakly labelled) multi-label classification problem, where the goal is to predict all species present in a quadrat image using single-label training data. This paper provides a detailed description of the data, the evaluation methodology, the methods and models used by participants, and the results achieved."
  },
  {
    "filename": "2509.17614v1.txt",
    "text": "To reduce computation times of simulations involving gas fueled internal combustion engines (ICEs), a model is developed that determines non-ideal nozzle exit conditions to spare expensive simulations of internal injector flows. The model, to which we will refer as the Global Conservation Model (GCM), computes nozzle exit values based on reservoir conditions, and a discharge and momentum coefficient. These coefficients can be obtained via common flow bench experiments or a validated numerical simulation setup. Furthermore, it allows to use real gas thermodynamics which is often needed for ICE applications. The model is validated for three commonly used injector types, injection pressures up to 300 bar, subsonic and choked injections, and at flow bench and engine conditions using numerical simulations. If real gas thermodynamics is applied, differences with the simulated nozzle conditions are typically within 1.5% uncertainty, which is within the typical range of experimental momentum flow measurements found in literature. Differences in mass and momentum flow obtained by the numerical simulations between flow bench and engine conditions are negligible, indicating that flow bench measurements can be applied to engine conditions. Once an injector is characterized for mass and momentum flow, applying the GCM can reduce computation times by more than a factor 8, while accurately simulating the spatial and temporal jet development. With the reduction in computation time, the GCM reduces costs associated with numerical simulations and accelerates research and design of efficient and low-emission gas fueled ICEs."
  },
  {
    "filename": "2509.17617v1.txt",
    "text": "Continued miniaturization of transistors is critical for sustaining advances in computing performance, energy efficiency, and integration density. A central nanoscale challenge is controlling gate leakage through ultrathin dielectrics. In evaluating candidate insulators, permittivity and bandgap are often emphasized; however, interfaces between two-dimensional (2D) semiconductors and gate dielectrics typically form a van der Waals (vdW) gap whose electronic properties are underappreciated. Using first-principles calculations and analytical modeling supported by experiment, we find typical vdW gaps of about 1.4 \\AA{} with an effective dielectric constant near 2, which adds roughly 2.7 \\AA{} to the equivalent oxide thickness (EOT). The vdW gap serves as an additional tunneling barrier that can reduce gate leakage by one to two orders of magnitude, but it also introduces parasitic capacitance that can offset the benefits of high-k dielectrics. We introduce a dimensionless figure of merit that combines dielectric screening, tunneling suppression, and thickness-dependent permittivity of ultrathin oxides to predict the minimum achievable EOT for a target gate-leakage current in the presence of a vdW gap. Although some materials may benefit from vdW gaps, our results indicate they often impose severe constraints on further scaling. In particular, due to the vdW gap, most currently considered insulators are unlikely to scale to an EOT of 5 \\AA{} as targeted by the IRDS roadmap for future nodes. As a potential alternative, we examine ``zippered'' structures in which quasi-covalent bonding between 2D layers eliminates the vdW gap while avoiding dangling bonds."
  },
  {
    "filename": "2509.17621v1.txt",
    "text": "Accurate battery modeling is essential for reliable state estimation in modern applications, such as predicting the remaining discharge time and remaining discharge energy in battery management systems. Existing approaches face several limitations: model-based methods require a large number of parameters; data-driven methods rely heavily on labeled datasets; and current physics-informed neural networks (PINNs) often lack aging adaptation, or still depend on many parameters, or continuously regenerate states. In this work, we propose SeqBattNet, a discrete-state PINN with built-in aging adaptation for battery modeling, to predict terminal voltage during the discharge process. SeqBattNet consists of two components: (i) an encoder, implemented as the proposed HRM-GRU deep learning module, which generates cycle-specific aging adaptation parameters; and (ii) a decoder, based on the equivalent circuit model (ECM) combined with deep learning, which uses these parameters together with the input current to predict voltage. The model requires only three basic battery parameters and, when trained on data from a single cell, still achieves robust performance. Extensive evaluations across three benchmark datasets (TRI, RT-Batt, and NASA) demonstrate that SeqBattNet significantly outperforms classical sequence models and PINN baselines, achieving consistently lower RMSE while maintaining computational efficiency."
  },
  {
    "filename": "2509.17625v1.txt",
    "text": "In this paper, we present the first systematic comparison of Data Assimilation (DA) and Likelihood-Based Inference (LBI) in the context of Agent-Based Models (ABMs). These models generate observable time series driven by evolving, partially-latent microstates. Latent states need to be estimated to align simulations with real-world data -- a task traditionally addressed by DA, especially in continuous and equation-based models such as those used in weather forecasting. However, the nature of ABMs poses challenges for standard DA methods. Solving such issues requires adaptation of previous DA techniques, or ad-hoc alternatives such as LBI. DA approximates the likelihood in a model-agnostic way, making it broadly applicable but potentially less precise. In contrast, LBI provides more accurate state estimation by directly leveraging the model's likelihood, but at the cost of requiring a hand-crafted, model-specific likelihood function, which may be complex or infeasible to derive. We compare the two methods on the Bounded-Confidence Model, a well-known opinion dynamics ABM, where agents are affected only by others holding sufficiently similar opinions. We find that LBI better recovers latent agent-level opinions, even under model mis-specification, leading to improved individual-level forecasts. At the aggregate level, however, both methods perform comparably, and DA remains competitive across levels of aggregation under certain parameter settings. Our findings suggest that DA is well-suited for aggregate predictions, while LBI is preferable for agent-level inference."
  },
  {
    "filename": "2509.17631v1.txt",
    "text": "The linear viscoelastic response of dilute solutions of semiflexible polymers is studied using Brownian dynamics simulations of coarse-grained bead-spring chains. The springs obey the FENE-Fraenkel force law, a bending potential is used to capture chain stiffness and hydrodynamic interactions are included through the Rotne-Prager-Yamakawa tensor. By calculating the relaxation modulus following a step strain, we demonstrate that the bead-spring chain behaves like an inextensible semiflexible rod over a wide time window with an appropriate choice of spring stiffness and chain extensibility. In the absence of hydrodynamic interactions, our results agree with the existing theoretical predictions for the linear viscoelastic response of free-draining, inextensible, semiflexible rods in dilute solutions. It is shown that at intermediate times, the stress relaxation modulus exhibits power law behaviour, with the exponent ranging from $(-1/2)$ for flexible chains to $(-5/4)$ for highly rigid chains. At long times, rigid chains undergo orientational relaxation, while flexible chains exhibit Rouse relaxation. Hydrodynamic interactions are found to effect the behaviour at intermediate and long times, with the difference from free-draining behaviour increasing with increasing chain flexibility. Computations of the frequency dependence of loss and storage moduli are found to be in good agreement with experimental data for a wide variety of systems involving semiflexible polymers of varying stiffness across a broad frequency range."
  },
  {
    "filename": "2509.17634v1.txt",
    "text": "Thermalization of a closed chaotic quantum system is commonly addressed in terms of the eigenstate ther- malization hypothesis (ETH). An alternative approach uses the Bohigas-Giannoni-Schmit (BGS) conjecture. The comparison shows that the two approaches differ significantly. In contrast to ETH, BGS fully uses the statistical properies of the chaotic Hamiltonian. In both approaches, the criterion for thermalization is similar. BGS goes beyond ETH in predicting quantitatively the time dependence of thermalization."
  },
  {
    "filename": "2509.17637v1.txt",
    "text": "The ability of computational fluid dynamics (CFD) models to predict flow boiling at high heat flux and high flow velocity conditions has been investigated. High heat fluxes of about 10 MW/m 2 and high flow velocities of about 10 m/s typically appear in water cooling channels of divertor target elements in fusion reactors. In particular, the heat flux partitioning model used in the two-fluid CFD formulation was studied. CFD simulations of flow boiling in realistic divertor target cooling channels were performed and compared with conservative single-phase simulations. The predictive capability of CFD models for boiling was evaluated using experimental data, covering a wide range of flow velocities and heat fluxes. Existing CFD models correctly predicted void fraction and wall temperature at low flow velocities, but showed physically irrelevant results at higher velocities (above 3 m/s) leading to wall temperature overestimation. The study identified the wall heat flux partitioning model as the main contributor to the mispredictions and thoroughly discussed the main modelling shortcomings. By analysing the impact of operating conditions, some key boiling parameters, and state-of-the-art heat flux partitioning models, improvements of the model parameters are proposed. The simulations and model analyses are performed within the framework of the ANSYS CFX code and the results are compared with flow boiling experiments in uniformly and top-heated flow channels."
  },
  {
    "filename": "2509.17653v1.txt",
    "text": "Real-time data processing of the next generation of experiments at FAIR requires reliable event reconstruction and thus depends heavily on in-situ calibration procedures. Previously, we developed a neural-network-based approach that predicts calibration parameters from continuously available environmental and operational data and validated it on the HADES Multiwire Drift Chambers (MDCs), achieving fast predictions as accurate as offline calibrations. In this work, we introduce several methodological improvements that enhance both accuracy and the ability to adapt to new data. These include changes to the input features, better offline calibration and trainable normalizations. Furthermore, by combining beam-time and cosmic-ray datasets, we demonstrate that the learned dependencies can be transferred between very different data-taking scenarios. This enables the network not only to provide real-time calibration predictions, but also to infer optimal high-voltage settings, thus establishing a practical framework for a real-time detector control during data acquisition process."
  },
  {
    "filename": "2509.17658v1.txt",
    "text": "We present FastNet version 1.0, a data-driven medium range numerical weather prediction (NWP) model based on a Graph Neural Network architecture, developed jointly between the Alan Turing Institute and the Met Office. FastNet uses an encode-process-decode structure to produce deterministic global weather predictions out to 10 days. The architecture is independent of spatial resolution and we have trained models at 1$^{\\circ}$ and 0.25$^{\\circ}$ resolution, with a six hour time step. FastNet uses a multi-level mesh in the processor, which is able to capture both short-range and long-range patterns in the spatial structure of the atmosphere. The model is pre-trained on ECMWF's ERA5 reanalysis data and then fine-tuned on additional autoregressive rollout steps, which improves accuracy over longer time horizons. We evaluate the model performance at 1.5$^{\\circ}$ resolution using 2022 as a hold-out year and compare with the Met Office Global Model, finding that FastNet surpasses the skill of the current Met Office Global Model NWP system using a variety of evaluation metrics on a number of atmospheric variables. Our results show that both our 1$^{\\circ}$ and 0.25$^{\\circ}$ FastNet models outperform the current Global Model and produce results with predictive skill approaching those of other data-driven models trained on 0.25$^{\\circ}$ ERA5."
  },
  {
    "filename": "2509.17661v1.txt",
    "text": "Monitoring the progression of neurodegenerative disease has important applications in the planning of treatment and the evaluation of future medications. Whereas much of the state-of-the-art in health monitoring from speech has been focused on classifying patients versus healthy controls, or predicting real-world health metrics, we propose here a novel measure of disease progression: the severity score. This score is derived from a model trained to minimize what we call the comparator loss. The comparator loss ensures scores follow an ordering relation, which can be based on diagnosis, clinically annotated scores, or simply the chronological order of the recordings. In addition to giving a more detailed picture than a simple discrete classification, the proposed comparator loss-based system has the potential to incorporate information from disparate health metrics, which is critical for making full use of small health-related datasets. We evaluated our proposed models based on their ability to affirmatively track the progression of patients with motor neuron disease (MND), the correlation of their output with clinical annotations such as ALSFRS-R, as well as their ability to distinguish between subjects with MND and healthy controls."
  },
  {
    "filename": "2509.17666v1.txt",
    "text": "Object insertion tasks are prone to failures under pose uncertainties and environmental variations, traditionally requiring manual finetuning or controller retraining. We present a novel approach for robust and resilient object insertion using a passively compliant soft wrist that enables safe contact absorption through large deformations, without high-frequency control or force sensing. Our method structures insertion as compliance-enabled contact formations, sequential contact states that progressively constrain degrees of freedom, and integrates automated failure recovery strategies. Our key insight is that wrist compliance permits safe, repeated recovery attempts; hence, we refer to it as compliance-enabled failure recovery. We employ a pre-trained vision-language model (VLM) that assesses each skill execution from terminal poses and images, identifies failure modes, and proposes recovery actions by selecting skills and updating goals. In simulation, our method achieved an 83% success rate, recovering from failures induced by randomized conditions--including grasp misalignments up to 5 degrees, hole-pose errors up to 20mm, fivefold increases in friction, and previously unseen square/rectangular pegs--and we further validate the approach on a real robot."
  },
  {
    "filename": "2509.17669v1.txt",
    "text": "With the rapid development of Large Language Models (LLMs), Controllable Text Generation (CTG) has become a critical technology for enhancing system reliability and user experience. Addressing the limitations of traditional methods, this paper proposes the PG-CE (Progressive Generation with Constraint Enhancement) approach, which decomposes CTG tasks into three steps: type prediction, constraint construction, and guided generation. This method employs constraint generation models to dynamically build multi-dimensional constraints including tone, expression style, and thematic focus to guide output. Experiments demonstrate that PG-CE significantly improves generation quality across multiple scenarios while maintaining text controllability, thematic relevance, and response practicality. The research developed a dataset containing 90,000 constraint-text pairs (with an 8:2 ratio between daily and other topics), effectively reflecting real-world application requirements."
  },
  {
    "filename": "2509.17674v1.txt",
    "text": "Purpose: Chest X-rays are essential for diagnosing pulmonary conditions, but limited access in resource-constrained settings can delay timely diagnosis. Electrocardiograms (ECGs), in contrast, are widely available, non-invasive, and often acquired earlier in clinical workflows. This study aims to assess whether ECG features and patient demographics can predict chest radiograph findings using an interpretable machine learning approach.   Methods: Using the MIMIC-IV database, Extreme Gradient Boosting (XGBoost) classifiers were trained to predict diverse chest radiograph findings from ECG-derived features and demographic variables. Recursive feature elimination was performed independently for each target to identify the most predictive features. Model performance was evaluated using the area under the receiver operating characteristic curve (AUROC) with bootstrapped 95% confidence intervals. Shapley Additive Explanations (SHAP) were applied to interpret feature contributions.   Results: Models successfully predicted multiple chest radiograph findings with varying accuracy. Feature selection tailored predictors to each target, and including demographic variables consistently improved performance. SHAP analysis revealed clinically meaningful contributions from ECG features to radiographic predictions.   Conclusion: ECG-derived features combined with patient demographics can serve as a proxy for certain chest radiograph findings, enabling early triage or pre-screening in settings where radiographic imaging is limited. Interpretable machine learning demonstrates potential to support radiology workflows and improve patient care."
  },
  {
    "filename": "2509.17677v1.txt",
    "text": "Large language models (LLMs) have shown strong performance on mathematical reasoning under well-posed conditions. However, real-world engineering problems require more than mathematical symbolic computation -- they need to deal with uncertainty, context, and open-ended scenarios. Existing benchmarks fail to capture these complexities. We introduce EngiBench, a hierarchical benchmark designed to evaluate LLMs on solving engineering problems. It spans three levels of increasing difficulty (foundational knowledge retrieval, multi-step contextual reasoning, and open-ended modeling) and covers diverse engineering subfields. To facilitate a deeper understanding of model performance, we systematically rewrite each problem into three controlled variants (perturbed, knowledge-enhanced, and math abstraction), enabling us to separately evaluate the model's robustness, domain-specific knowledge, and mathematical reasoning abilities. Experiment results reveal a clear performance gap across levels: models struggle more as tasks get harder, perform worse when problems are slightly changed, and fall far behind human experts on the high-level engineering tasks. These findings reveal that current LLMs still lack the high-level reasoning needed for real-world engineering, highlighting the need for future models with deeper and more reliable problem-solving capabilities. Our source code and data are available at https://github.com/EngiBench/EngiBench."
  },
  {
    "filename": "2509.17679v1.txt",
    "text": "The recent observation of multiple near-threshold structures in $e^+e^-$ annihilation-including $\\psi(3770)$, $G(3900)$, $R(3760)$, $R(3780)$, and $R(3810)$-reveals limitations in existing models of charmonium and exotic hadrons. In this Letter, we propose a unified coupled-channel description that simultaneously incorporates all five near-threshold structures using parameters constrained by hadron spectroscopy. Our model accurately reproduces the line shapes in both $D\\bar{D}$ and non-open-charm hadron (nOCH) channels, resolves the asymmetric profile of $\\psi(3770)$, and produces the $G(3900)$ bump. We identify the $R(3780)$ as the dominant manifestation of $\\psi(3770)$, attribute $R(3760)$ to $D\\bar{D} \\to \\text{nOCH}$ rescattering and suggest that $R(3810)$ arises from coupling between the $\\psi(1D)$ state and the $h_c\\pi\\pi$ channel. The significant non-$D\\bar{D}$ decay of $\\psi(3770)$ is explained by its near-threshold position. This analysis provides a coherent, unquenched framework centered on a charmonium $\\psi(1D)$ core for near-threshold phenomena and offers a predictive approach extendable to bottomonium systems and future data from Belle II."
  },
  {
    "filename": "2509.17686v1.txt",
    "text": "Depth imaging is a crucial area in Autonomous Driving Systems (ADS), as it plays a key role in detecting and measuring objects in the vehicle's surroundings. However, a significant challenge in this domain arises from missing information in Depth images, where certain points are not measurable due to gaps or inconsistencies in pixel data. Our research addresses two key tasks to overcome this challenge. First, we developed an algorithm using a multi-layered training approach to generate Depth images from a single RGB image. Second, we addressed the issue of missing information in Depth images by applying our algorithm to rectify these gaps, resulting in Depth images with complete and accurate data. We further tested our algorithm on the Cityscapes dataset and successfully resolved the missing information in its Depth images, demonstrating the effectiveness of our approach in real-world urban environments."
  },
  {
    "filename": "2509.17692v1.txt",
    "text": "Despite decades of direct and indirect searches within the Weakly Interacting Massive Particle (WIMP) framework, no conclusive results have been found in the GeV--TeV range. This has motivated exploring alternatives, including new particles and macroscopic objects. Two well-motivated scenarios are sub-GeV DM and Primordial Black Holes (PBHs). Molecular clouds (MCs), typically studied as star-forming sites, can serve as astrophysical laboratories to probe these candidates via their ionization rates. Observations show ionization levels exceeding expectations from known CR fluxes, pointing to an additional ionizing component. Here, we consider electrons and positrons from annihilating and decaying MeV DM particles, as well as Hawking radiation from evaporating PBHs, as possible contributors. We model transport driven by energy losses within the clouds. By comparing predicted ionization rates with observations, conservative constraints are set on the thermally averaged cross section $\\langle\\sigma v\\rangle$, decay lifetime $\\tau$ and PBH abundances $f_{PBH}$. The analysis assumes all the observed ionization comes from DM and adopts a 95% confidence level. Results show that, even in the most conservative case of local MCs such as L1551, these constraints are very close to the most competitive bounds from X-ray observations, while inner-Galaxy clouds like DRAGON or G1.4--1.8+87 provide stronger limits, sometimes improving X-ray and cosmological constraints. For sub-GeV DM, MCs exclude parameter space competitive with the one tested by NuSTAR, INTEGRAL, or Voyager, especially below $\\sim$30 MeV. In the PBH case, asteroid-mass black holes are restricted to a low fraction of DM, with optimistic scenarios getting close to the strongest limits. This demonstrates the potential of MCs as a novel probe in indirect DM searches."
  },
  {
    "filename": "2509.17695v1.txt",
    "text": "This research investigates how Machine Learning (ML) algorithms can assist in workload allocation strategies by detecting tasks with node affinity operators (referred to as constraint operators), which constrain their execution to a limited number of nodes. Using real-world Google Cluster Data (GCD) workload traces and the AGOCS framework, the study extracts node attributes and task constraints, then analyses them to identify suitable node-task pairings. It focuses on tasks that can be executed on either a single node or fewer than a thousand out of 12.5k nodes in the analysed GCD cluster. Task constraint operators are compacted, pre-processed with one-hot encoding, and used as features in a training dataset. Various ML classifiers, including Artificial Neural Networks, K-Nearest Neighbours, Decision Trees, Naive Bayes, Ridge Regression, Adaptive Boosting, and Bagging, are fine-tuned and assessed for accuracy and F1-scores. The final ensemble voting classifier model achieved 98% accuracy and a 1.5-1.8% misclassification rate for tasks with a single suitable node."
  },
  {
    "filename": "2509.17707v1.txt",
    "text": "The standardisation of Intermodal Loading Units (ILUs), such as containers, semi-trailers and swap bodies, has revolutionised global trade yet their efficient and robust identification remains a critical bottleneck in high-throughput ports and terminals. This paper reviews 63 empirical studies that propose computer vision (CV) based solutions. It covers the last 35 years (1990-2025), tracing the field's evolution from early digital image processing (DIP) and traditional machine learning (ML) to the current dominance of deep learning (DL) techniques. While CV offers cost-effective alternatives for other types of identification techniques, its development is hindered by the lack of publicly available benchmarking datasets. This results in high variance for the reported results such as end-to-end accuracy ranging from 5 % to 96 %. Beyond dataset limitations, this review highlights the emerging challenges especially introduced by the shift from character-based text recognition to scene-text spotting and the integration of mobile cameras (e.g. drones, sensor equipped ground vehicles) for dynamic terminal monitoring. To advance the field, the paper calls for standardised terminology, open-access datasets, shared source code, while outlining future research directions such as contextless text recognition optimised for ISO6346 codes."
  },
  {
    "filename": "2509.17712v1.txt",
    "text": "Radar-camera fusion methods have emerged as a cost-effective approach for 3D object detection but still lag behind LiDAR-based methods in performance. Recent works have focused on employing temporal fusion and Knowledge Distillation (KD) strategies to overcome these limitations. However, existing approaches have not sufficiently accounted for uncertainties arising from object motion or sensor-specific errors inherent in radar and camera modalities. In this work, we propose RCTDistill, a novel cross-modal KD method based on temporal fusion, comprising three key modules: Range-Azimuth Knowledge Distillation (RAKD), Temporal Knowledge Distillation (TKD), and Region-Decoupled Knowledge Distillation (RDKD). RAKD is designed to consider the inherent errors in the range and azimuth directions, enabling effective knowledge transfer from LiDAR features to refine inaccurate BEV representations. TKD mitigates temporal misalignment caused by dynamic objects by aligning historical radar-camera BEV features with current LiDAR representations. RDKD enhances feature discrimination by distilling relational knowledge from the teacher model, allowing the student to differentiate foreground and background features. RCTDistill achieves state-of-the-art radar-camera fusion performance on both the nuScenes and View-of-Delft (VoD) datasets, with the fastest inference speed of 26.2 FPS."
  },
  {
    "filename": "2509.17715v1.txt",
    "text": "The estimation of fill probabilities for trade orders represents a key ingredient in the optimization of algorithmic trading strategies. It is bound by the complex dynamics of financial markets with inherent uncertainties, and the limitations of models aiming to learn from multivariate financial time series that often exhibit stochastic properties with hidden temporal patterns. In this paper, we focus on algorithmic responses to trade inquiries in the corporate bond market and investigate fill probability estimation errors of common machine learning models when given real production-scale intraday trade event data, transformed by a quantum algorithm running on IBM Heron processors, as well as on noiseless quantum simulators for comparison. We introduce a framework to embed these quantum-generated data transforms as a decoupled offline component that can be selectively queried by models in low-latency institutional trade optimization settings. A trade execution backtesting method is employed to evaluate the fill prediction performance of these models in relation to their input data. We observe a relative gain of up to ~ 34% in out-of-sample test scores for those models with access to quantum hardware-transformed data over those using the original trading data or transforms by noiseless quantum simulation. These empirical results suggest that the inherent noise in current quantum hardware contributes to this effect and motivates further studies. Our work demonstrates the emerging potential of quantum computing as a complementary explorative tool in quantitative finance and encourages applied industry research towards practical applications in trading."
  },
  {
    "filename": "2509.17717v1.txt",
    "text": "We show that, in Schwarzschild equivalent mediums, the massless spin particles obey the same dynamical equation, from which we obtain remarkably simple formulae for the frequencies of the quasibound states. We find that the quasibound frequencies of different bosons can be identical at the same quantum number $l$, and the same is true of different fermions, but a quasibound frequency for bosons can never equal a quasibound frequency for fermions. These results mean that, in Schwarzschild equivalent mediums with the quasibound-state boundary conditions, characteristics of electromagnetic waves are the same as those for all the massless bosonic waves, thereby allowing electromagnetic waves to simulate gravitational waves. Our predictions can be tested in future experiments, building upon the successful preparation of Schwarzschild equivalent mediums."
  },
  {
    "filename": "2509.17723v1.txt",
    "text": "We introduce a data-driven approach for extracting two-level system (TLS) parameters-frequency $\\omega_{TLS}$, coupling strength $g$, dissipation time $T_{TLS, 1}$, and the pure dephasing time $T^{\\phi}_{TLS, 2}$, labelled as a 4-component vector $\\vec{q}$, directly from simulated spectroscopy data generated for a single TLS by a form of two-tone spectroscopy. Specifically, we demonstrate that a custom convolutional neural network model(CNN) can simultaneously predict $\\omega_{TLS}$, $g$, $T_{TLS, 1}$ and $T^{\\phi}_{TLS, 2}$ from the spectroscopy data presented in the form of images. Our results show that the model achieves superior performance to perturbation theory methods in successfully extracting the TLS parameters. Although the model, initially trained on noise-free data, exhibits a decline in accuracy when evaluated on noisy images, retraining it on a noisy dataset leads to a substantial performance improvement, achieving results comparable to those obtained under noise-free conditions. Furthermore, the model exhibits higher predictive accuracy for parameters $\\omega_{TLS}$ and $g$ in comparison to $T_{TLS, 1}$ and $T^{\\phi}_{TLS, 2}$."
  },
  {
    "filename": "2509.17726v1.txt",
    "text": "Accurate anatomical labeling of intracranial arteries is essential for cerebrovascular diagnosis and hemodynamic analysis but remains time-consuming and subject to interoperator variability. We present a deep learning-based framework for automated artery labeling from 3D Time-of-Flight Magnetic Resonance Angiography (3D ToF-MRA) segmentations (n=35), incorporating uncertainty quantification to enhance interpretability and reliability. We evaluated three convolutional neural network architectures: (1) a UNet with residual encoder blocks, reflecting commonly used baselines in vascular labeling; (2) CS-Net, an attention-augmented UNet incorporating channel and spatial attention mechanisms for enhanced curvilinear structure recognition; and (3) nnUNet, a self-configuring framework that automates preprocessing, training, and architectural adaptation based on dataset characteristics. Among these, nnUNet achieved the highest labeling performance (average Dice score: 0.922; average surface distance: 0.387 mm), with improved robustness in anatomically complex vessels. To assess predictive confidence, we implemented test-time augmentation (TTA) and introduced a novel coordinate-guided strategy to reduce interpolation errors during augmented inference. The resulting uncertainty maps reliably indicated regions of anatomical ambiguity, pathological variation, or manual labeling inconsistency. We further validated clinical utility by comparing flow velocities derived from automated and manual labels in co-registered 4D Flow MRI datasets, observing close agreement with no statistically significant differences. Our framework offers a scalable, accurate, and uncertainty-aware solution for automated cerebrovascular labeling, supporting downstream hemodynamic analysis and facilitating clinical integration."
  },
  {
    "filename": "2509.17734v1.txt",
    "text": "In recent years, great progress has been made in the field of forecasting meteorological variables. Recently, deep learning architectures have made a major breakthrough in forecasting the daily average temperature over a ten-day horizon. However, advances in forecasting events related to the maximum temperature over short horizons remain a challenge for the community. A problem that is even more complex consists in making predictions of the maximum daily temperatures in the short, medium, and long term. In this work, we focus on forecasting events related to the maximum daily temperature over medium-term periods (90 days). Therefore, instead of addressing the problem from a meteorological point of view, this article tackles it from a climatological point of view. Due to the complexity of this problem, a common approach is to frame the study as a temporal classification problem with the classes: maximum temperature \"above normal\", \"normal\" or \"below normal\". From a practical point of view, we created a large historical dataset (from 1981 to 2018) collecting information from weather stations located in South America. In addition, we also integrated exogenous information from the Pacific, Atlantic, and Indian Ocean basins. We applied the AutoGluonTS platform to solve the above-mentioned problem. This AutoML tool shows competitive forecasting performance with respect to large operational platforms dedicated to tackling this climatological problem; but with a \"relatively\" low computational cost in terms of time and resources."
  },
  {
    "filename": "2509.17738v1.txt",
    "text": "Neural collapse, i.e., the emergence of highly symmetric, class-wise clustered representations, is frequently observed in deep networks and is often assumed to reflect or enable generalization. In parallel, flatness of the loss landscape has been theoretically and empirically linked to generalization. Yet, the causal role of either phenomenon remains unclear: Are they prerequisites for generalization, or merely by-products of training dynamics? We disentangle these questions using grokking, a training regime in which memorization precedes generalization, allowing us to temporally separate generalization from training dynamics and we find that while both neural collapse and relative flatness emerge near the onset of generalization, only flatness consistently predicts it. Models encouraged to collapse or prevented from collapsing generalize equally well, whereas models regularized away from flat solutions exhibit delayed generalization. Furthermore, we show theoretically that neural collapse implies relative flatness under classical assumptions, explaining their empirical co-occurrence. Our results support the view that relative flatness is a potentially necessary and more fundamental property for generalization, and demonstrate how grokking can serve as a powerful probe for isolating its geometric underpinnings."
  },
  {
    "filename": "2509.17749v1.txt",
    "text": "Formulating information retrieval as a variant of generative modeling, specifically using autoregressive models to generate relevant identifiers for a given query, has recently attracted considerable attention. However, its application to personalized sticker retrieval remains largely unexplored and presents unique challenges: existing relevance-based generative retrieval methods typically lack personalization, leading to a mismatch between diverse user expectations and the retrieved results. To address this gap, we propose PEARL, a novel generative framework for personalized sticker retrieval, and make two key contributions: (i) To encode user-specific sticker preferences, we design a representation learning model to learn discriminative user representations. It is trained on three prediction tasks that leverage personal information and click history; and (ii) To generate stickers aligned with a user's query intent, we propose a novel intent-aware learning objective that prioritizes stickers associated with higher-ranked intents. Empirical results from both offline evaluations and online tests demonstrate that PEARL significantly outperforms state-of-the-art methods."
  },
  {
    "filename": "2509.17765v1.txt",
    "text": "We present Qwen3-Omni, a single multimodal model that, for the first time, maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages, and speech generation in 10 languages. To reduce first-packet latency in streaming synthesis, Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms. To further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license."
  },
  {
    "filename": "2509.17774v1.txt",
    "text": "The Rashomon set of decision trees (DTs) finds importance uses. Recent work showed that DTs computing the same classification function, i.e. predictive equivalent DTs, can represent a significant fraction of the Rashomon set. Such redundancy is undesirable. For example, feature importance based on the Rashomon set becomes inaccurate due the existence of predictive equivalent DTs, i.e. DTs with the same prediction for every possible input. In recent work, McTavish et al. proposed solutions for several computational problems related with DTs, including that of deciding predictive equivalent DTs. This approach, which this paper refers to as MBDSR, consists of applying the well-known method of Quine-McCluskey (QM) for obtaining minimum-size DNF (disjunctive normal form) representations of DTs, which are then used for comparing DTs for predictive equivalence. Furthermore, the minimum-size DNF representation was also applied to computing explanations for the predictions made by DTs, and to finding predictions in the presence of missing data. However, the problem of formula minimization is hard for the second level of the polynomial hierarchy, and the QM method may exhibit worst-case exponential running time and space. This paper first demonstrates that there exist decision trees that trigger the worst-case exponential running time and space of the QM method. Second, the paper shows that the MBDSR approach can produce incorrect results for the problem of deciding predictive equivalence. Third, the paper shows that any of the problems to which the minimum-size DNF representation has been applied to can in fact be solved in polynomial time, in the size of the DT. The experiments confirm that, for DTs for which the the worst-case of the QM method is triggered, the algorithms proposed in this paper are orders of magnitude faster than the ones proposed by McTavish et al."
  },
  {
    "filename": "2509.17783v1.txt",
    "text": "Optimizing and refining action execution through   exploration and interaction is a promising way for robotic   manipulation. However, practical approaches to interaction driven robotic learning are still underexplored, particularly for   long-horizon tasks where sequential decision-making, physical   constraints, and perceptual uncertainties pose significant chal lenges. Motivated by embodied cognition theory, we propose   RoboSeek, a framework for embodied action execution that   leverages interactive experience to accomplish manipulation   tasks. RoboSeek optimizes prior knowledge from high-level   perception models through closed-loop training in simulation   and achieves robust real-world execution via a real2sim2real   transfer pipeline. Specifically, we first replicate real-world   environments in simulation using 3D reconstruction to provide   visually and physically consistent environments., then we train   policies in simulation using reinforcement learning and the   cross-entropy method leveraging visual priors. The learned   policies are subsequently deployed on real robotic platforms   for execution. RoboSeek is hardware-agnostic and is evaluated   on multiple robotic platforms across eight long-horizon ma nipulation tasks involving sequential interactions, tool use, and   object handling. Our approach achieves an average success rate   of 79%, significantly outperforming baselines whose success   rates remain below 50%, highlighting its generalization and   robustness across tasks and platforms. Experimental results   validate the effectiveness of our training framework in complex,   dynamic real-world settings and demonstrate the stability of the   proposed real2sim2real transfer mechanism, paving the way for   more generalizable embodied robotic learning. Project Page:   https://russderrick.github.io/Roboseek/"
  },
  {
    "filename": "2509.17794v1.txt",
    "text": "Natural language generation (NLG) tasks are often subject to inherent variability; \\emph{e.g.} predicting the next word given a context has multiple valid responses, evident when asking multiple humans to complete the task. While having language models (LMs) that are aligned pluralistically, so that they are able to reproduce well the inherent diversity in perspectives of an entire population of interest is clearly beneficial, \\citet{ilia2024predict} show that LMs do not reproduce this type of linguistic variability well. They speculate this inability might stem from the lack of consistent training of LMs with data reflecting this type of inherent variability. As such, we investigate whether training LMs on multiple plausible word continuations per context can improve their ability to reproduce human linguistic variability for next-word prediction. We employ fine-tuning techniques for pre-trained and instruction-tuned models; and demonstrate their potential when fine-tuning GPT-2 and Mistral-7B-IT, using Provo Corpus. Our evaluation, which measures divergence among empirically estimated human and model next-word distributions across contexts before and after fine-tuning, shows that our multi-label fine-tuning improves the LMs' ability to reproduce linguistic variability; both for contexts that admit higher and lower variability."
  },
  {
    "filename": "2509.17800v1.txt",
    "text": "The behavior of honeybees is an important ecological phenomenon not only in terms of honey and beeswax production but also due to the proliferation of flora and fauna around it. The best way to study this significant phenomenon is by non-invasive monitoring of beehives using the sounds produced by various body movements that give out audio signals which can be exploited for various predictions related to the objectives mentioned above. This study investigates the application of Convolutional Neural Networks to classify and monitor different hive states with the help of joint time and frequency image representations such as Spectrogram, Mel-Spectrogram, Smoothed-Spectrogram, and Cochleagram. Our findings indicate that the Cochleagram outperformed all the other representations, achieving an accuracy of 98.31% on unseen data. Furthermore, we employed various strategies including pruning, quantization, and knowledge distillation to optimize the network and prevent any potential issues with model size. With these optimizations, the network size was lowered by 91.8% and the inference time was accelerated by 66%, increasing its suitability for real-time applications. Thus our study emphasizes the significance of using optimization approaches to minimize model size, avoid deployment problems, and expedite inference for real-time application as well as the selection of an appropriate time-frequency representation for optimal performance."
  },
  {
    "filename": "2509.17808v1.txt",
    "text": "World models have shown potential in artificial intelligence by predicting and reasoning about world states beyond direct observations. However, existing approaches are predominantly evaluated in synthetic environments or constrained scene settings, limiting their validation in real-world contexts with broad spatial coverage and complex semantics. Meanwhile, remote sensing applications urgently require spatial reasoning capabilities for disaster response and urban planning. This paper bridges these gaps by introducing the first framework for world modeling in remote sensing. We formulate remote sensing world modeling as direction-conditioned spatial extrapolation, where models generate semantically consistent adjacent image tiles given a central observation and directional instruction. To enable rigorous evaluation, we develop RSWISE (Remote Sensing World-Image Spatial Evaluation), a benchmark containing 1,600 evaluation tasks across four scenarios: general, flood, urban, and rural. RSWISE combines visual fidelity assessment with instruction compliance evaluation using GPT-4o as a semantic judge, ensuring models genuinely perform spatial reasoning rather than simple replication. Afterwards, we present RemoteBAGEL, a unified multimodal model fine-tuned on remote sensing data for spatial extrapolation tasks. Extensive experiments demonstrate that RemoteBAGEL consistently outperforms state-of-the-art baselines on RSWISE."
  },
  {
    "filename": "2509.17810v1.txt",
    "text": "Accurate prediction of molecular properties in complex chemical systems is crucial for accelerating material discovery and chemical innovation.However,current computational methods often struggle to capture the intricate compositional interplay across complex chemical systems,from intramolecular bonds to intermolecular forces.In this work,we introduce MesoNet,a novel framework founded on the principle of multi-representation learning and specifically designed for multi-molecule modeling.The core innovation of MesoNet lies in the construction of context-aware representation-dynamically enriched atomic descriptors generated via Neural Circuit Policies. These parameters efficiently capture both intrinsic atomic properties and their dynamic compositional context through a cross-attention mechanism spanning both intramolecular and intermolecular message passing. Driven by this mechanism,the influence of the mixed system is progressively applied to each molecule and atom, making message passing both efficient and meaningful.Comprehensive evaluations across diverse public datasets, spanning both pure components and mixtures,demonstrate that MesoNet achieves superior accuracy and enhanced chemical interpretability for molecular properties.This work establishes a powerful,interpretable approach for modeling compositional complexity,aiming to advance chemical simulation and design."
  },
  {
    "filename": "2509.17811v1.txt",
    "text": "Accurate prediction of road accidents remains challenging due to intertwined spatial, temporal, and contextual factors in urban traffic. We propose MSGAT-GRU, a multi-scale graph attention and recurrent model that jointly captures localized and long-range spatial dependencies while modeling sequential dynamics. Heterogeneous inputs, such as traffic flow, road attributes, weather, and points of interest, are systematically fused to enhance robustness and interpretability. On the Hybrid Beijing Accidents dataset, MSGAT-GRU achieves an RMSE of 0.334 and an F1-score of 0.878, consistently outperforming strong baselines. Cross-dataset evaluation on METR-LA under a 1-hour horizon further supports transferability, with RMSE of 6.48 (vs. 7.21 for the GMAN model) and comparable MAPE. Ablations indicate that three-hop spatial aggregation and a two-layer GRU offer the best accuracy-stability trade-off. These results position MSGAT-GRU as a scalable and generalizable model for intelligent transportation systems, providing interpretable signals that can inform proactive traffic management and road safety analytics."
  },
  {
    "filename": "2509.17816v1.txt",
    "text": "Self-supervised learning (SSL) has emerged as a central paradigm for training foundation models by leveraging large-scale unlabeled datasets, often producing representations with strong generalization capabilities. These models are typically pre-trained on general-purpose datasets such as ImageNet and subsequently adapted to various downstream tasks through finetuning. While recent advances have explored parameter-efficient strategies for adapting pre-trained models, extending SSL pre-training itself to new domains - particularly under limited data regimes and for dense prediction tasks - remains underexplored. In this work, we address the problem of adapting vision foundation models to new domains in an unsupervised and data-efficient manner, specifically targeting downstream semantic segmentation. We propose GLARE (Global Local and Regional Enforcement), a novel continual self-supervised pre-training task designed to enhance downstream segmentation performance. GLARE introduces patch-level augmentations to encourage local consistency and incorporates a regional consistency constraint that leverages spatial semantics in the data. For efficient continual pre-training, we initialize Vision Transformers (ViTs) with weights from existing SSL models and update only lightweight adapter modules - specifically UniAdapter - while keeping the rest of the backbone frozen. Experiments across multiple semantic segmentation benchmarks on different domains demonstrate that GLARE consistently improves downstream performance with minimal computational and parameter overhead."
  },
  {
    "filename": "2509.17825v1.txt",
    "text": "This work explores the role of oxygen in industrial methane oxidation. Oxygen, a well-known oxidizing agent, drives CH$_4$ conversion to CO$_2$ and H$_2$O. We report how oxygen influences oxidation on single Pd and PdO clusters supported on CeO$_2$(111). Oxygen is introduced by (1) lattice O in PdO and (2) O$_2$ adsorption on an isolated Pd atom, forming PdO$_x$ clusters. Density-functional theory (DFT) mapped multiple reaction pathways on the Pd$_1$/PdO$_1$@CeO$_2$(111) surface; both Pd and PdO clusters were found to thermodynamically favour methane activation. The computed barrier for CH$_4$ activation is 0.63 eV on PdO$_1$@CeO$_2$(111). A single Pd atom markedly accelerates O$_2$ dissociation to PdO$_2$, and the presence of lattice oxygen lowers this barrier by 0.36 eV relative to an oxygen-deficient surface, enhancing catalytic efficiency. Reaction selectivity, coverage-dependent production rates, degree of rate control (DRC), and intrinsic turnover frequency (TOF) were quantified through steady-state microkinetic modelling. The simulations predict full conversion of CH$_4$ to CO$_2$ and H$_2$O above 600 K, whereas partial-oxidation intermediates dominate at lower temperature and high O coverage. Rate constants for all elementary steps were derived via the Sure Independence Screening and Sparsifying Operator (SISSO) symbolic-regression method, yielding a concise predictive expression based on charge, coordination number, and key Pd-O/C-H distances. These combined DFT-microkinetic-SISSO results clarify oxygen's mechanistic participation and provide practical guidelines for designing Pd/CeO$_2$ catalysts with improved activity toward methane oxidation, a reaction of pressing environmental and industrial importance."
  },
  {
    "filename": "2509.17827v1.txt",
    "text": "This paper addresses two interrelated problems of the nonlinear filtering mechanism and fast attitude filtering with the matrix Fisher distribution (MFD) on the special orthogonal group. By analyzing the distribution evolution along Bayes' rule, we reveal two essential properties that enhance the performance of Bayesian attitude filters with MFDs, particularly in challenging conditions, from a theoretical viewpoint.   Benefiting from the new understanding of the filtering mechanism associated with MFDs, two closed-form filters with MFDs is then proposed. These filters avoid the burdensome computations in previous MFD-based filters by introducing linearized error systems with right-invariant errors but retaining the two advantageous properties. Moreover, we leverage the two properties and closed-form filtering iteration to prove the almost-global exponential stability of the proposed filter with right-invariant error for the single-axis rotation, which, to our knowledge, is not achieved by existing directional statistics-based filters. Numerical simulations demonstrate that the proposed filters are significantly more accurate than the classic invariant Kalman filter. Besides, they are also as accurate as recent MFD-based Bayesian filters in challenging circumstances with large initial error and measurement uncertainty but consumes far less computation time (about 1/5 to 1/100 of previous MFD-based attitude filters)."
  },
  {
    "filename": "2509.17830v1.txt",
    "text": "Generation of Artificial Intelligence (AI) texts in important works has become a common practice that can be used to misuse and abuse AI at various levels. Traditional AI detectors often rely on document-level classification, which struggles to identify AI content in hybrid or slightly edited texts designed to avoid detection, leading to concerns about the model's efficiency, which makes it hard to distinguish between human-written and AI-generated texts. A sentence-level sequence labeling model proposed to detect transitions between human- and AI-generated text, leveraging nuanced linguistic signals overlooked by document-level classifiers. By this method, detecting and segmenting AI and human-written text within a single document at the token-level granularity is achieved. Our model combines the state-of-the-art pre-trained Transformer models, incorporating Neural Networks (NN) and Conditional Random Fields (CRFs). This approach extends the power of transformers to extract semantic and syntactic patterns, and the neural network component to capture enhanced sequence-level representations, thereby improving the boundary predictions by the CRF layer, which enhances sequence recognition and further identification of the partition between Human- and AI-generated texts. The evaluation is performed on two publicly available benchmark datasets containing collaborative human and AI-generated texts. Our experimental comparisons are with zero-shot detectors and the existing state-of-the-art models, along with rigorous ablation studies to justify that this approach, in particular, can accurately detect the spans of AI texts in a completely collaborative text. All our source code and the processed datasets are available in our GitHub repository."
  },
  {
    "filename": "2509.17833v1.txt",
    "text": "Copper is a highly promising catalyst for the electrochemical CO$_2$ reduction reaction (CO2RR) since it is the only pure metal that can form highly added-value products such as ethylene and ethanol. Since the CO2RR takes place in aqueous solution, the detailed atomic structure of the water-copper interface is essential for unraveling the key reaction mechanisms. In this study, we investigate copper-water interfaces exhibiting nanometer-scale roughnesses. We introduce two molecular dynamics protocols to create rough copper surfaces, which are subsequently brought into contact with water. From these interfaces, we sample additional training configurations from machine-learning-interatomic-potential-driven molecular dynamics simulations containing hundreds of thousands of atoms. An active learning workflow is developed to identify regions with high spatially resolved uncertainty and convert them into DFT-feasible cells through a modified amorphous matrix embedding approach. Finally, we analyze the local environments at the interface using unsupervised machine-learning techniques. Unique environments emerge on the rough copper surfaces absent from model systems, including stacking-fault-induced configurations and undercoordinated corner atoms. Notably, corner atoms consistently feature chemisorbed water molecules in our simulations, indicating their potential importance in catalytic processes."
  },
  {
    "filename": "2509.17836v1.txt",
    "text": "Machine Learning (ML) techniques have shown strong potential for network traffic analysis; however, their effectiveness depends on access to representative, up-to-date datasets, which is limited in cybersecurity due to privacy and data-sharing restrictions. To address this challenge, Federated Learning (FL) has recently emerged as a novel paradigm that enables collaborative training of ML models across multiple clients while ensuring that sensitive data remains local. Nevertheless, Federated Averaging (FedAvg), the canonical FL algorithm, has proven poor convergence in heterogeneous environments where data distributions are non-independent and identically distributed (i.i.d.) and client datasets are unbalanced, conditions frequently observed in cybersecurity contexts. To overcome these challenges, several alternative FL strategies have been developed, yet their applicability to network intrusion detection remains insufficiently explored. This study systematically reviews and evaluates a range of FL methods in the context of intrusion detection for DDoS attacks. Using a dataset of network attacks within a Kubernetes-based testbed, we assess convergence efficiency, computational overhead, bandwidth consumption, and model accuracy. To the best of our knowledge, this is the first comparative analysis of FL algorithms for intrusion detection under realistic non-i.i.d. and unbalanced settings, providing new insights for the design of robust, privacypreserving network security solutions."
  },
  {
    "filename": "2509.17843v1.txt",
    "text": "This study presents a machine learning approach to predict the Curie temperature in binary alloys, specifically focusing on the Fe-Pt, Fe-Ni, Fe-Pd, and Co-Pt compounds within a concentration range of 10 to 90 atomic percent. The optimal mathematical algorithm for this task is the Voting Ensemble algorithm, which combines the predictions from multiple individual models to produce a final prediction. The results are validated against classical methods for calculating Curie temperatures. The experimental findings indicate that factors such as external pressure, atomic ordering, and alloy composition have a significant influence on the Curie temperatures in all examined binary systems. These factors can be leveraged to design alloys with specific Curie temperatures. Moreover, the proposed features, feature analysis algorithms, and computational methods pave the way for advancements across various materials, including ternary alloys, bulk materials, and nanomaterials, inspiring innovation in the field."
  },
  {
    "filename": "2509.17848v1.txt",
    "text": "As part of the pilot survey of the Widefield ASKAP L-band Legacy All-sky Survey (WALLABY), high-resolution neutral atomic hydrogen (HI) observations of the dwarf galaxy pair NGC 4532/DDO 137 (WALLABY J123424+062511) have revealed a huge (48 kpc) bridge of gas between the two galaxies, as well as numerous arms and clouds which connect with the even longer (0.5 Mpc) tail of gas previously discovered with the Arecibo telescope. Our modelling suggests that a combination of ram pressure and tidal forces are responsible for the nature of the system. Although the pair lies well outside of the virial radius of the Virgo cluster, ram pressure due to infall through an extensive envelope of hot gas around the cluster is most likely responsible for the HI tail. Over a timescale of 1 Gyr, the predicted electron density ($1.2\\times 10^{-5}$ cm$^{-3}$) and infall velocity (880 km s$^{-1}$) are probably sufficient to explain the extensive stripping from the common gaseous envelope of NGC 4532/DDO 137. The ongoing tidal interaction with the Virgo cluster appears to have prevented a rapid merger of the binary pair, with the mutual tidal interaction between the galaxy pair being responsible for raising gas from the outer parts of the galaxy potential wells into the HI bridge and common envelope. The NGC 4532/DDO 137 system mirrors many of the physical features of the Magellanic System, and may lead to a better understanding of that system, as well as casting more light on the relative importance of interaction mechanisms in the outskirts of dynamically young galaxy clusters such as Virgo."
  },
  {
    "filename": "2509.17850v1.txt",
    "text": "Accurate trajectory prediction of surrounding vehicles (SVs) is crucial for autonomous driving systems to avoid misguided decisions and potential accidents. However, achieving reliable predictions in highly dynamic and complex traffic scenarios remains a significant challenge. One of the key impediments lies in the limited effectiveness of current approaches to capture the multi-modal behaviors of drivers, which leads to predicted trajectories that deviate from actual future motions. To address this issue, we propose SocialTraj, a novel trajectory prediction framework integrating social psychology principles through social value orientation (SVO). By utilizing Bayesian inverse reinforcement learning (IRL) to estimate the SVO of SVs, we obtain the critical social context to infer the future interaction trend. To ensure modal consistency in predicted behaviors, the estimated SVOs of SVs are embedded into a conditional denoising diffusion model that aligns generated trajectories with historical driving styles. Additionally, the planned future trajectory of the ego vehicle (EV) is explicitly incorporated to enhance interaction modeling. Extensive experiments on NGSIM and HighD datasets demonstrate that SocialTraj is capable of adapting to highly dynamic and interactive scenarios while generating socially compliant and behaviorally consistent trajectory predictions, outperforming existing baselines. Ablation studies demonstrate that dynamic SVO estimation and explicit ego-planning components notably improve prediction accuracy and substantially reduce inference time."
  },
  {
    "filename": "2509.17860v1.txt",
    "text": "The T2K ND280 upgrade aims to reduce the systematic uncertainty of the CP-violating phase, $\\delta_{CP}$, to reject non-CP violation hypothesis at $3\\sigma$ confidence level. A crucial component of the ND280 upgrade, alongside the Super Fine Grained Detector (SuperFGD) and two High-Angle Time Projection Chambers (TPCs), is the Time-of-Flight (ToF) detector, which significantly enhances background rejection and particle identification capabilities. The ToF detector features six modules in a cube configuration, each with 20 plastic scintillator bars measuring $\\text{220}\\times\\text{12}\\times\\text{1}\\,\\text{cm}^3$ and is equipped with Silicon Photomultiplier (SiPM) arrays at both ends to capture scintillation light. This letter outlines the modelling of the detector response and the signal reconstruction process."
  },
  {
    "filename": "2509.17861v1.txt",
    "text": "Plasma turbulence cascading from MHD to kinetic scales in the heliospheric plasma is believed to play a key role in coronal heating and fast solar wind acceleration, but the properties of the turbulence remain poorly constrained by observations. Here we compare the ion-scale density fluctuation levels inferred from the properties of solar radio bursts with the magnetic field fluctuation levels obtained through in-situ measurements in the inner heliosphere. We find that the observed magnetic and density fluctuation amplitudes are consistent with excitation by kinetic Alfv\\'en waves and/or KAW structures over broad range of distances from the Sun. We then use the radio diagnostics and the KAW scenario to deduce the radial variation of magnetic fluctuation amplitudes in regions close to the Sun where in-situ measurements cannot be obtained. Further, we calculate the energy cascade rate (plasma heating rate) profile over a region that extends from the low corona ($\\sim 0.1$~R$_\\odot$) into the heliosphere (out to $\\sim 1$~au), and compare it to the energy deposition rate required to drive the solar wind. The cascade rate agrees with the available in-situ measurements and also provides predictions closer than $\\sim 10$~R$_\\odot$ where in-situ approaches are not available. The results provide unique diagnostics of the ion-scale plasma turbulence amplitude and energy cascade rate spanning over three orders of magnitude in solar distance."
  },
  {
    "filename": "2509.17862v1.txt",
    "text": "Catalysis at solid-liquid interfaces plays a central role in the advancement of energy storage and sustainable chemical production technologies. By enabling accurate, long-time scale simulations, machine learning (ML) models have the potential to accelerate the discovery of (electro)catalysts. While prior Open Catalyst datasets (OC20 and OC22) have advanced the field by providing large-scale density functional theory (DFT) data of adsorbates on surfaces at solid-gas interfaces, they do not capture the critical role of solvent and electrolyte effects at solid-liquid interfaces. To bridge this gap, we introduce the Open Catalyst 2025 (OC25) dataset, consisting of 7,801,261 calculations across 1,511,270 unique explicit solvent environments. OC25 constitutes the largest and most diverse solid-liquid interface dataset that is currently available and provides configurational and elemental diversity: spanning 88 elements, commonly used solvents/ions, varying solvent layers, and off-equilibrium sampling. State-of-the-art models trained on the OC25 dataset exhibit energy, force, and solvation energy errors as low as 0.1 eV, 0.015 eV/\\r{A}, and 0.04 eV, respectively; significantly lower than than the recently released Universal Models for Atoms (UMA-OC20). Additionally, we discuss the impact of the quality of DFT-calculated forces on model training and performance. The dataset and accompanying baseline models are made openly available for the community. We anticipate the dataset to facilitate large length-scale and long-timescale simulations of catalytic transformations at solid-liquid interfaces, advancing molecular-level insights into functional interfaces and enabling the discovery of next-generation energy storage and conversion technologies."
  },
  {
    "filename": "2509.17874v1.txt",
    "text": "Large neural networks are typically trained for a fixed computational budget, creating a rigid trade-off between performance and efficiency that is ill-suited for deployment in resource-constrained or dynamic environments. Existing approaches to this problem present a difficult choice: training a discrete collection of specialist models is computationally prohibitive, while dynamic methods like slimmable networks often lack the flexibility to be applied to large, pre-trained foundation models. In this work, we propose Nested Subspace Networks (NSNs), a novel architectural paradigm that enables a single model to be dynamically and granularly adjusted across a continuous spectrum of compute budgets at inference time. The core of our approach is to re-parameterize linear layers to satisfy a nested subspace property, such that the function computed at a given rank is a strict subspace of the function at any higher rank. We show that this entire hierarchy of models can be optimized jointly via an uncertainty-aware objective that learns to balance the contributions of different ranks based on their intrinsic difficulty. We demonstrate empirically that NSNs can be surgically applied to pre-trained LLMs and unlock a smooth and predictable compute-performance frontier. For example, a single NSN-adapted model can achieve a 50% reduction in inference FLOPs with only a 5 percentage point loss in accuracy. Our findings establish NSNs as a powerful framework for creating the next generation of adaptive foundation models."
  },
  {
    "filename": "2509.17884v1.txt",
    "text": "When do locomotion controllers require reasoning about nonlinearities? In this work, we show that a whole-body model-predictive controller using a simple linear time-invariant approximation of the whole-body dynamics is able to execute basic locomotion tasks on complex legged robots. The formulation requires no online nonlinear dynamics evaluations or matrix inversions. We demonstrate walking, disturbance rejection, and even navigation to a goal position without a separate footstep planner on a quadrupedal robot. In addition, we demonstrate dynamic walking on a hydraulic humanoid, a robot with significant limb inertia, complex actuator dynamics, and large sim-to-real gap."
  },
  {
    "filename": "2509.17885v1.txt",
    "text": "Early-exit neural networks reduce inference cost by enabling confident predictions at intermediate layers. However, joint training often leads to gradient interference, with deeper classifiers dominating optimization. We propose Confidence-Gated Training (CGT), a paradigm that conditionally propagates gradients from deeper exits only when preceding exits fail. This encourages shallow classifiers to act as primary decision points while reserving deeper layers for harder inputs. By aligning training with the inference-time policy, CGT mitigates overthinking, improves early-exit accuracy, and preserves efficiency. Experiments on the Indian Pines and Fashion-MNIST benchmarks show that CGT lowers average inference cost while improving overall accuracy, offering a practical solution for deploying deep models in resource-constrained environments."
  },
  {
    "filename": "2509.17898v1.txt",
    "text": "Robustness certification against bounded input noise or adversarial perturbations is increasingly important for deployment recurrent neural networks (RNNs) in safety-critical control applications. To address this challenge, we present RNN-SDP, a relaxation based method that models the RNN's layer interactions as a convex problem and computes a certified upper bound on the Lipschitz constant via semidefinite programming (SDP). We also explore an extension that incorporates known input constraints to further tighten the resulting Lipschitz bounds. RNN-SDP is evaluated on a synthetic multi-tank system, with upper bounds compared to empirical estimates. While incorporating input constraints yields only modest improvements, the general method produces reasonably tight and certifiable bounds, even as sequence length increases. The results also underscore the often underestimated impact of initialization errors, an important consideration for applications where models are frequently re-initialized, such as model predictive control (MPC)."
  },
  {
    "filename": "2509.17908v1.txt",
    "text": "The hyper-radial barrier strongly hinders formation of more than three clusters. We investigate how well the dominating cluster components in $^7$He and $^8$He, respectively can be described as $\\alpha$+$n$+$^2n$ and $\\alpha$+$^2n$+$^2n$, where $^2n$ is the dineutron. Effective interactions compatible with $^5$He and $^6$He are used. We vary the lesser known $n$-$^2n$ and $^2n$-$^2n$ interactions, where very small strengths are required. We provide energies, radii, and partial wave decomposition of all computed, predicted or measured, ground and resonance states. We predict substructures within each of the three-body quantum states. We also calculate the neutron-structure sensitive invariant mass spectrum of the four-nucleon system, after fast removal of the $\\alpha$-particle from $^8$He. We show that all available experimental information are fairly well reproduced. Very little room is left for variation of the effective interaction parameters. Thus, the dominating features of the subsequently derived reaction and structure properties are well supported."
  },
  {
    "filename": "2509.17917v1.txt",
    "text": "Recent advances in GUI agents have achieved remarkable grounding and action-prediction performance, yet existing models struggle with unreliable reward signals and limited online trajectory generation. In this paper, we introduce Orcust, a framework that integrates Principle-Constrained Reward Modeling (PCRM) and Online VM-Grounded Trajectory Construction (OVTC) to enhance reasoning reliability and data efficiency in interactive GUI tasks. We leverages environment-verifiable and LLM-derived principle to enforce interpretable reward signals that constrain long chain-of-thought reasoning and rule-based feedback. OVTC spins up instrumented virtual machines to autonomously collect structured GUI interaction trajectories with explicit procedural and structural objectives, enabling the training of a stepwise reward model that robustly captures human preferences and adheres to task-specific constraints. Extensive experiments on standard GUI benchmarks covering perceptual grounding, foundational operations, and end-to-end task execution reveal that Orcust achieves state-of-the-art performance, improving by 22.2\\% on ScreenSpot and 23.9\\% on ScreenSpot-Pro over the base model (i.e. Qwen2.5-VL-7B). The results demonstrate Orcust's effectiveness in enhancing the reasoning, adaptability and scalability of GUI agents across various environments and task complexities."
  },
  {
    "filename": "2509.17931v1.txt",
    "text": "Accurate multi-needle localization in intraoperative CT images is crucial for optimizing seed placement in pelvic seed implant brachytherapy. However, this task is challenging due to poor image contrast and needle adhesion. This paper presents a novel approach that reframes needle localization as a tip-handle detection and matching problem to overcome these difficulties. An anchor-free network, based on HRNet, is proposed to extract multi-scale features and accurately detect needle tips and handles by predicting their centers and orientations using decoupled branches for heatmap regression and polar angle prediction. To associate detected tips and handles into individual needles, a greedy matching and merging (GMM) method designed to solve the unbalanced assignment problem with constraints (UAP-C) is presented. The GMM method iteratively selects the most probable tip-handle pairs and merges them based on a distance metric to reconstruct 3D needle paths. Evaluated on a dataset of 100 patients, the proposed method demonstrates superior performance, achieving higher precision and F1 score compared to a segmentation-based method utilizing the nnUNet model,thereby offering a more robust and accurate solution for needle localization in complex clinical scenarios."
  },
  {
    "filename": "2509.17940v1.txt",
    "text": "End-to-end autonomous driving has substantially progressed by directly predicting future trajectories from raw perception inputs, which bypasses traditional modular pipelines. However, mainstream methods trained via imitation learning suffer from critical safety limitations, as they fail to distinguish between trajectories that appear human-like but are potentially unsafe. Some recent approaches attempt to address this by regressing multiple rule-driven scores but decoupling supervision from policy optimization, resulting in suboptimal performance. To tackle these challenges, we propose DriveDPO, a Safety Direct Preference Optimization Policy Learning framework. First, we distill a unified policy distribution from human imitation similarity and rule-based safety scores for direct policy optimization. Further, we introduce an iterative Direct Preference Optimization stage formulated as trajectory-level preference alignment. Extensive experiments on the NAVSIM benchmark demonstrate that DriveDPO achieves a new state-of-the-art PDMS of 90.0. Furthermore, qualitative results across diverse challenging scenarios highlight DriveDPO's ability to produce safer and more reliable driving behaviors."
  },
  {
    "filename": "2509.17942v1.txt",
    "text": "Stewarding natural resources, mitigating floods, droughts, wildfires, and landslides, and meeting growing demands require models that can predict climate-driven land-surface responses and human feedback with high accuracy. Traditional impact models, whether process-based, statistical, or machine learning, struggle with spatial generalization due to limited observations and concept drift. Recently proposed vision foundation models trained on satellite imagery demand massive compute and are ill-suited for dynamic land-surface prediction. We introduce StefaLand, a generative spatiotemporal earth foundation model centered on landscape interactions. StefaLand improves predictions on three tasks and four datasets: streamflow, soil moisture, and soil composition, compared to prior state-of-the-art. Results highlight its ability to generalize across diverse, data-scarce regions and support broad land-surface applications. The model builds on a masked autoencoder backbone that learns deep joint representations of landscape attributes, with a location-aware architecture fusing static and time-series inputs, attribute-based representations that drastically reduce compute, and residual fine-tuning adapters that enhance transfer. While inspired by prior methods, their alignment with geoscience and integration in one model enables robust performance on dynamic land-surface tasks. StefaLand can be pretrained and finetuned on academic compute yet outperforms state-of-the-art baselines and even fine-tuned vision foundation models. To our knowledge, this is the first geoscience land-surface foundation model that demonstrably improves dynamic land-surface interaction predictions and supports diverse downstream applications."
  },
  {
    "filename": "2509.17944v1.txt",
    "text": "We study noise in iterative reconstruction from discrete noisy data of a generalized Radon transform in the plane. Our approach builds on Local Reconstruction Analysis (LRA), a framework for analyzing reconstructions at the native scale. We establish that the rescaled reconstruction error converges in distribution to a zero-mean Gaussian random field with explicitly computable covariance, providing a complete local characterization of noise in iterative reconstruction. Numerical experiments show strong agreement with the theoretical predictions. Combined with earlier deterministic results, our findings complete the analysis of iterative reconstruction at the native scale with respect to the two most fundamental limitations: the discreteness of the data and the presence of noise."
  },
  {
    "filename": "2509.17945v1.txt",
    "text": "Improving the modeling of neutrino-nuclei interactions using data-driven methods is crucial for high-precision neutrino oscillation experiments. This paper investigates Peelle's Pertinent Puzzle (PPP) in the context of neutrino measurements, a longstanding challenge to fitting theoretical models to experimental data. Inconsistencies in data-model comparisons hinder efforts to enhance the accuracy and reliability of model predictions. We analyze various sources contributing to these inconsistencies and propose strategies to address them, supported by practical case studies. We advocate for incorporating model fitting exercises as a standard practice in cross section publications to enhance the robustness of results. We use a common analysis framework to explore PPP-related challenges with MicroBooNE and T2K data in an unified manner. Our findings offer valuable insights for improving the accuracy and reliability of neutrino-nuclei interaction models, particularly by systematically tuning models using data."
  },
  {
    "filename": "2509.17948v1.txt",
    "text": "Cooperation is fundamental to the functioning of biological and social systems in both human and animal populations, with the structure of interactions playing a crucial role. Previous studies have used networks to describe interactions and explore the evolution of cooperation, but with limited transposability to social settings due to biologically relevant assumptions. Exogenous processes -- that affect the individual and are not derived from social interactions -- even if unbiased, have a role in supporting cooperation over defection, and this role has been largely overlooked in the context of network-based interactions. Here, we show that selection can favor either cooperation or defection depending on the frequency of exogenous, even if neutral, processes in any population structure. Our framework allows for deriving analytically the conditions for favoring a specific behavior in any network structure strongly affected by non-social environments (frequent exogenous forcing, FEF), which contrasts with previous computationally prohibitive methods. Our results demonstrate that the requirements for favoring cooperation under FEF do not match those in the rare-mutation limit, establishing that underlying neutral processes can be considered a mechanism for cooperation. We reveal that, under FEF, populations are less cooperative, and network heterogeneity can provide an advantage only if targeting specific network properties, clarifying seemingly contradictory experimental results and evolutionary predictions. While focused on cooperation, our assumptions generalize to any decision-making process involving a choice between alternative options. Our framework is particularly applicable to non-homogeneous human populations, offering a new perspective on cooperation science in the context of cultural evolution, where neutral and biased processes within structured interactions are abundant."
  },
  {
    "filename": "2509.17948v2.txt",
    "text": "Cooperation is fundamental to the functioning of biological and social systems in both human and animal populations, with the structure of interactions playing a crucial role. Previous studies have used networks to describe interactions and explore the evolution of cooperation, but with limited transposability to social settings due to biologically relevant assumptions. Exogenous processes -- that affect the individual and are not derived from social interactions -- even if unbiased, have a role in supporting cooperation over defection, and this role has been largely overlooked in the context of network-based interactions. Here, we show that selection can favor either cooperation or defection depending on the frequency of exogenous, even if neutral, processes in any population structure. Our framework allows for deriving analytically the conditions for favoring a specific behavior in any network structure strongly affected by non-social environments (frequent exogenous forcing, FEF), which contrasts with previous computationally prohibitive methods. Our results demonstrate that the requirements for favoring cooperation under FEF do not match those in the rare-mutation limit, establishing that underlying neutral processes can be considered a mechanism for cooperation. We reveal that, under FEF, populations are less cooperative, and network heterogeneity can provide an advantage only if targeting specific network properties, clarifying seemingly contradictory experimental results and evolutionary predictions. While focused on cooperation, our assumptions generalize to any decision-making process involving a choice between alternative options. Our framework is particularly applicable to non-homogeneous human populations, offering a new perspective on cooperation science in the context of cultural evolution, where neutral and biased processes within structured interactions are abundant."
  },
  {
    "filename": "2509.17954v1.txt",
    "text": "The extragalactic background is composed of the emission from all astrophysical sources, both resolved and unresolved, in addition to any diffuse components. In the last decade, there has been significant progress in our understanding of the cosmic history of extragalactic emissions associated with stellar evolution and accretion onto supermassive black holes, largely enabled by the extensive body of multi-wavelength data. The brightness of the extragalactic sky is now measured in photons, neutrinos, and cosmic rays, using observatories on the ground, in the sea, and in the ice, satellites in Earth orbit, and probes at the edge of the solar system. This wealth of disparate data is essential to unraveling the mysteries of the source populations that contribute to the extragalactic background.   In this contribution, we present an open database containing the most comprehensive collection of measurements of the extragalactic background spectrum to date. The combination of multi-messenger measurements over 27 frequency decades allows us to estimate the energy density of most extragalactic background components with an uncertainty of less than 30%. We explore the consistency of this cosmic inventory of the observed fields of relativistic particles populating the Universe with the cosmic history of star formation and accretion around supermassive black holes. Models incorporating these cosmic histories, as well as the redshift-dependent luminosity functions of extragalactic sources, currently match the electromagnetic component of the extragalactic background spectrum over 14 frequency decades, from the near UV to sub-TeV gamma rays. The knowledge gained from synthetic population models in the electromagnetic bands may become a crucial tool for understanding the origin of the most energetic extragalactic messengers, neutrinos and ultrahigh-energy cosmic rays."
  },
  {
    "filename": "2509.17957v1.txt",
    "text": "The human mind is capable of extraordinary achievements, yet it often appears to work against itself. It actively defends its cherished beliefs even in the face of contradictory evidence, conveniently interprets information to conform to desired narratives, and selectively searches for or avoids information to suit its various purposes. Despite these behaviours deviating from common normative standards for belief updating, we argue that such 'biases' are not inherently cognitive flaws, but rather an adaptive response to the significant pragmatic and cognitive costs associated with revising one's beliefs. This paper introduces a formal framework that aims to model the influence of these costs on our belief updating mechanisms.   We treat belief updating as a motivated variational decision, where agents weigh the perceived 'utility' of a belief against the informational cost required to adopt a new belief state, quantified by the Kullback-Leibler divergence from the prior to the variational posterior. We perform computational experiments to demonstrate that simple instantiations of this resource-rational model can be used to qualitatively emulate commonplace human behaviours, including confirmation bias and attitude polarisation. In doing so, we suggest that this framework makes steps toward a more holistic account of the motivated Bayesian mechanics of belief change and provides practical insights for predicting, compensating for, and correcting deviations from desired belief updating processes."
  },
  {
    "filename": "2509.17959v1.txt",
    "text": "Molecular dynamics methods have proven their applicability for the reproduction and prediction of molecular conformations during the last decades. However, most of works considered dilute solutions and relatively short trajectories that limit insights into conformational dynamics. In this study, we investigate the conformational dynamics of sucrose in aqueous solution using microsecond-scale molecular dynamics simulations. For the most of the calculations we use the OPLS-AA/1.14*CM1A-LBCC force field, but we also utilize OPLS-AA/1.14*CM1A and GLYCAM06 for the comparison. We focused on the glycosidic linkage conformers and their lifetimes, glucopyranose and fructofuranose ring puckering. Our findings indicate that the $^1\\mathrm{C}_4$ glucopyranose ring conformation can stabilize the sucrose conformer, appeared only in the GLYCAM06 and OPLS-AA/1.14*CM1A force fields. All the results are strengthened by comparison with the available experimental data on NMR J-coupling constants and ultrasonic spectra."
  },
  {
    "filename": "2509.17967v1.txt",
    "text": "We show that the operational definition of contextuality introduced by Spekkens is, in general, not Lorentz invariant. Specifically, we consider particle states with both spin and momentum, apply a Lorentz transformation to obtain the states in a new inertial frame, and then trace out the momentum degrees of freedom in both frames. We find that spin states that are contextual in one frame can become non-contextual in the other. Hence, the Spekkens' notion of contextuality, when restricted to spin states only, is a frame-dependent concept. We apply our results to predict a novel relativistic effect concerning the task of discriminating between two quantum states. We show that the probability of success for a moving observer exceeds that of an observer at rest."
  },
  {
    "filename": "2509.17984v1.txt",
    "text": "Rational methods are intended to time integrate linear homogeneous problems. However, their scope can be extended so as to cover linear nonhomogeneous problems. In this paper the integration of semilinear problems is considered. The resulting procedure requires the same computational cost than the one of a linked Runge--Kutta method, with the advantage that the order reduction phenomenon is avoided. Some numerical illustrations are included showing the predicted behaviour of the proposed methods."
  },
  {
    "filename": "2509.17986v1.txt",
    "text": "A self-consistent saturation model for the prediction of aeroacoustic limit cycles emerging in turbulent low-Mach cavity flows (Re=O(10^5), M\\simeq 0.2) is proposed. It predicts the nonlinear interactions between the acoustic modes of a deep rectangular cavity and the hydrodynamic instabilities of the turbulent shear-layer that forms over its opening due to the presence of a grazing flow. The model is based on the triple decomposition of the flow variables and the compressible Navier-Stokes equations. At each step of the iterative process, the nonlinear eigenvalue problem associated to perturbations around the mean flow is updated with the steady component of the forcing from the unstable eigenmode's Reynolds stress. The iterations are performed until the dominant eigenmode becomes marginally stable, i.e. its growth rate vanishes. The evolution of the coherent velocity fluctuations as function of the oscillation amplitude is in good qualitative agreement with previously published compressible Large Eddy Simulations. Furthermore, the predictions of the frequency and amplitude of the aeroacoustic limit cycle oscillations are validated against the ones obtained from a low order model, whose parameters were adjusted to reproduce the experimental measurements of the deep cavity whistling."
  },
  {
    "filename": "2509.18007v1.txt",
    "text": "Recent advancements in deep learning have significantly enhanced the performance and efficiency of traffic classification in networking systems. However, the lack of transparency in their predictions and decision-making has made network operators reluctant to deploy DL-based solutions in production networks. To tackle this challenge, we propose Traffic-Explainer, a model-agnostic and input-perturbation-based traffic explanation framework. By maximizing the mutual information between predictions on original traffic sequences and their masked counterparts, Traffic-Explainer automatically uncovers the most influential features driving model predictions. Extensive experiments demonstrate that Traffic-Explainer improves upon existing explanation methods by approximately 42%. Practically, we further apply Traffic-Explainer to identify influential features and demonstrate its enhanced transparency across three critical tasks: application classification, traffic localization, and network cartography. For the first two tasks, Traffic-Explainer identifies the most decisive bytes that drive predicted traffic applications and locations, uncovering potential vulnerabilities and privacy concerns. In network cartography, Traffic-Explainer identifies submarine cables that drive the mapping of traceroute to physical path, enabling a traceroute-informed risk analysis."
  },
  {
    "filename": "2509.18010v1.txt",
    "text": "Cross-attention is a core mechanism in encoder-decoder architectures, widespread in many fields, including speech-to-text (S2T) processing. Its scores have been repurposed for various downstream applications--such as timestamp estimation and audio-text alignment--under the assumption that they reflect the dependencies between input speech representation and the generated text. While the explanatory nature of attention mechanisms has been widely debated in the broader NLP literature, this assumption remains largely unexplored within the speech domain. To address this gap, we assess the explanatory power of cross-attention in S2T models by comparing its scores to input saliency maps derived from feature attribution. Our analysis spans monolingual and multilingual, single-task and multi-task models at multiple scales, and shows that attention scores moderately to strongly align with saliency-based explanations, particularly when aggregated across heads and layers. However, it also shows that cross-attention captures only about 50% of the input relevance and, in the best case, only partially reflects how the decoder attends to the encoder's representations--accounting for just 52-75% of the saliency. These findings uncover fundamental limitations in interpreting cross-attention as an explanatory proxy, suggesting that it offers an informative yet incomplete view of the factors driving predictions in S2T models."
  },
  {
    "filename": "2509.18011v1.txt",
    "text": "Gaussian processes (GPs) offer a flexible, uncertainty-aware framework for modeling complex signals, but scale cubically with data, assume static targets, and are brittle to outliers, limiting their applicability in large-scale problems with dynamic and noisy environments. Recent work introduced decentralized random Fourier feature Gaussian processes (DRFGP), an online and distributed algorithm that casts GPs in an information-filter form, enabling exact sequential inference and fully distributed computation without reliance on a fusion center. In this paper, we extend DRFGP along two key directions: first, by introducing a robust-filtering update that downweights the impact of atypical observations; and second, by incorporating a dynamic adaptation mechanism that adapts to time-varying functions. The resulting algorithm retains the recursive information-filter structure while enhancing stability and accuracy. We demonstrate its effectiveness on a large-scale Earth system application, underscoring its potential for in-situ modeling."
  },
  {
    "filename": "2509.18013v1.txt",
    "text": "Gradient boosting has become a cornerstone of machine learning, enabling base learners such as decision trees to achieve exceptional predictive performance. While existing algorithms primarily handle scalar or Euclidean outputs, increasingly prevalent complex-structured data, such as distributions, networks, and manifold-valued outputs, present challenges for traditional methods. Such non-Euclidean data lack algebraic structures such as addition, subtraction, or scalar multiplication required by standard gradient boosting frameworks. To address these challenges, we introduce Fr\\'echet geodesic boosting (FGBoost), a novel approach tailored for outputs residing in geodesic metric spaces. FGBoost leverages geodesics as proxies for residuals and constructs ensembles in a way that respects the intrinsic geometry of the output space. Through theoretical analysis, extensive simulations, and real-world applications, we demonstrate the strong performance and adaptability of FGBoost, showcasing its potential for modeling complex data."
  },
  {
    "filename": "2509.18015v1.txt",
    "text": "Recent work has shown promising performance of frontier large language models (LLMs) and their multimodal counterparts in medical quizzes and diagnostic tasks, highlighting their potential for broad clinical utility given their accessible, general-purpose nature. However, beyond diagnosis, a fundamental aspect of medical image interpretation is the ability to localize pathological findings. Evaluating localization not only has clinical and educational relevance but also provides insight into a model's spatial understanding of anatomy and disease. Here, we systematically assess two general-purpose MLLMs (GPT-4 and GPT-5) and a domain-specific model (MedGemma) in their ability to localize pathologies on chest radiographs, using a prompting pipeline that overlays a spatial grid and elicits coordinate-based predictions. Averaged across nine pathologies in the CheXlocalize dataset, GPT-5 exhibited a localization accuracy of 49.7%, followed by GPT-4 (39.1%) and MedGemma (17.7%), all lower than a task-specific CNN baseline (59.9%) and a radiologist benchmark (80.1%). Despite modest performance, error analysis revealed that GPT-5's predictions were largely in anatomically plausible regions, just not always precisely localized. GPT-4 performed well on pathologies with fixed anatomical locations, but struggled with spatially variable findings and exhibited anatomically implausible predictions more frequently. MedGemma demonstrated the lowest performance on all pathologies, showing limited capacity to generalize to this novel task. Our findings highlight both the promise and limitations of current MLLMs in medical imaging and underscore the importance of integrating them with task-specific tools for reliable use."
  },
  {
    "filename": "2509.18016v1.txt",
    "text": "We apply polymer quantization, a quantization technique sometimes used in high energy physics, to several superconducting circuits including: transmons, transmission line resonators, and LC circuits. In the case of transmon qubits and transmission line resonators, experimental predictions are very close to what is found with canonical quantization, though in this approach constant charge offsets can be interpreted as quantization ambiguities. In the case of LC circuits, polymer quantization predicts nonlinearities which are not present in the canonical approach. Based on this analysis we design and analyze a qubit which uses a meander inductor instead of a Josephson junction. Implications for qubit performance and fabrication are discussed. Given a choice for an effective phase operator, relevant parameters such as anharmonicity, frequency, and dispersive shifts are calculated for this meander inductor based qubit."
  },
  {
    "filename": "2509.18023v1.txt",
    "text": "In this work, we undertake the problem of formally introducing a notion of quantum many-body scarring in open quantum systems governed by the Lindblad equation. To this goal, we rely on the commutant-algebra framework for the description of strong symmetries to introduce the unconventional strong-symmetry structure leading to the existence of anomalous stationary states, which we dub open-system quantum many-body scars (OSQMBS), besides a typical infinite-temperature state. We provide several benchmarks of the theoretical predictions on the stationary-state manifold and on convergence to stationarity, as well as describe the time-evolution of off-diagonal coherences among the Hilbert space symmetry sectors identified by OSQMBS and their orthogonal complement. Moreover, we investigate the existence of asymptotic OSQMBS (AOSQMBS), the latter being states that, despite converging to the typical infinite-temperature state in the large-time limit, display anomalously-large relaxation time scales, which we thoroughly describe by means of the behavior of their fidelity as a function of time through proper scaling Ansaetze."
  },
  {
    "filename": "2509.18040v1.txt",
    "text": "The ability to centrally control network infrastructure using a programmable middleware has made Software-Defined Networking (SDN) ideal for emerging applications, such as immersive environments. However, such flexibility introduces new vulnerabilities, such as switch misreporting led load imbalance, which in turn make such immersive environment vulnerable to severe quality degradation. In this paper, we present a hybrid machine learning (ML)-based network anomaly detection framework that identifies such stealthy misreporting by capturing temporal inconsistencies in switch-reported loads, and thereby counter potentially catastrophic quality degradation of hosted immersive application. The detection system combines unsupervised anomaly scoring with supervised classification to robustly distinguish malicious behavior. Data collected from a realistic testbed deployment under both benign and adversarial conditions is used to train and evaluate the model. Experimental results show that the framework achieves high recall in detecting misreporting behavior, making it effective for early and reliable detection in SDN environments."
  },
  {
    "filename": "2509.18043v1.txt",
    "text": "Imitation learning (IL) has proven effective across a wide range of manipulation tasks. However, IL policies often struggle when faced with out-of-distribution observations; for instance, when the target object is in a previously unseen position or occluded by other objects. In these cases, extensive demonstrations are needed for current IL methods to reach robust and generalizable behaviors. But when humans are faced with these sorts of atypical initial states, we often rearrange the environment for more favorable task execution. For example, a person might rotate a coffee cup so that it is easier to grasp the handle, or push a box out of the way so they can directly grasp their target object. In this work we seek to equip robot learners with the same capability: enabling robots to prepare the environment before executing their given policy. We propose ReSET, an algorithm that takes initial states -- which are outside the policy's distribution -- and autonomously modifies object poses so that the restructured scene is similar to training data. Theoretically, we show that this two step process (rearranging the environment before rolling out the given policy) reduces the generalization gap. Practically, our ReSET algorithm combines action-agnostic human videos with task-agnostic teleoperation data to i) decide when to modify the scene, ii) predict what simplifying actions a human would take, and iii) map those predictions into robot action primitives. Comparisons with diffusion policies, VLAs, and other baselines show that using ReSET to prepare the environment enables more robust task execution with equal amounts of total training data. See videos at our project website: https://reset2025paper.github.io/"
  },
  {
    "filename": "2509.18047v1.txt",
    "text": "In this paper, we present a general specification for Functional Effects Models, which use Machine Learning (ML) methodologies to learn individual-specific preference parameters from socio-demographic characteristics, therefore accounting for inter-individual heterogeneity in panel choice data. We identify three specific advantages of the Functional Effects Model over traditional fixed, and random/mixed effects models: (i) by mapping individual-specific effects as a function of socio-demographic variables, we can account for these effects when forecasting choices of previously unobserved individuals (ii) the (approximate) maximum-likelihood estimation of functional effects avoids the incidental parameters problem of the fixed effects model, even when the number of observed choices per individual is small; and (iii) we do not rely on the strong distributional assumptions of the random effects model, which may not match reality. We learn functional intercept and functional slopes with powerful non-linear machine learning regressors for tabular data, namely gradient boosting decision trees and deep neural networks. We validate our proposed methodology on a synthetic experiment and three real-world panel case studies, demonstrating that the Functional Effects Model: (i) can identify the true values of individual-specific effects when the data generation process is known; (ii) outperforms both state-of-the-art ML choice modelling techniques that omit individual heterogeneity in terms of predictive performance, as well as traditional static panel choice models in terms of learning inter-individual heterogeneity. The results indicate that the FI-RUMBoost model, which combines the individual-specific constants of the Functional Effects Model with the complex, non-linear utilities of RUMBoost, performs marginally best on large-scale revealed preference panel data."
  },
  {
    "filename": "2509.18053v1.txt",
    "text": "Current state-of-the-art autonomous vehicles could face safety-critical situations when their local sensors are occluded by large nearby objects on the road. Vehicle-to-vehicle (V2V) cooperative autonomous driving has been proposed as a means of addressing this problem, and one recently introduced framework for cooperative autonomous driving has further adopted an approach that incorporates a Multimodal Large Language Model (MLLM) to integrate cooperative perception and planning processes. However, despite the potential benefit of applying graph-of-thoughts reasoning to the MLLM, this idea has not been considered by previous cooperative autonomous driving research. In this paper, we propose a novel graph-of-thoughts framework specifically designed for MLLM-based cooperative autonomous driving. Our graph-of-thoughts includes our proposed novel ideas of occlusion-aware perception and planning-aware prediction. We curate the V2V-GoT-QA dataset and develop the V2V-GoT model for training and testing the cooperative driving graph-of-thoughts. Our experimental results show that our method outperforms other baselines in cooperative perception, prediction, and planning tasks."
  },
  {
    "filename": "2509.18053v2.txt",
    "text": "Current state-of-the-art autonomous vehicles could face safety-critical situations when their local sensors are occluded by large nearby objects on the road. Vehicle-to-vehicle (V2V) cooperative autonomous driving has been proposed as a means of addressing this problem, and one recently introduced framework for cooperative autonomous driving has further adopted an approach that incorporates a Multimodal Large Language Model (MLLM) to integrate cooperative perception and planning processes. However, despite the potential benefit of applying graph-of-thoughts reasoning to the MLLM, this idea has not been considered by previous cooperative autonomous driving research. In this paper, we propose a novel graph-of-thoughts framework specifically designed for MLLM-based cooperative autonomous driving. Our graph-of-thoughts includes our proposed novel ideas of occlusion-aware perception and planning-aware prediction. We curate the V2V-GoT-QA dataset and develop the V2V-GoT model for training and testing the cooperative driving graph-of-thoughts. Our experimental results show that our method outperforms other baselines in cooperative perception, prediction, and planning tasks. Our project website: https://eddyhkchiu.github.io/v2vgot.github.io/ ."
  },
  {
    "filename": "2509.18058v1.txt",
    "text": "Large language model (LLM) developers aim for their models to be honest, helpful, and harmless. However, when faced with malicious requests, models are trained to refuse, sacrificing helpfulness. We show that frontier LLMs can develop a preference for dishonesty as a new strategy, even when other options are available. Affected models respond to harmful requests with outputs that sound harmful but are subtly incorrect or otherwise harmless in practice. This behavior emerges with hard-to-predict variations even within models from the same model family. We find no apparent cause for the propensity to deceive, but we show that more capable models are better at executing this strategy. Strategic dishonesty already has a practical impact on safety evaluations, as we show that dishonest responses fool all output-based monitors used to detect jailbreaks that we test, rendering benchmark scores unreliable. Further, strategic dishonesty can act like a honeypot against malicious users, which noticeably obfuscates prior jailbreak attacks. While output monitors fail, we show that linear probes on internal activations can be used to reliably detect strategic dishonesty. We validate probes on datasets with verifiable outcomes and by using their features as steering vectors. Overall, we consider strategic dishonesty as a concrete example of a broader concern that alignment of LLMs is hard to control, especially when helpfulness and harmlessness conflict."
  },
  {
    "filename": "2509.18058v2.txt",
    "text": "Large language model (LLM) developers aim for their models to be honest, helpful, and harmless. However, when faced with malicious requests, models are trained to refuse, sacrificing helpfulness. We show that frontier LLMs can develop a preference for dishonesty as a new strategy, even when other options are available. Affected models respond to harmful requests with outputs that sound harmful but are crafted to be subtly incorrect or otherwise harmless in practice. This behavior emerges with hard-to-predict variations even within models from the same model family. We find no apparent cause for the propensity to deceive, but show that more capable models are better at executing this strategy. Strategic dishonesty already has a practical impact on safety evaluations, as we show that dishonest responses fool all output-based monitors used to detect jailbreaks that we test, rendering benchmark scores unreliable. Further, strategic dishonesty can act like a honeypot against malicious users, which noticeably obfuscates prior jailbreak attacks. While output monitors fail, we show that linear probes on internal activations can be used to reliably detect strategic dishonesty. We validate probes on datasets with verifiable outcomes and by using them as steering vectors. Overall, we consider strategic dishonesty as a concrete example of a broader concern that alignment of LLMs is hard to control, especially when helpfulness and harmlessness conflict."
  },
  {
    "filename": "2509.18065v1.txt",
    "text": "Biofilm infections on medical implants are difficult to eradicate because insufficient nutrient availability promotes antibiotic-tolerant persister cells that survive treatment and reseed growth. Existing mathematical models usually omit nutrient-dependent phenotypic switching between proliferative and persister states. Without this mechanism, models cannot capture how environmental conditions control the balance between active growth and dormancy, which is central to biofilm persistence. We present a continuum model that couples nutrient transport with the dynamics of proliferative bacteria, persisters, dead cells, and extracellular polymeric substances. The switching rates between proliferative and persister phenotypes depend on local nutrient concentration through two thresholds, enabling adaptation across nutrient-poor, intermediate, and nutrient-rich regimes. Simulations show that nutrient limitation produces a high and sustained proportion of persister cells even when biomass is reduced, whereas nutrient-rich conditions support reversion to proliferative growth and lead to greater biomass. The model also predicts that persister populations peak at times that vary with nutrient availability, and these peaks coincide with turning points in biofilm growth, identifying critical intervention windows. By directly linking nutrient availability to phenotypic switching, our model reveals mechanisms of biofilm persistence that earlier models could not capture, and it points toward strategies that target nutrient-driven adaptation as a means to improve the control of implant-associated infections."
  },
  {
    "filename": "2509.18090v1.txt",
    "text": "Reconstructing accurate surfaces with radiance fields has achieved remarkable progress in recent years. However, prevailing approaches, primarily based on Gaussian Splatting, are increasingly constrained by representational bottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based framework that explores and extends the under-investigated potential of sparse voxels for achieving accurate, detailed, and complete surface reconstruction. As strengths, sparse voxels support preserving the coverage completeness and geometric clarity, while corresponding challenges also arise from absent scene constraints and locality in surface refinement. To ensure correct scene convergence, we first propose a Voxel-Uncertainty Depth Constraint that maximizes the effect of monocular depth cues while presenting a voxel-oriented uncertainty to avoid quality degradation, enabling effective and robust scene constraints yet preserving highly accurate geometries. Subsequently, Sparse Voxel Surface Regularization is designed to enhance geometric consistency for tiny voxels and facilitate the voxel-based formation of sharp and accurate surfaces. Extensive experiments demonstrate our superior performance compared to existing methods across diverse challenging scenarios, excelling in geometric accuracy, detail preservation, and reconstruction completeness while maintaining high efficiency. Code is available at https://github.com/Fictionarry/GeoSVR."
  },
  {
    "filename": "2509.18240v1.txt",
    "text": "Galaxy clusters, the most massive, dark-matter-dominated, and most recently assembled structures in the Universe, are key tools for probing cosmology. However, uncertainties in scaling relations that connect cluster mass to observables like X-ray luminosity and temperature remain a significant challenge. In this paper, we present the results of an extensive investigation of 329 simulated clusters from Illustris TNG300 cosmological simulations. Our analysis involves cross-correlating dark matter and the hot X-ray-emitting gas, considering both the 3D and 2D projected distributions to account for projection effects. We demonstrate that this approach is highly effective in evaluating the dynamical state of these systems and validating the often-utilized assumption of hydrostatic equilibrium, which is key for inferring cluster masses and constructing scaling relations. Our study revisits both the X-ray luminosity-mass and X-ray temperature-mass scaling relations, and demonstrates how the scatter in these relations correlates with the clusters' dynamical state. We demonstrate that matter-gas coherence enables the identification of an optimal set of relaxed clusters, reducing scatter in scaling relations by up to 40%. This innovative approach, which integrates higher-dimensional insights into scaling relations, might offer a new path to further reduce uncertainties in determining cosmological parameters from galaxy clusters."
  },
  {
    "filename": "2509.18241v1.txt",
    "text": "OJ 287 is the best-known supermassive black hole binary candidate in the nanohertz gravitational wave band. It exhibits periodic flares every $\\sim$ 12 years, likely caused by collisions of a smaller-mass secondary with the accretion disk surrounding a larger-mass primary. It is therefore an important benchmark for understanding black hole binary accretion in the approaching era of space-based gravitational wave detectors and large electromagnetic surveys. Because the electromagnetic emission of the system is determined by a complex interplay of plasma, accretion, and radiation physics in strong gravity, numerical simulations are required for realistic modeling. We present the first global, three-dimensional, general relativistic magnetohydrodynamic (GRMHD) simulations of OJ 287-like systems; namely, smaller-mass secondaries colliding with a radiatively-cooled (thin) disk surrounding a larger-mass primary. We focus on disks with scale heights that are 10\\% of the distance from the primary and binary mass ratios of $q = 0.1,0.05$, and $0.025$ using an optically-thin cooling prescription. We confirm the basic paradigm that impacts of the secondary on the disk can generate enough power to outshine the quiescent emission. The secondary also causes spiral shocks to form in the disk, enhanced accretion events, overall heating of the flow, and stochastic tilting of the disk, though these effects are small for $q<0.05$. Our results can be extrapolated to the parameters of OJ 287 and similar systems, an important step on the path toward fully realistic simulations of accretion onto small-mass-ratio black hole binaries and predicting electromagnetic counterparts to low-frequency gravitational wave detections."
  },
  {
    "filename": "2509.18244v1.txt",
    "text": "Internal Gravity Waves (IGWs) are thought to cause mixing in stellar interiors, a process that has been widely studied both theoretically and numerically. Our aim is to determine the physical mechanism responsible for the wave-induced mixing in stellar interiors. We compare the mixing profiles obtained from two-dimensional (2D) equatorial hydrodynamical and tracer particle simulations with theoretical predictions from R. J. Garcia Lopez & H. C. Spruit (1991) and J. P. Zahn (1992) on wave mixing due to wave-induced shear turbulence. Our results show that, despite not satisfying the vertical shear instability threshold, the mixing profiles from the simulations agree remarkably well with the theoretical predictions of both prescriptions, strongly suggesting that shear from IGWs plays an important role in mixing even at low shear rates. This agreement remains robust across different stellar masses, ages, rotation and simulation parameters. This provides an important step in providing realistic parameterisations for wave mixing in stellar structure and evolution models."
  },
  {
    "filename": "2509.18256v1.txt",
    "text": "The spectral functions of twisted bilayer graphene (TBG) in the absence of strain have recently been investigated in both the symmetric and symmetry-broken phases using dynamical mean-field theory (DMFT). The theoretically predicted Mott-Hubbard bands and gapless semimetallic state at half-filling have since been confirmed experimentally. Here, we develop several second-order perturbation theory approaches to the topological heavy-fermion (THF) model of TBG and twisted symmetric trilayer graphene (TSTG). In the symmetric phase, we adapt, implement, and benchmark an iterative perturbation theory (IPT) impurity solver within DMFT, enabling computationally efficient yet accurate spectral function calculations. We present momentum- and energy-resolved spectra over a broad range of temperatures and fillings for both symmetric and symmetry-broken states. In addition, we derive analytic expressions for the spectral function within the ``Hubbard-I'' approximation of the THF model and, as expected, find that while it provides a tractable description of Mott physics, it does not capture the low-energy Kondo peak or the finite lifetime broadening of the bands. Our methodology can be extended to include strain, lattice relaxation, and parameter variations, thereby allowing systematic predictions of TBG and TSTG spectral properties across a wide range of physical regimes. Because our perturbative approaches are far less computationally intensive than DMFT with numerically exact impurity solvers, they can be used to efficiently benchmark and scan extensive phase diagrams of the THF parameters, paving the way for full DMFT analyses of the TBG spectral function in the presence of strain and relaxation."
  },
  {
    "filename": "2509.18263v1.txt",
    "text": "Accurately predicting protein structures from amino acid sequences remains a fundamental challenge in computational biology, with profound implications for understanding biological functions and enabling structure-based drug discovery. Quantum computing approaches based on coarse-grained lattice models combined with variational algorithms have been proposed as an initial step towards predicting protein structures using quantum computers. In this work, we introduce a more efficient quantum protein structure prediction workflow that bypasses the need for explicit Hamiltonian construction by employing a problem-agnostic ansatz. The ansatz is trained to minimize an energy-based cost function that can be efficiently computed on classical computers, eliminating the need for ancillary qubits and reducing circuit depth compared to previous Hamiltonian-based methods. This enables a more scalable approach for larger proteins and facilitates the inclusion of higher-order interactions, previously hard to achieve in quantum approaches. We validate our method by benchmarking a hardware-efficient ansatz on a large set of proteins with up to 26 amino acids, modeled on the tetrahedral, body-centered cubic, and face-centered cubic lattices, incorporating up to second-nearest-neighbor interactions. We assess the performance on both a noise-free simulator and the ibm_kingston quantum computer using a set of distinct metrics to probe different aspects of the prediction quality. These experiments push the boundaries of quantum methods for protein structure prediction, targeting sequences that are longer than those typically addressed in prior studies. Overall, the results highlight the scalability and versatility of our approach, while also identifying key areas for improvement to inform future algorithm development and hardware advancements."
  },
  {
    "filename": "2509.18264v1.txt",
    "text": "We implement an outlier detection model, an Isolation Foest (iForest), to uncover anomalous objects in the Galaxy and Mass Assembly Fourth Data Release (GAMA DR4). The iForest algorithm is an unsupervise Machine Learning (ML) technique. The data used is the spectroscopic and photometric data from GAMA DR4, which compiless information for over 300000 objects. We select two samples of galaxies to isolate, high signal-to-noise galaxies, to analyse the iForest's robustness, and E+A galaxies, to study the extremes of their population. This results in six-subsamples of spectroscopic, photometric and combined data isolations, finding 101 anomalous objects, half of which have not been identified as outliers in other works. We also find a number of fringing errors and false emission lines, displaying the iForest's potential in detecting these errors. Finding anomalous E+A galaxies, that although selected in a normal manner, using low [OII] and strong H{\\delta} absorption, are still star-forming, with strong H{\\alpha} emission. We propose two solutions to why these E+A galaxies are still star-forming but also question if these galaxies can be truly classified as E+A galaxies. We suggest that small-scale interactions on the galaxies causes small star bursts. The radiative pressure when forming high mass stars form expels the accreting material quicker than it can be accreted. We also suggest that the Jeans limit in our anomalous E+A galaxies is so low that it is simply not possible to form O and B class stars, but not low enough to fully prevent star-formation."
  },
  {
    "filename": "2509.18271v1.txt",
    "text": "We present a new cosmological analysis of the small-scale Lyman alpha forest 1D flux power spectrum (P1D) using high-resolution quasar spectra from XQ100 and KODIAQ-SQUAD, interpreted through the PRIYA emulator. PRIYA is a suite of galaxy formation simulations spanning a range of cosmological and inhomogeneous HeII reionization parameters, enabling few-percent-level predictions of the P1D. These datasets, probing down to $k \\sim 6\\,h\\,\\mathrm{Mpc}^{-1}$ at $z = 2-5$, offer access to non-linear scales inaccessible to large-volume surveys like eBOSS. We find that the XQ100 P1D yields constraints on the primordial power spectrum parameters $(A_P, n_P)$ at pivot scale $k_0 = 0.78\\,\\mathrm{Mpc}^{-1}$ that are consistent with PRIYA results from eBOSS DR14 and Planck CMB, albeit with broader uncertainties. Notably, this is achieved without external IGM temperature data, showing that XQ100 alone provides stronger constraints on thermal history than eBOSS DR14. In contrast, the KODIAQ-SQUAD P1D favors a significantly higher $A_P$ value, driven by the selection bias toward high-column density absorbers (HCDs). We also find that the P1D at $k > 0.045\\,\\mathrm{s/km}$ is more sensitive to Lyman limit system contamination and thermal history. When imposing a prior on $(A_P, n_P)$, the reduced $\\chi^2$ remains unchanged and the inferred mean IGM temperature is unaffected, suggesting that cosmological and thermal parameters are largely sensitive to different scales. The XQ100 P1D therefore provides complementary information on thermal nuisance parameters, which can be jointly fit with eBOSS or DESI P1D measurements to improve cosmological constraints."
  },
  {
    "filename": "2509.18278v1.txt",
    "text": "We study 24 massive quiescent galaxies with $\\log \\textrm{M}_*/\\textrm{M}_\\odot > 10$ at $1 < z < 3$ with JWST/NIRSpec medium-resolution observations from the Early eXtragalactic Continuum and Emission Line Survey (EXCELS). We reconstruct their star formation histories and find that they have large bursts ($100\\textrm{ M}_{\\odot} \\textrm{yr}^{-1} -1000 \\textrm{ M}_{\\odot} \\textrm{yr}^{-1}$), followed by a rapid truncation of star formation. The number densities of the quenched galaxies in our sample that we predict underwent a submillimeter phase are consistent with submillimeter galaxies being the progenitors of our quenched population. The median post-starburst visibility time is $\\sim600$ Myr, with more massive galaxies ($\\log \\textrm{M}_*/\\textrm{M}_\\odot > 10.7$) exhibiting shorter visibility times than lower mass galaxies. The range of quenching times -- defined as the time from the peak starburst to the time of quiescence -- found in this sample ($0.06-1.75$ Gyr) suggests multiple quenching pathways, consistent with previous studies. We do not see evidence for quenching mechanisms varying with redshift between $1<z<3$. We detect evidence for weak AGN activity in 4 out of the 8 galaxies with robust emission line detections, based on line ratio diagnostics. Our findings suggest that there are a diverse range of quenching mechanisms at cosmic noon, and support a scenario in which the primary quenching mechanisms are rapid ($<500$ Myr) following a starburst."
  },
  {
    "filename": "2509.18282v1.txt",
    "text": "Robotic manipulation policies often fail to generalize because they must simultaneously learn where to attend, what actions to take, and how to execute them. We argue that high-level reasoning about where and what can be offloaded to vision-language models (VLMs), leaving policies to specialize in how to act. We present PEEK (Policy-agnostic Extraction of Essential Keypoints), which fine-tunes VLMs to predict a unified point-based intermediate representation: 1. end-effector paths specifying what actions to take, and 2. task-relevant masks indicating where to focus. These annotations are directly overlaid onto robot observations, making the representation policy-agnostic and transferable across architectures. To enable scalable training, we introduce an automatic annotation pipeline, generating labeled data across 20+ robot datasets spanning 9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot generalization, including a 41.4x real-world improvement for a 3D policy trained only in simulation, and 2-3.5x gains for both large VLAs and small manipulation policies. By letting VLMs absorb semantic and visual complexity, PEEK equips manipulation policies with the minimal cues they need--where, what, and how. Website at https://peek-robot.github.io/."
  },
  {
    "filename": "2509.18284v1.txt",
    "text": "As medical diagnoses increasingly leverage multimodal data, machine learning models are expected to effectively fuse heterogeneous information while remaining robust to missing modalities. In this work, we propose a novel multimodal learning framework that integrates enhanced modalities dropout and contrastive learning to address real-world limitations such as modality imbalance and missingness. Our approach introduces learnable modality tokens for improving missingness-aware fusion of modalities and augments conventional unimodal contrastive objectives with fused multimodal representations. We validate our framework on large-scale clinical datasets for disease detection and prediction tasks, encompassing both visual and tabular modalities. Experimental results demonstrate that our method achieves state-of-the-art performance, particularly in challenging and practical scenarios where only a single modality is available. Furthermore, we show its adaptability through successful integration with a recent CT foundation model. Our findings highlight the effectiveness, efficiency, and generalizability of our approach for multimodal learning, offering a scalable, low-cost solution with significant potential for real-world clinical applications. The code is available at https://github.com/omron-sinicx/medical-modality-dropout."
  },
  {
    "filename": "2509.18301v1.txt",
    "text": "Integrative biomolecular modeling of RNA relies on structural refined collections and accurate experimental data that reflect binding and folding behavior. However, the prediction of such collections remains challenging due to the rugged energy landscape and extensive conformational heterogeneity of large RNAs. To overcome these limitations, we applied a FRET-guided strategy to identify RNA conformational states consistent with single-molecule FRET (smFRET) experiments. We predicted 3D structures of a ribosomal RNA tertiary contact comprising a GAAA tetraloop and a kissing loop using three popular RNA 3D modeling tools namely RNAComposer, FARFAR2, and AlphaFold3, yielding a collection of candidate conformations. These models were structurally validated based on Watson-Crick base-pairing patterns and filtered using an eRMSD threshold. For each retained structure, we computed the accessible contact volume (ACV) of the sCy3/sCy5 dye pair using FRETraj to predict FRET distributions. These distributions were then compared and weighted against experimental smFRET data to identify conformational states compatible with the observed FRET states. Our results demonstrate that experimental transfer efficiencies can be reproduced using \\textit{in silico} predicted RNA 3D structures. This FRET-guided workflow, combined with structural validation, lays the foundation for capturing the highly diverse conformational states characteristic of flexible RNA motifs."
  },
  {
    "filename": "2509.18302v1.txt",
    "text": "With JWST, it is now possible to use Lyman-Alpha (Ly$\\alpha$) emission from galaxies beyond z>8 to trace neutral hydrogen in the intergalactic medium (IGM) as the Universe became reionized. However, observed Ly$\\alpha$ emission is scattered by neutral hydrogen in the IGM and the interstellar and circum-galactic medium, necessitating `baseline' models of Ly$\\alpha$ properties in the ionized IGM to disentangle their impacts. In this work, we characterize Ly$\\alpha$ properties at the end of reionization, z~5-6, providing a baseline that can be applied to z>6 observations. We targeted GOODS-N with MMT/Binospec, obtaining R~4360 rest-frame UV spectra of 236 galaxies at z~5-6, selected from HST/CANDELS, finding 62 Ly$\\alpha$ detections. We use JWST observations from JADES and FRESCO for a subset of our sources to characterize Ly$\\alpha$ properties as a function of UV continuum and H$\\alpha$ emission. We present the first statistical measurements of the Ly$\\alpha$ FWHM distribution at z~5-6, and produce empirical baseline models of Ly$\\alpha$ equivalent width (EWLy$\\alpha$) and escape fraction (f$_{esc}^{Ly\\alpha}$) conditional on UV magnitude and slope. We find our EWLy$\\alpha$ and f$_{esc}^{Ly\\alpha}$ models depend on UV magnitude, and infer 45$\\pm$5$\\%$ and <62$\\pm$8$\\%$ of MUV=-19.5 galaxies have EWLy$\\alpha$>25$\\r{A}$ and f$_{esc}^{Ly\\alpha}$>0.2, respectively. We find a mean Ly$\\alpha$ FWHM of 245km/s and median Ly$\\alpha$ velocity offset of 258km/s, both correlating with higher UV luminosity. Our median observed Ly$\\alpha$ line profile is broader and has higher velocity offset compared to pre-JWST models based on z~2 lines, which may reflect resonant scattering by residual neutral hydrogen in the IGM at z~5-6 and increasing ISM/CGM densities. Our median line profile predicts higher Ly$\\alpha$ transmission in a fully neutral IGM, providing insight into recent z>10 Ly$\\alpha$ detections."
  },
  {
    "filename": "2509.18315v1.txt",
    "text": "The presence of molecular isomers in interstellar environments has become a topic of growing interest within the astrochemical community. Contrary to predictions based on thermodynamic equilibrium, recent observations reveal a diverse array of high-energy isomers and conformers. One of the most iconic molecular isomers detected in space, formic acid (HCOOH, FA), has been the focus of extensive theoretical research aimed at understanding its speciation into cis and trans conformers in dark clouds and photodissociation regions. In this work, we report the detection of c-FA, the higher-energy conformer, using ultrasensitive observations of TMC-1. This detection adds to previous findings in the Barnard-5 and L483 dark clouds. The derived trans-to-cis isomer ratio in TMC-1, 17.5, closely matches those observed in other sources, suggesting that the same chemical processes are at play across these environments. To investigate this, we conducted detailed astrochemical gas-grain models tailored to formic acid isomerism to explain the observed ratios. Our models successfully reproduce the observed trans/cis ratios and indicate that the presence of cis-formic acid can be attributed to the release of c-FA from grains, followed by isomerization driven by the excess energy released during the desorption process, a process that we name as isomerization upon desorption. The models also show that the isomerization of t-FA to c-FA in the gas phase is negligible at 10 K, meaning the observed ratios are a direct consequence of the formation pathways of both isomers on the surface of dust grains. However, at higher temperatures, quantum tunneling mediated direct isomerization in the gas becomes significant, and the ratios converge toward the thermodynamic equilibrium value."
  },
  {
    "filename": "2509.18326v1.txt",
    "text": "Reliable out-of-distribution (OOD) detection is important for safe deployment of deep learning models in fetal ultrasound amidst heterogeneous image characteristics and clinical settings. OOD detection relies on estimating a classification model's uncertainty, which should increase for OOD samples. While existing research has largely focused on uncertainty quantification methods, this work investigates the impact of the classification task itself. Through experiments with eight uncertainty quantification methods across four classification tasks, we demonstrate that OOD detection performance significantly varies with the task, and that the best task depends on the defined ID-OOD criteria; specifically, whether the OOD sample is due to: i) an image characteristic shift or ii) an anatomical feature shift. Furthermore, we reveal that superior OOD detection does not guarantee optimal abstained prediction, underscoring the necessity to align task selection and uncertainty strategies with the specific downstream application in medical image analysis."
  },
  {
    "filename": "2509.18328v1.txt",
    "text": "Resolved high-redshift galaxy gas kinematics is a rapidly evolving field driven by increasingly powerful instrumentation. However, the resolution and sensitivity still impose constraints on interpretation. We investigate the uncertainties inherent to high-$z$ galaxy kinematical analysis by modelling a suite of rotating disk galaxies, generating synthetic interferometric ALMA observations, and fitting them with the 3D-kinematical tools 3DBarolo, GalPaK3D, and Qubefit. We present the recovered 3D-fitted kinematical parameters to assess their reliability, quantify the range of values possible for individual source studies, and establish the systematic biases present for observed samples. The $V/\\sigma_{\\rm V}$ ratio, which indicates how dynamically cold a system is, is of particular importance and depends on the choice of 3D-fitting tool. On average, 3DBarolo and Qubefit slightly overestimates $V/\\sigma_{\\rm V}$ ($<1\\sigma$) and GalPaK3D underestimates it ($<2\\sigma$). Therefore, all three tools are reliable for kinematical studies of averages of high-redshift galaxy samples. The value range possible for individual sources is significant, however, even more so for samples of not purely rotation dominated sources. To determine whether an observed galaxy is rotation dominated enough to be fitted with a 3D-kinematical tool, $V/\\sigma_{\\rm V}$ can be extracted directly from the observed data cube, with some caveats. We recommend that the median offsets, value ranges, and tool-dependent biases presented in this paper are taken into account when interpreting 3D-fitted kinematics of observed high-redshift galaxies."
  },
  {
    "filename": "2509.18347v1.txt",
    "text": "We explore the physical origins of the observed deficit of polycyclic aromatic hydrocarbons (PAHs) at sub-solar metallicity using JWST/NIRCam imaging of the nearby galaxy M101, covering regions from solar metallicity (Z$_{\\odot}$) down to 0.4 Z$_{\\odot}$. These maps are used to trace the radial evolution of the shortest-wavelength PAH feature at 3.3 $\\mu$m, which is emitted preferentially by the smallest PAHs ($<100$ carbon atoms). The fractional contribution of PAH 3.3 $\\mu$m to the total PAH luminosity ($\\Sigma$PAH) increases by 3x as metallicity declines, rising from $\\sim$1$\\%$ to $\\sim$3$\\%$ over the observed range, consistent with prior predictions from the inhibited grain growth model based on Spitzer spectroscopy. We explore model refinements including photon effects and alternative size evolution prescriptions, and find that a modest amount of small grain photo-destruction remains possible, provided the grain size cutoff does not exceed $\\sim55$ carbon atoms. The best-fit models predict 3.3 $\\mu$m/$\\Sigma$PAH will rise to $\\sim5.6-7.7\\%$ at 10$\\%$ Z$_{\\odot}$. Surprisingly, even as $\\Sigma$PAH drops significantly relative to the total infrared luminosity (TIR) as metallicity declines, 3.3 $\\mu$m/TIR alone rises, potentially indicating the mass fraction of the smallest PAH grains increases as the total dust content in galaxies drops. The current model cannot fully reproduce this trend even if the unusually strong effects of changing radiation field hardness on 3.3 $\\mu$m/TIR are included. This may be evidence that the smallest PAHs are uniquely robust against destruction and inhibited growth effects. These results highlight the pivotal role that short-wavelength PAH emission can play in studies of low-metallicity and high-redshift galaxies."
  },
  {
    "filename": "2509.18349v1.txt",
    "text": "Meta-learning has emerged as a powerful paradigm for leveraging information across related tasks to improve predictive performance on new tasks. In this paper, we propose a statistical framework for analyzing meta-learning through the lens of predictor subspace characterization and quantification of task diversity. Specifically, we model the shared structure across tasks using a latent subspace and introduce a measure of diversity that captures heterogeneity across task-specific predictors. We provide both simulation-based and theoretical evidence indicating that achieving the desired prediction accuracy in meta-learning depends on the proportion of predictor variance aligned with the shared subspace, as well as on the accuracy of subspace estimation."
  },
  {
    "filename": "2509.18353v1.txt",
    "text": "The size, diversity, and quality of pretraining datasets critically determine the generalization ability of foundation models. Despite their growing importance in chemoinformatics, the effectiveness of molecular representation learning has been hindered by limitations in existing small molecule datasets. To address this gap, we present MolPILE, large-scale, diverse, and rigorously curated collection of 222 million compounds, constructed from 6 large-scale databases using an automated curation pipeline. We present a comprehensive analysis of current pretraining datasets, highlighting considerable shortcomings for training ML models, and demonstrate how retraining existing models on MolPILE yields improvements in generalization performance. This work provides a standardized resource for model training, addressing the pressing need for an ImageNet-like dataset in molecular chemistry."
  },
  {
    "filename": "2509.18372v1.txt",
    "text": "We present TinyBEV, a unified, camera only Bird's Eye View (BEV) framework that distills the full-stack capabilities of a large planning-oriented teacher (UniAD [19]) into a compact, real-time student model. Unlike prior efficient camera only baselines such as VAD[23] and VADv2[7], TinyBEV supports the complete autonomy stack 3D detection, HD-map segmentation, motion forecasting, occupancy prediction, and goal-directed planning within a streamlined 28M-parameter backbone, achieving a 78% reduction in parameters over UniAD [19]. Our model-agnostic, multi-stage distillation strategy combines feature-level, output-level, and adaptive region-aware supervision to effectively transfer high-capacity multi-modal knowledge to a lightweight BEV representation. On nuScenes[4], Tiny-BEV achieves 39.0 mAP for detection, 1.08 minADE for motion forecasting, and a 0.32 collision rate, while running 5x faster (11 FPS) and requiring only camera input. These results demonstrate that full-stack driving intelligence can be retained in resource-constrained settings, bridging the gap between large-scale, multi-modal perception-planning models and deployment-ready real-time autonomy."
  },
  {
    "filename": "2509.18376v1.txt",
    "text": "Graph Neural Networks (GNNs) are widely used for node classification, yet their opaque decision-making limits trust and adoption. While local explanations offer insights into individual predictions, global explanation methods, those that characterize an entire class, remain underdeveloped. Existing global explainers rely on motif discovery in small graphs, an approach that breaks down in large, real-world settings where subgraph repetition is rare, node attributes are high-dimensional, and predictions arise from complex structure-attribute interactions. We propose GnnXemplar, a novel global explainer inspired from Exemplar Theory from cognitive science. GnnXemplar identifies representative nodes in the GNN embedding space, exemplars, and explains predictions using natural language rules derived from their neighborhoods. Exemplar selection is framed as a coverage maximization problem over reverse k-nearest neighbors, for which we provide an efficient greedy approximation. To derive interpretable rules, we employ a self-refining prompt strategy using large language models (LLMs). Experiments across diverse benchmarks show that GnnXemplar significantly outperforms existing methods in fidelity, scalability, and human interpretability, as validated by a user study with 60 participants."
  },
  {
    "filename": "2509.18386v1.txt",
    "text": "Trajectory anomaly detection is essential for identifying unusual and unexpected movement patterns in applications ranging from intelligent transportation systems to urban safety and fraud prevention.   Existing methods only consider limited aspects of the trajectory nature and its movement space by treating trajectories as sequences of sampled locations, with sampling determined by positioning technology, e.g., GPS, or by high-level abstractions such as staypoints. Trajectories are analyzed in Euclidean space, neglecting the constraints and connectivity information of the underlying movement network, e.g., road or transit networks.   The proposed Graph Enhanced Trajectory Anomaly Detection (GETAD) framework tightly integrates road network topology, segment semantics, and historical travel patterns to model trajectory data. GETAD uses a Graph Attention Network to learn road-aware embeddings that capture both physical attributes and transition behavior, and augments these with graph-based positional encodings that reflect the spatial layout of the road network.   A Transformer-based decoder models sequential movement, while a multiobjective loss function combining autoregressive prediction and supervised link prediction ensures realistic and structurally coherent representations.   To improve the robustness of anomaly detection, we introduce Confidence Weighted Negative Log Likelihood (CW NLL), an anomaly scoring function that emphasizes high-confidence deviations.   Experiments on real-world and synthetic datasets demonstrate that GETAD achieves consistent improvements over existing methods, particularly in detecting subtle anomalies in road-constrained environments. These results highlight the benefits of incorporating graph structure and contextual semantics into trajectory modeling, enabling more precise and context-aware anomaly detection."
  },
  {
    "filename": "2509.18387v1.txt",
    "text": "Motion blur reduces the clarity of fast-moving objects, posing challenges for detection systems, especially in racket sports, where balls often appear as streaks rather than distinct points. Existing labeling conventions mark the ball at the leading edge of the blur, introducing asymmetry and ignoring valuable motion cues correlated with velocity. This paper introduces a new labeling strategy that places the ball at the center of the blur streak and explicitly annotates blur attributes. Using this convention, we release a new table tennis ball detection dataset. We demonstrate that this labeling approach consistently enhances detection performance across various models. Furthermore, we introduce BlurBall, a model that jointly estimates ball position and motion blur attributes. By incorporating attention mechanisms such as Squeeze-and-Excitation over multi-frame inputs, we achieve state-of-the-art results in ball detection. Leveraging blur not only improves detection accuracy but also enables more reliable trajectory prediction, benefiting real-time sports analytics."
  },
  {
    "filename": "2509.18390v1.txt",
    "text": "Advances in high dynamic range (HDR) lighting estimation from a single image have opened new possibilities for augmented reality (AR) applications. Predicting complex lighting environments from a single input image allows for the realistic rendering and compositing of virtual objects. In this work, we investigate the color robustness of such methods -- an often overlooked yet critical factor for achieving visual realism. While most evaluations conflate color with other lighting attributes (e.g., intensity, direction), we isolate color as the primary variable of interest. Rather than introducing a new lighting estimation algorithm, we explore whether simple adaptation techniques can enhance the color accuracy of existing models. Using a novel HDR dataset featuring diverse lighting colors, we systematically evaluate several adaptation strategies. Our results show that preprocessing the input image with a pre-trained white balance network improves color robustness, outperforming other strategies across all tested scenarios. Notably, this approach requires no retraining of the lighting estimation model. We further validate the generality of this finding by applying the technique to three state-of-the-art lighting estimation methods from recent literature."
  },
  {
    "filename": "2509.18394v1.txt",
    "text": "Artificial intelligence risks are multidimensional in nature, as the same risk scenarios may have legal, operational, and financial risk dimensions. With the emergence of new AI regulations, the state of the art of artificial intelligence risk management seems to be highly immature due to upcoming AI regulations. Despite the appearance of several methodologies and generic criteria, it is rare to find guidelines with real implementation value, considering that the most important issue is customizing artificial intelligence risk metrics and risk models for specific AI risk scenarios. Furthermore, the financial departments, legal departments and Government Risk Compliance teams seem to remain unaware of many technical aspects of AI systems, in which data scientists and AI engineers emerge as the most appropriate implementers. It is crucial to decompose the problem of artificial intelligence risk in several dimensions: data protection, fairness, accuracy, robustness, and information security. Consequently, the main task is developing adequate metrics and risk models that manage to reduce uncertainty for decision-making in order to take informed decisions concerning the risk management of AI systems.   The purpose of this paper is to orientate AI stakeholders about the depths of AI risk management. Although it is not extremely technical, it requires a basic knowledge of risk management, quantifying uncertainty, the FAIR model, machine learning, large language models and AI context engineering. The examples presented pretend to be very basic and understandable, providing simple ideas that can be developed regarding specific AI customized environments. There are many issues to solve in AI risk management, and this paper will present a holistic overview of the inter-dependencies of AI risks, and how to model them together, within risk scenarios."
  },
  {
    "filename": "2509.18407v1.txt",
    "text": "Uncontrolled intersections account for a significant fraction of roadway crashes due to ambiguous right-of-way rules, occlusions, and unpredictable driver behavior. While autonomous vehicle research has explored uncertainty-aware decision making, few systems exist to retrofit human-operated vehicles with assistive navigation support. We present a driver-assist framework for right-of-way reasoning at uncontrolled intersections, formulated as a Partially Observable Markov Decision Process (POMDP). Using a custom simulation testbed with stochastic traffic agents, pedestrians, occlusions, and adversarial scenarios, we evaluate four decision-making approaches: a deterministic finite state machine (FSM), and three probabilistic planners: QMDP, POMCP, and DESPOT. Results show that probabilistic planners outperform the rule-based baseline, achieving up to 97.5 percent collision-free navigation under partial observability, with POMCP prioritizing safety and DESPOT balancing efficiency and runtime feasibility. Our findings highlight the importance of uncertainty-aware planning for driver assistance and motivate future integration of sensor fusion and environment perception modules for real-time deployment in realistic traffic environments."
  },
  {
    "filename": "2509.18409v1.txt",
    "text": "Er-doped Al$_2$O$_3$ is a promising host for telecom-band integrated photonics. Here we combine ab initio calculations with a symmetry-resolved analysis to elucidate substitutional Er on the Al site (Er$_\\mathrm{Al}$) in $\\alpha$-Al$_2$O$_3$. First-principles relaxations confirm the structural stability of Er$_\\mathrm{Al}$. We then use the local trigonal crystal-field symmetry to classify the Er-derived impurity levels by irreducible representations and to derive polarization-resolved electric-dipole selection rules, explicitly identifying the symmetry-allowed $f$\\textendash$d$ hybridization channels. Kubo--Greenwood absorption spectra computed from Kohn--Sham states quantitatively corroborate these symmetry predictions. Furthermore, we connect the calculated intra-$4f$ line strengths to Judd--Ofelt theory, clarifying the role of $4f$\\textendash$5d$ admixture in enabling optical activity. Notably, we predict a characteristic absorption near $1.47~\\mu\\mathrm{m}$ (telecom band), relevant for on-chip amplification and emission. To our knowledge, a symmetry-resolved first-principles treatment of Er:Al$_2$O$_3$ with an explicit Judd--Ofelt interpretation has not been reported, providing a transferable framework for tailoring rare-earth dopants in wide-band-gap oxides for integrated photonics. Our results for the optical spectra are in good agreement with experimental data."
  },
  {
    "filename": "2509.18414v1.txt",
    "text": "Residential electricity demand at granular scales is driven by what people do and for how long. Accurately forecasting this demand for applications like microgrid management and demand response therefore requires generative models that can produce realistic daily activity sequences, capturing both the timing and duration of human behavior. This paper develops a generative model of human activity sequences using nationally representative time-use diaries at a 10-minute resolution. We use this model to quantify which demographic factors are most critical for improving predictive performance.   We propose a hierarchical semi-Markov framework that addresses two key modeling challenges. First, a time-inhomogeneous Markov \\emph{router} learns the patterns of ``which activity comes next.\" Second, a semi-Markov \\emph{hazard} component explicitly models activity durations, capturing ``how long\" activities realistically last. To ensure statistical stability when data are sparse, the model pools information across related demographic groups and time blocks. The entire framework is trained and evaluated using survey design weights to ensure our findings are representative of the U.S. population.   On a held-out test set, we demonstrate that explicitly modeling durations with the hazard component provides a substantial and statistically significant improvement over purely Markovian models. Furthermore, our analysis reveals a clear hierarchy of demographic factors: Sex, Day-Type, and Household Size provide the largest predictive gains, while Region and Season, though important for energy calculations, contribute little to predicting the activity sequence itself. The result is an interpretable and robust generator of synthetic activity traces, providing a high-fidelity foundation for downstream energy systems modeling."
  },
  {
    "filename": "2509.18438v1.txt",
    "text": "We study Landau-level mixing in a time-reversal-symmetric Hamiltonian composed of two sets of Landau levels with opposite magnetic field, relevant to moir\\'e minibands in twisted homobilayer transition-metal dichalcogenides in the adiabatic limit, where electrons in opposite valleys have flat Chern bands with opposite Chern numbers. Strong spin-orbit coupling polarizes spins in opposite directions in opposite valleys, separating Coulomb interactions into like-spin ($V^{\\uparrow\\uparrow}$) and opposite-spin ($V^{\\uparrow\\downarrow}$). Using degenerate perturbation theory, we compute Landau-level mixing corrections to $V^{\\uparrow\\uparrow}$ and $V^{\\uparrow\\downarrow}$ for different filling fractions. In the lowest Landau level, screening exhibits an even-odd effect: $V^{\\uparrow\\uparrow}$ is reduced more strongly than $V^{\\uparrow\\downarrow}$ in even-$m$ angular momentum Haldane pseudopotential and less strongly in odd-$m$ angular momentum ones. In the first Landau level, the short-range part ($m=0,1$) of $V^{\\uparrow\\downarrow}$ is reduced comparably to $V^{\\uparrow\\uparrow}$, while the strongest spin anisotropy appears in the $m=2$ pseudopotential. These novel short-range spin correlations have important implications for candidate correlated phases of fractional quantum spin Hall insulators. A distinctive feature of this time-reversal-symmetric Hamiltonian, absent in conventional quantum Hall systems, is that spin-flip excitations form localized quasiparticles. We compute their excitation spectrum and predict a non-monotonic dependence of the ordering temperature of Chern ferromagnetism in MoTe$_2$ on the Landau-level mixing parameter."
  },
  {
    "filename": "2509.18439v1.txt",
    "text": "Shared decision-making (SDM) is necessary to achieve patient-centred care. Currently no methodology exists to automatically measure SDM at scale. This study aimed to develop an automated approach to measure SDM by using language modelling and the conversational alignment (CA) score. A total of 157 video-recorded patient-doctor conversations from a randomized multi-centre trial evaluating SDM decision aids for anticoagulation in atrial fibrillations were transcribed and segmented into 42,559 sentences. Context-response pairs and negative sampling were employed to train deep learning (DL) models and fine-tuned BERT models via the next sentence prediction (NSP) task. Each top-performing model was used to calculate four types of CA scores. A random-effects analysis by clinician, adjusting for age, sex, race, and trial arm, assessed the association between CA scores and SDM outcomes: the Decisional Conflict Scale (DCS) and the Observing Patient Involvement in Decision-Making 12 (OPTION12) scores. p-values were corrected for multiple comparisons with the Benjamini-Hochberg method. Among 157 patients (34% female, mean age 70 SD 10.8), clinicians on average spoke more words than patients (1911 vs 773). The DL model without the stylebook strategy achieved a recall@1 of 0.227, while the fine-tuned BERTbase (110M) achieved the highest recall@1 with 0.640. The AbsMax (18.36 SE7.74 p=0.025) and Max CA (21.02 SE7.63 p=0.012) scores generated with the DL without stylebook were associated with OPTION12. The Max CA score generated with the fine-tuned BERTbase (110M) was associated with the DCS score (-27.61 SE12.63 p=0.037). BERT model sizes did not have an impact the association between CA scores and SDM. This study introduces an automated, scalable methodology to measure SDM in patient-doctor conversations through explainable CA scores, with potential to evaluate SDM strategies at scale."
  },
  {
    "filename": "2509.18445v1.txt",
    "text": "The simulation of complex physical systems using a discretized mesh is a cornerstone of applied mechanics, but traditional numerical solvers are often computationally prohibitive for many-query tasks. While Graph Neural Networks (GNNs) have emerged as powerful surrogate models for mesh-based data, their standard autoregressive application for long-term prediction is often plagued by error accumulation and instability. To address this, we introduce MeshODENet, a general framework that synergizes the spatial reasoning of GNNs with the continuous-time modeling of Neural Ordinary Differential Equations. We demonstrate the framework's effectiveness and versatility on a series of challenging structural mechanics problems, including one- and two-dimensional elastic bodies undergoing large, non-linear deformations. The results demonstrate that our approach significantly outperforms baseline models in long-term predictive accuracy and stability, while achieving substantial computational speed-ups over traditional solvers. This work presents a powerful and generalizable approach for developing data-driven surrogates to accelerate the analysis and modeling of complex structural systems."
  },
  {
    "filename": "2509.18446v1.txt",
    "text": "The 2024 US presidential election is the first major contest to occur in the US since the popularization of large language models (LLMs). Building on lessons from earlier shifts in media (most notably social media's well studied role in targeted messaging and political polarization) this moment raises urgent questions about how LLMs may shape the information ecosystem and influence political discourse. While platforms have announced some election safeguards, how well they work in practice remains unclear. Against this backdrop, we conduct a large-scale, longitudinal study of 12 models, queried using a structured survey with over 12,000 questions on a near-daily cadence from July through November 2024. Our design systematically varies content and format, resulting in a rich dataset that enables analyses of the models' behavior over time (e.g., across model updates), sensitivity to steering, responsiveness to instructions, and election-related knowledge and \"beliefs.\" In the latter half of our work, we perform four analyses of the dataset that (i) study the longitudinal variation of model behavior during election season, (ii) illustrate the sensitivity of election-related responses to demographic steering, (iii) interrogate the models' beliefs about candidates' attributes, and (iv) reveal the models' implicit predictions of the election outcome. To facilitate future evaluations of LLMs in electoral contexts, we detail our methodology, from question generation to the querying pipeline and third-party tooling. We also publicly release our dataset at https://huggingface.co/datasets/sarahcen/llm-election-data-2024"
  },
  {
    "filename": "2509.18452v1.txt",
    "text": "Large, sparse linear systems are pervasive in modern science and engineering, and Krylov subspace solvers are an established means of solving them. Yet convergence can be slow for ill-conditioned matrices, so practical deployments usually require preconditioners. Markov chain Monte Carlo (MCMC)-based matrix inversion can generate such preconditioners and accelerate Krylov iterations, but its effectiveness depends on parameters whose optima vary across matrices; manual or grid search is costly. We present an AI-driven framework recommending MCMC parameters for a given linear system. A graph neural surrogate predicts preconditioning speed from $A$ and MCMC parameters. A Bayesian acquisition function then chooses the parameter sets most likely to minimise iterations. On a previously unseen ill-conditioned system, the framework achieves better preconditioning with 50\\% of the search budget of conventional methods, yielding about a 10\\% reduction in iterations to convergence. These results suggest a route for incorporating MCMC-based preconditioners into large-scale systems."
  },
  {
    "filename": "2509.18454v1.txt",
    "text": "Computer games, as fully controlled simulated environments, have been utilized in significant scientific studies demonstrating the application of Reinforcement Learning (RL). Gaming and esports are key areas influenced by the application of Artificial Intelligence (AI) and Machine Learning (ML) solutions at scale. Tooling simplifies scientific workloads and is essential for developing the gaming and esports research area.   In this work, we present ``SC2Tools'', a toolset containing multiple submodules responsible for working with, and producing larger datasets. We provide a modular structure of the implemented tooling, leaving room for future extensions where needed. Additionally, some of the tools are not StarCraft~2 exclusive and can be used with other types of data for dataset creation.   The tools we present were leveraged in creating one of the largest StarCraft~2 tournament datasets to date with a separate PyTorch and PyTorch Lightning application programming interface (API) for easy access to the data.   We conclude that alleviating the burden of data collection, preprocessing, and custom code development is essential for less technically proficient researchers to engage in the growing gaming and esports research area. Finally, our solution provides some foundational work toward normalizing experiment workflow in StarCraft~2"
  },
  {
    "filename": "2509.18455v1.txt",
    "text": "Nonprehensile manipulation, such as pushing and pulling, enables robots to move, align, or reposition objects that may be difficult to grasp due to their geometry, size, or relationship to the robot or the environment. Much of the existing work in nonprehensile manipulation relies on parallel-jaw grippers or tools such as rods and spatulas. In contrast, multi-fingered dexterous hands offer richer contact modes and versatility for handling diverse objects to provide stable support over the objects, which compensates for the difficulty of modeling the dynamics of nonprehensile manipulation. Therefore, we propose Geometry-aware Dexterous Pushing and Pulling (GD2P) for nonprehensile manipulation with dexterous robotic hands. We study pushing and pulling by framing the problem as synthesizing and learning pre-contact dexterous hand poses that lead to effective manipulation. We generate diverse hand poses via contact-guided sampling, filter them using physics simulation, and train a diffusion model conditioned on object geometry to predict viable poses. At test time, we sample hand poses and use standard motion planners to select and execute pushing and pulling actions. We perform 840 real-world experiments with an Allegro Hand, comparing our method to baselines. The results indicate that GD2P offers a scalable route for training dexterous nonprehensile manipulation policies. We further demonstrate GD2P on a LEAP Hand, highlighting its applicability to different hand morphologies. Our pre-trained models and dataset, including 1.3 million hand poses across 2.3k objects, will be open-source to facilitate further research. Our project website is available at: geodex2p.github.io."
  },
  {
    "filename": "2509.18457v1.txt",
    "text": "This paper proposes GluMind, a transformer-based multimodal framework designed for continual and long-term blood glucose forecasting. GluMind devises two attention mechanisms, including cross-attention and multi-scale attention, which operate in parallel and deliver accurate predictive performance. Cross-attention effectively integrates blood glucose data with other physiological and behavioral signals such as activity, stress, and heart rate, addressing challenges associated with varying sampling rates and their adverse impacts on robust prediction. Moreover, the multi-scale attention mechanism captures long-range temporal dependencies. To mitigate catastrophic forgetting, GluMind incorporates a knowledge retention technique into the transformer-based forecasting model. The knowledge retention module not only enhances the model's ability to retain prior knowledge but also boosts its overall forecasting performance. We evaluate GluMind on the recently released AIREADI dataset, which contains behavioral and physiological data collected from healthy people, individuals with prediabetes, and those with type 2 diabetes. We examine the performance stability and adaptability of GluMind in learning continuously as new patient cohorts are introduced. Experimental results show that GluMind consistently outperforms other state-of-the-art forecasting models, achieving approximately 15% and 9% improvements in root mean squared error (RMSE) and mean absolute error (MAE), respectively."
  },
  {
    "filename": "2509.18459v1.txt",
    "text": "The Binary Emax model is widely employed in dose-response analysis during Phase II clinical studies to identify the optimal dose for subsequence confirmatory trials. The parameter estimation and inference heavily rely on the asymptotic properties of Maximum Likelihood (ML) estimators; however, this approach may be questionable under small or moderate sample sizes and is not robust to violation of model assumptions. To provide a reliable solution, this paper examines three bias-reduction methods: the Cox-Snell bias correction, Firth-score modification, and a maximum penalized likelihood estimator (MPLE) using Jeffreys prior. Through comprehensive simulation studies, we evaluate the performance of these methods in reducing bias and controlling variance, especially when model assumptions are violated. The results demonstrate that both Firth and MPLE methods provide robust estimates, with MPLE outperforming in terms of stability and lower variance. We further illustrate the practical application of these methods using data from the TURANDOT study, a Phase II clinical trial. Our findings suggest that MPLE with Jeffreys prior offers an effective and reliable alternative to the Firth method, particularly for dose-response relationships that deviate from monotonicity, making it valuable for robust parameter estimation in dose-ranging studies."
  },
  {
    "filename": "2509.18461v1.txt",
    "text": "Generative adversarial networks (GANs) and diffusion models have dramatically advanced deepfake technology, and its threats to digital security, media integrity, and public trust have increased rapidly. This research explored zero-shot deepfake detection, an emerging method even when the models have never seen a particular deepfake variation. In this work, we studied self-supervised learning, transformer-based zero-shot classifier, generative model fingerprinting, and meta-learning techniques that better adapt to the ever-evolving deepfake threat. In addition, we suggested AI-driven prevention strategies that mitigated the underlying generation pipeline of the deepfakes before they occurred. They consisted of adversarial perturbations for creating deepfake generators, digital watermarking for content authenticity verification, real-time AI monitoring for content creation pipelines, and blockchain-based content verification frameworks. Despite these advancements, zero-shot detection and prevention faced critical challenges such as adversarial attacks, scalability constraints, ethical dilemmas, and the absence of standardized evaluation benchmarks. These limitations were addressed by discussing future research directions on explainable AI for deepfake detection, multimodal fusion based on image, audio, and text analysis, quantum AI for enhanced security, and federated learning for privacy-preserving deepfake detection. This further highlighted the need for an integrated defense framework for digital authenticity that utilized zero-shot learning in combination with preventive deepfake mechanisms. Finally, we highlighted the important role of interdisciplinary collaboration between AI researchers, cybersecurity experts, and policymakers to create resilient defenses against the rising tide of deepfake attacks."
  },
  {
    "filename": "2509.18466v1.txt",
    "text": "Model predictive control (MPC) has demonstrated effectiveness for humanoid bipedal locomotion; however, its applicability in challenging environments, such as rough and slippery terrain, is limited by the difficulty of modeling terrain interactions. In contrast, reinforcement learning (RL) has achieved notable success in training robust locomotion policies over diverse terrain, yet it lacks guarantees of constraint satisfaction and often requires substantial reward shaping. Recent efforts in combining MPC and RL have shown promise of taking the best of both worlds, but they are primarily restricted to flat terrain or quadrupedal robots. In this work, we propose an RL-augmented MPC framework tailored for bipedal locomotion over rough and slippery terrain. Our method parametrizes three key components of single-rigid-body-dynamics-based MPC: system dynamics, swing leg controller, and gait frequency. We validate our approach through bipedal robot simulations in NVIDIA IsaacLab across various terrains, including stairs, stepping stones, and low-friction surfaces. Experimental results demonstrate that our RL-augmented MPC framework produces significantly more adaptive and robust behaviors compared to baseline MPC and RL."
  },
  {
    "filename": "2509.18472v1.txt",
    "text": "RISC-V ISA-based processors have recently emerged as both powerful and energy-efficient computing platforms. The release of the MILK-V Pioneer marked a significant milestone as the first desktop-grade RISC-V system. With increasing engagement from both academia and industry, such platforms exhibit strong potential for adoption in high-performance computing (HPC) environments.   The open-source, FPGA-accelerated FireSim framework has emerged as a flexible and scalable tool for architectural exploration, enabling simulation of various system configurations using RISC-V cores. Despite its capabilities, there remains a lack of systematic evaluation regarding the feasibility and performance prediction accuracy of FireSim when compared to physical hardware.   In this study, we address this gap by modeling a commercially available single-board computer and a desktop-grade RISC-V CPU within FireSim. To ensure fidelity between simulation and real hardware, we first measure the performance of a series of benchmarks to compare runtime behavior under single-core and four-core configurations. Based on the closest matching simulation parameters, we subsequently evaluate performance using a representative mini-application and the LAMMPS molecular dynamics code.   Our findings indicate that while FireSim provides valuable insights into architectural performance trends, discrepancies remain between simulated and measured runtimes. These deviations stem from both inherent limitations of the simulation environment and the restricted availability of detailed performance specifications from CPU manufacturers, which hinder precise configuration matching."
  },
  {
    "filename": "2509.18478v1.txt",
    "text": "The conformational properties of linear alkanes, C$_n$H$_{2n+2}$, have been of intense interest for many years. Experiments and corresponding electronic structure calculations were first reported in the mid-2000s and continue to the present time. These focus on the minimum chain length where the transition from the linear minimum to the hairpin minimum occurs. We recently reported a transferable many-body permutationally invariant polynomial (MB-PIP) for linear alkanes using B3LYP electronic energies, which do not account for dispersion. Here we report a $\\Delta$-ML approach to elevate this B3LYP-based and new PBE0+MBD MB-PIP potentials using PNO-LCCSD(T)-F12 energies. The new $\\Delta$-corrected potentials predict the difference in these minima accurately, compared to benchmark CCSD(T) results, over the range C$_{12}$H$_{28}$ to C$_{28}$H$_{58}$. Vibrational power spectra are also reported for C$_{14}$H$_{30}$ and C$_{30}$H$_{62}$ using the uncorrected and $\\Delta$-ML B3LYP. These new PIP-MB potentials for linear alkanes are the most accurate ones currently available and can be used in studies of properties of linear alkanes."
  },
  {
    "filename": "2509.18480v1.txt",
    "text": "Protein folding models have achieved groundbreaking results typically via a combination of integrating domain knowledge into the architectural blocks and training pipelines. Nonetheless, given the success of generative models across different but related problems, it is natural to question whether these architectural designs are a necessary condition to build performant models. In this paper, we introduce SimpleFold, the first flow-matching based protein folding model that solely uses general purpose transformer blocks. Protein folding models typically employ computationally expensive modules involving triangular updates, explicit pair representations or multiple training objectives curated for this specific domain. Instead, SimpleFold employs standard transformer blocks with adaptive layers and is trained via a generative flow-matching objective with an additional structural term. We scale SimpleFold to 3B parameters and train it on approximately 9M distilled protein structures together with experimental PDB data. On standard folding benchmarks, SimpleFold-3B achieves competitive performance compared to state-of-the-art baselines, in addition SimpleFold demonstrates strong performance in ensemble prediction which is typically difficult for models trained via deterministic reconstruction objectives. Due to its general-purpose architecture, SimpleFold shows efficiency in deployment and inference on consumer-level hardware. SimpleFold challenges the reliance on complex domain-specific architectures designs in protein folding, opening up an alternative design space for future progress."
  },
  {
    "filename": "2509.18483v1.txt",
    "text": "The prediction of quantum dynamical responses lies at the heart of modern physics. Yet, modeling these time-dependent behaviors remains a formidable challenge because quantum systems evolve in high-dimensional Hilbert spaces, often rendering traditional numerical methods computationally prohibitive. While large language models have achieved remarkable success in sequential prediction, quantum dynamics presents a fundamentally different challenge: forecasting the entire temporal evolution of quantum systems rather than merely the next element in a sequence. Existing neural architectures such as recurrent and convolutional networks often require vast training datasets and suffer from spurious oscillations that compromise physical interpretability. In this work, we introduce a fundamentally new approach: Kolmogorov Arnold Networks (KANs) augmented with physics-informed loss functions that enforce the Ehrenfest theorems. Our method achieves superior accuracy with significantly less training data: it requires only 5.4 percent of the samples (200) compared to Temporal Convolution Networks (3,700). We further introduce the Chain of KANs, a novel architecture that embeds temporal causality directly into the model design, making it particularly well-suited for time series modeling. Our results demonstrate that physics-informed KANs offer a compelling advantage over conventional black-box models, maintaining both mathematical rigor and physical consistency while dramatically reducing data requirements."
  },
  {
    "filename": "2509.18492v1.txt",
    "text": "Strategic Traffic Management Initiatives (TMIs) such as Ground Delay Programs (GDPs) play a crucial role in mitigating operational costs associated with air traf- fic demand-capacity imbalances. However, GDPs can only be planned (e.g., duration, delay assignments) with confidence if the future capacities at constrained re- sources (i.e., airports) are predictable. In reality, such future capacities are uncer- tain, and predictive models may provide predictions that are vulnerable to errors and distribution shifts. Motivated by the goal of planning optimal GDPs that are distributionally robust against airport capacity prediction errors, we study a fully integrated learning-driven optimization framework. We design a deep learning- based prediction model capable of forecasting arrival and departure capacity distributions across a network of airports. We incorporate the predictions into a distributionally robust formulation of the multi-airport ground holding program (DR- MAGHP). Our results demonstrate that DR-MAGHP can achieve up to a 15.6% improvement over the stochastic programming formulation (SP-MAGHP) under airport capacity distribution shifts. We conclude by outlining future research di- rections aimed at enhancing both the learning and optimization components of the framework."
  },
  {
    "filename": "2509.18495v1.txt",
    "text": "Gravity plays important roles at multiple scales in the universe. An important, yet often neglected, role of gravity is its ability in driving anisotropic fragmentation through tides. When tides dominate, fragmentation becomes anisotropic, and the Jeans length along the short axis, $l_{\\rm tidal, Jeans}$, is approximately $\\sigma_{\\rm v}/\\sqrt{G \\rho_{\\rm mean}}$, determined by the external tides through the mean density $\\rho_{\\rm mean}$. We compare predictions of $l_{\\rm tidal, Jeans}$ against observational results in massive star-forming clumps, the Circumnuclear Disk (CND) around the supermassive black hole Sgr A* at the center of the Galaxy, the Central Molecular Zone in the Galactic Center, a hub-filament system, and a streamer around a young star. We find that the observed widths of these filamentary structures match theoretical predictions from tidally-controlled Jeans fragmentation. The formation of filaments can potentially shield cold gas against radiation pressure and photoevaporation, as well as hydrodynamical interaction with the ambient medium, potentially enabling the cold gas to survive. Thus, tidal forces are major players regulating gas transport around massive objects."
  },
  {
    "filename": "2509.18500v1.txt",
    "text": "To make useful connections with experimental measurements, correlated electronic structure theories must accurately predict chemical properties in addition to energies. We present a finite-difference based algorithm to compute first-order response properties with phaseless auxiliary-field quantum Monte Carlo (ph-AFQMC) that relies on a branching correlated sampling approach. Focusing on electric dipole moments, we show that mean-field trial wave functions are sufficient to obtain high accuracy relative to CCSD(T) and experimental measurements for a set of 21 molecules, ranging in size from 2 to 18 atoms in their equilibrium geometries. As with energies, the quality of predicted dipole moments can be systematically improved with the use of correlated trial wave functions, even (or especially) in strongly correlated regimes. We show that the challenges faced by low-order perturbation theories in predicting the dipole moment of hydrogen fluoride across its dissociation coordinate are overcome with ph-AFQMC when using relatively simple trials. The key advantage of our approach over those previously reported for ph-AFQMC is its scalability to large system sizes with a phaseless bias no worse than that of a typical ground-state energy calculation; we routinely converge dipole moments for systems with more than one thousand basis functions."
  },
  {
    "filename": "2509.18506v1.txt",
    "text": "This paper presents a novel envelope based model predictive control (MPC) framework designed to enable autonomous vehicles to handle high performance driving across a wide range of scenarios without a predefined reference. In high performance autonomous driving, safe operation at the vehicle's dynamic limits requires a real time planning and control framework capable of accounting for key vehicle dynamics and environmental constraints when following a predefined reference trajectory is suboptimal or even infeasible. State of the art planning and control frameworks, however, are predominantly reference based, which limits their performance in such situations. To address this gap, this work first introduces a computationally efficient vehicle dynamics model tailored for optimization based control and a continuously differentiable mathematical formulation that accurately captures the entire drivable envelope. This novel model and formulation allow for the direct integration of dynamic feasibility and safety constraints into a unified planning and control framework, thereby removing the necessity for predefined references. The challenge of envelope planning, which refers to maximally approximating the safe drivable area, is tackled by combining reinforcement learning with optimization techniques. The framework is validated through both simulations and real world experiments, demonstrating its high performance across a variety of tasks, including racing, emergency collision avoidance and off road navigation. These results highlight the framework's scalability and broad applicability across a diverse set of scenarios."
  },
  {
    "filename": "2509.18507v1.txt",
    "text": "High-dimensional imaging of neural activity, such as widefield calcium and functional ultrasound imaging, provide a rich source of information for understanding the relationship between brain activity and behavior. Accurately modeling neural dynamics in these modalities is crucial for understanding this relationship but is hindered by the high-dimensionality, complex spatiotemporal dependencies, and prevalent behaviorally irrelevant dynamics in these modalities. Existing dynamical models often employ preprocessing steps to obtain low-dimensional representations from neural image modalities. However, this process can discard behaviorally relevant information and miss spatiotemporal structure. We propose SBIND, a novel data-driven deep learning framework to model spatiotemporal dependencies in neural images and disentangle their behaviorally relevant dynamics from other neural dynamics. We validate SBIND on widefield imaging datasets, and show its extension to functional ultrasound imaging, a recent modality whose dynamical modeling has largely remained unexplored. We find that our model effectively identifies both local and long-range spatial dependencies across the brain while also dissociating behaviorally relevant neural dynamics. Doing so, SBIND outperforms existing models in neural-behavioral prediction. Overall, SBIND provides a versatile tool for investigating the neural mechanisms underlying behavior using imaging modalities."
  },
  {
    "filename": "2509.18512v1.txt",
    "text": "In Navier-Stokes turbulence, a bottleneck effect in the energy cascade near the viscous cutoff causes an overshoot in the energy spectrum, or spectral bump, relative to Kolmogorov's -5/3 scaling. A similar overshoot occurs in large-eddy simulations (LES) when an eddy viscosity model is used. It is not a viscous phenomenon but is caused by error in the residual stress model. This artificial bottleneck effect in LES leads to an over-prediction of kinetic energy even when a dynamic procedure is used to capture the spectral decay at the cutoff scale. Recently, Johnson [2022, J. Fluid Mech., 934, A30] introduced a generalization of spatial filtering that provides a dynamic procedure without a test filter. In this paper, this method of Stokes flow regularization (SFR) is used with kinetic energy considerations to generate a range of LES models and explore the bottleneck effect in more detail. The coefficients for each dynamic model are determined locally, without averaging over homogeneous directions. A posteriori tests of the models in isotropic turbulence are reported, demonstrating the robustness of the SFR-based procedure for different model forms and enabling fair comparisons in terms of their impact on the bottleneck effect. An effective way to mitigate the bottleneck is to add a nonlinear gradient component in the residual stress closure, forming a dynamic mixed model. This approach improves representation of residual stress structure and energy cascade efficiencies."
  },
  {
    "filename": "2509.18517v1.txt",
    "text": "In recent work [Vavrek et al. (2025)], we developed the performance optimization framework spectre-ml for gamma spectrometers with variable performance across many readout channels. The framework uses non-negative matrix factorization (NMF) and clustering to learn groups of similarly-performing channels and sweep through various learned channel combinations to optimize the performance tradeoff of including worse-performing channels for better total efficiency. In this work, we integrate the pyGEM uranium enrichment assay code with our spectre-ml framework, and show that the U-235 enrichment relative uncertainty can be directly used as an optimization target. We find that this optimization reduces relative uncertainties after a 30-minute measurement by an average of 20%, as tested on six different H3D M400 CdZnTe spectrometers, which can significantly improve uranium non-destructive assay measurement times in nuclear safeguards contexts. Additionally, this work demonstrates that the spectre-ml optimization framework can accommodate arbitrary end-user spectroscopic analysis code and performance metrics, enabling future optimizations for complex Pu spectra."
  },
  {
    "filename": "2509.18519v1.txt",
    "text": "We present Whack-a-Mole, a deterministic packet spraying algorithm for distributing packets across multiple network paths with provably tight discrepancy bounds. The algorithm is motivated by large-scale distributed AI/ML training and inference workloads, where collective completion time (CCT) and effective training time ratio (ETTR) are highly sensitive to tail latency and transport imbalance. Whack-a-Mole represents the path profile as a discrete allocation of $m$ selection units across $n$ paths and uses a bit-reversal counter to choose a path for each packet. We prove that the discrepancy between expected and actual packet counts per path is bounded by $O(\\log m)$ over any contiguous packet sequence. The algorithm responds quickly to congestion feedback by reducing allocations to degraded paths and redistributing load to healthier ones. This combination of deterministic distribution, low per-packet overhead, and compatibility with erasure-coded transport makes Whack-a-Mole an effective building block for multipath transport protocols that aim to minimize CCT and maximize GPU utilization."
  },
  {
    "filename": "2509.18525v1.txt",
    "text": "The low-density region of the interstellar medium (ISM) where the Sun is located is known as the Local Bubble, a cavity filled with high-temperature and low-density plasma that may be created by a series of supernova (SN) explosions over the past 14 Myr. However, the effects of these SN explosions on the formation and evolution of the Local Bubble, as well as on nearby star formation, remain not fully understood. To study the expansion history of the Local Bubble, we use the kinematic data of the young stars obtained by cross-matching the pre-main-sequence (PMS) star catalog of \\citet{Zari2018} with the high-precision astrometric and photometric data from the {\\it Gaia} DR3 database. We perform a three-dimensional spatial clustering analysis on these young stars to identify star associations. We discover three unique star associations that exhibit a wiggle-like velocity pattern. The distances of these star associations are 108.5308, 141.5284, and 176.0318 pc, respectively. Their radial velocities in the Local Standard of Rest (LSR) are 10.0622, 5.4982, and 9.0581 km/s, showing a pattern of decreasing and then increasing. This velocity pattern, as predicted by \\citet{Krause&Diehl2014}, is caused by a recent re-acceleration affected by the SN explosion, reinforcing the picture of the Local Bubble as an evolving entity."
  },
  {
    "filename": "2509.18529v1.txt",
    "text": "A fundamental property of DNA is that the reverse complement (RC) of a sequence often carries identical biological meaning. However, state-of-the-art DNA language models frequently fail to capture this symmetry, producing inconsistent predictions for a sequence and its RC counterpart, which undermines their reliability. In this work, we introduce Reverse-Complement Consistency Regularization (RCCR), a simple and model-agnostic fine-tuning objective that directly penalizes the divergence between a model's prediction on a sequence and the aligned prediction on its reverse complement. We evaluate RCCR across three diverse backbones (Nucleotide Transformer, HyenaDNA, DNABERT-2) on a wide range of genomic tasks, including sequence classification, scalar regression, and profile prediction. Our experiments show that RCCR substantially improves RC robustness by dramatically reducing prediction flips and errors, all while maintaining or improving task accuracy compared to baselines such as RC data augmentation and test-time averaging. By integrating a key biological prior directly into the learning process, RCCR produces a single, intrinsically robust, and computationally efficient model fine-tuning recipe for diverse biology tasks."
  },
  {
    "filename": "2509.18532v1.txt",
    "text": "The EXL-50U spherical tokamak was built by Energy iNNovation to develop technologies for proton-boron fusion in spherical tokamaks (Liu et al., Phys. Plasmas 2024). We present a conceptual design of the Doppler backscattering (DBS) diagnostic for the EXL-50U spherical tokamak. DBS is a diagnostic capable of measuring plasma turbulence, which is especially important for transport in tokamaks. Starting from a set of physical design constraints, such as port window availability and in-vessel space, we used SCOTTY (Hall-Chen et al., PPCF 2022), an in-house beam tracing code, to predict the location of the cutoffs and the corresponding scattering wavenumbers for several EXL-50U plasma scenarios. We find that we are able to measure scattering locations of 0.15 $<$ $\\rho$ $<$ 1, with corresponding turbulent wavenumbers of 2.47 cm$^{-1}$$<$ $k_{\\perp}$ $<$ 9.49 cm$^{-1}$. Here, $\\rho$ is the normalised radial coordinate of the scattering location, and $k_{\\perp}$ is the corresponding turbulent wavenumber. We then determine the optimal toroidal launch angles to ensure that the probe beam's wavevector is perpendicular to the magnetic field at the cutoff location, thereby maximising the backscattered signal. This matching is crucial due to the EXL-50U's high magnetic pitch angle, $\\sim35^{\\circ}$ at the outboard midplane. Given our results, we propose the use of toroidal steering and tunable frequency channels to ensure beams are well-matched with the magnetic pitch angle. We propose a quasioptical system that covers the U-band range (40--60 GHz)."
  },
  {
    "filename": "2509.18573v1.txt",
    "text": "Porous materials exhibit vast structural diversity and support critical applications in gas storage, separations, and catalysis. However, predictive modeling remains challenging due to the multiscale nature of structure-property relationships, where performance is governed by both local chemical environments and global pore-network topology. These complexities, combined with sparse and unevenly distributed labeled data, hinder generalization across material families. We propose the Interaction Topological Transformer (ITT), a unified data-efficient framework that leverages novel interaction topology to capture materials information across multiple scales and multiple levels, including structural, elemental, atomic, and pairwise-elemental organization. ITT extracts scale-aware features that reflect both compositional and relational structure within complex porous frameworks, and integrates them through a built-in Transformer architecture that supports joint reasoning across scales. Trained using a two-stage strategy, i.e., self-supervised pretraining on 0.6 million unlabeled structures followed by supervised fine-tuning, ITT achieves state-of-the-art, accurate, and transferable predictions for adsorption, transport, and stability properties. This framework provides a principled and scalable path for learning-guided discovery in structurally and chemically diverse porous materials."
  },
  {
    "filename": "2509.18584v1.txt",
    "text": "Diffusion models are the mainstream approach for time series generation tasks. However, existing diffusion models for time series generation require retraining the entire framework to introduce specific conditional guidance. There also exists a certain degree of distributional bias between the generated data and the real data, which leads to potential model biases in downstream tasks. Additionally, the complexity of diffusion models and the latent spaces leads to an uninterpretable inference process. To address these issues, we propose the data style-guided diffusion model (DS-Diffusion). In the DS-Diffusion, a diffusion framework based on style-guided kernels is developed to avoid retraining for specific conditions. The time-information based hierarchical denoising mechanism (THD) is developed to reduce the distributional bias between the generated data and the real data. Furthermore, the generated samples can clearly indicate the data style from which they originate. We conduct comprehensive evaluations using multiple public datasets to validate our approach. Experimental results show that, compared to the state-of-the-art model such as ImagenTime, the predictive score and the discriminative score decrease by 5.56% and 61.55%, respectively. The distributional bias between the generated data and the real data is further reduced, the inference process is also more interpretable. Moreover, by eliminating the need to retrain the diffusion model, the flexibility and adaptability of the model to specific conditions are also enhanced."
  },
  {
    "filename": "2509.18609v1.txt",
    "text": "End-to-end motion planning is promising for simplifying complex autonomous driving pipelines. However, challenges such as scene understanding and effective prediction for decision-making continue to present substantial obstacles to its large-scale deployment. In this paper, we present PIE, a pioneering framework that integrates advanced perception, reasoning, and intention modeling to dynamically capture interactions between the ego vehicle and surrounding agents. It incorporates a bidirectional Mamba fusion that addresses data compression losses in multimodal fusion of camera and LiDAR inputs, alongside a novel reasoning-enhanced decoder integrating Mamba and Mixture-of-Experts to facilitate scene-compliant anchor selection and optimize adaptive trajectory inference. PIE adopts an action-motion interaction module to effectively utilize state predictions of surrounding agents to refine ego planning. The proposed framework is thoroughly validated on the NAVSIM benchmark. PIE, without using any ensemble and data augmentation techniques, achieves an 88.9 PDM score and 85.6 EPDM score, surpassing the performance of prior state-of-the-art methods. Comprehensive quantitative and qualitative analyses demonstrate that PIE is capable of reliably generating feasible and high-quality ego trajectories."
  },
  {
    "filename": "2509.18611v1.txt",
    "text": "Pretraining on large-scale collections of PDE-governed spatiotemporal trajectories has recently shown promise for building generalizable models of dynamical systems. Yet most existing PDE foundation models rely on deterministic Transformer architectures, which lack generative flexibility for many science and engineering applications. We propose Flow Marching, an algorithm that bridges neural operator learning with flow matching motivated by an analysis of error accumulation in physical dynamical systems, and we build a generative PDE foundation model on top of it. By jointly sampling the noise level and the physical time step between adjacent states, the model learns a unified velocity field that transports a noisy current state toward its clean successor, reducing long-term rollout drift while enabling uncertainty-aware ensemble generations. Alongside this core algorithm, we introduce a Physics-Pretrained Variational Autoencoder (P2VAE) to embed physical states into a compact latent space, and an efficient Flow Marching Transformer (FMT) that combines a diffusion-forcing scheme with latent temporal pyramids, achieving up to 15x greater computational efficiency than full-length video diffusion models and thereby enabling large-scale pretraining at substantially reduced cost. We curate a corpus of ~2.5M trajectories across 12 distinct PDE families and train suites of P2VAEs and FMTs at multiple scales. On downstream evaluation, we benchmark on unseen Kolmogorov turbulence with few-shot adaptation, demonstrate long-term rollout stability over deterministic counterparts, and present uncertainty-stratified ensemble results, highlighting the importance of generative PDE foundation models for real-world applications."
  },
  {
    "filename": "2509.18624v1.txt",
    "text": "Lane changes (LCs) in congested traffic are complex, multi-vehicle interactive events that pose significant safety concerns. Providing early warnings can enable more proactive driver assistance system and support more informed decision-making for drivers under LCs. This paper presents an interaction-aware Lane-Changing Early Warning (LCEW) system designed to issue reliable early warning signals based on future trajectory predictions. We first investigate the stochastic nature of LCs, characterized by (i) variable-size multi-vehicle interactions and (ii) the direct and indirect risks resulting from these interactions. To model these stochastic interactions, a Social Spatio-Temporal Graph Convolutional Neural Network framework informed by mutual information (STGCNN-MI) is introduced to predict multi-vehicle trajectories. By leveraging a MI-based adjacency matrix, the framework enhances trajectory prediction accuracy while providing interpretable representations of vehicle interactions. Then, potential collisions between the LC vehicle and adjacent vehicles (direct risks) or among the non-adjacent vehicles (indirect risks) are identified using oriented bounding box detection applied to the predicted trajectories. Finally, a warning signal is generated to inform the LC driver of location of potential collisions within the predicted time window. Traffic simulation experiments conducted in SUMO demonstrate that the proposed interaction-aware LCEW improves both vehicle-level safety and overall traffic efficiency, while also promoting more natural behavioral adaptation."
  },
  {
    "filename": "2509.18632v1.txt",
    "text": "To assist users in complex tasks, LLMs generate plans: step-by-step instructions towards a goal. While alignment methods aim to ensure LLM plans are helpful, they train (RLHF) or evaluate (ChatbotArena) on what users prefer, assuming this reflects what helps them. We test this with Planorama: an interface where 126 users answer 300 multi-step questions with LLM plans. We get 4388 plan executions and 5584 comparisons to measure plan helpfulness (QA success) and user preferences on plans, and recreate the setup in agents and reward models to see if they simulate or prefer what helps users. We expose: 1) user/model preferences and agent success do not accurately predict which plans help users, so common alignment feedback can misalign with helpfulness; 2) this gap is not due to user-specific preferences, as users are similarly successful when using plans they prefer/disprefer; 3) surface-level cues like brevity and question similarity strongly link to preferences, but such biases fail to predict helpfulness. In all, we argue aligning helpful LLMs needs feedback from real user interactions, not just preferences of what looks helpful, so we discuss the plan NLP researchers can execute to solve this problem."
  },
  {
    "filename": "2509.18644v1.txt",
    "text": "Imitation-learning-based visuomotor policies have been widely used in robot manipulation, where both visual observations and proprioceptive states are typically adopted together for precise control. However, in this study, we find that this common practice makes the policy overly reliant on the proprioceptive state input, which causes overfitting to the training trajectories and results in poor spatial generalization. On the contrary, we propose the State-free Policy, removing the proprioceptive state input and predicting actions only conditioned on visual observations. The State-free Policy is built in the relative end-effector action space, and should ensure the full task-relevant visual observations, here provided by dual wide-angle wrist cameras. Empirical results demonstrate that the State-free policy achieves significantly stronger spatial generalization than the state-based policy: in real-world tasks such as pick-and-place, challenging shirt-folding, and complex whole-body manipulation, spanning multiple robot embodiments, the average success rate improves from 0\\% to 85\\% in height generalization and from 6\\% to 64\\% in horizontal generalization. Furthermore, they also show advantages in data efficiency and cross-embodiment adaptation, enhancing their practicality for real-world deployment."
  },
  {
    "filename": "2509.18648v1.txt",
    "text": "Safety remains a major concern for deploying reinforcement learning (RL) in real-world applications. Simulators provide safe, scalable training environments, but the inevitable sim-to-real gap introduces additional safety concerns, as policies must satisfy constraints in real-world conditions that differ from simulation. To address this challenge, robust safe RL techniques offer principled methods, but are often incompatible with standard scalable training pipelines. In contrast, domain randomization, a simple and popular sim-to-real technique, stands out as a promising alternative, although it often results in unsafe behaviors in practice. We present SPiDR, short for Sim-to-real via Pessimistic Domain Randomization -- a scalable algorithm with provable guarantees for safe sim-to-real transfer. SPiDR uses domain randomization to incorporate the uncertainty about the sim-to-real gap into the safety constraints, making it versatile and highly compatible with existing training pipelines. Through extensive experiments on sim-to-sim benchmarks and two distinct real-world robotic platforms, we demonstrate that SPiDR effectively ensures safety despite the sim-to-real gap while maintaining strong performance."
  },
  {
    "filename": "2509.18658v1.txt",
    "text": "LLM-as-a-judge has become a promising paradigm for using large language models (LLMs) to evaluate natural language generation (NLG), but the uncertainty of its evaluation remains underexplored. This lack of reliability may limit its deployment in many applications. This work presents the first framework to analyze the uncertainty by offering a prediction interval of LLM-based scoring via conformal prediction. Conformal prediction constructs continuous prediction intervals from a single evaluation run, and we design an ordinal boundary adjustment for discrete rating tasks. We also suggest a midpoint-based score within the interval as a low-bias alternative to raw model score and weighted average. We perform extensive experiments and analysis, which show that conformal prediction can provide valid prediction interval with coverage guarantees. We also explore the usefulness of interval midpoint and judge reprompting for better judgment."
  },
  {
    "filename": "2509.18663v1.txt",
    "text": "Conjugated polymers are experiencing a surge of renewed interest due to their promising applications in various organic electronic devices. These include organic light-emitting diodes (OLEDs), field-effect transistors (FETs), and organic photovoltaic (OPV) devices, among many others. Their appeal stems from distinct advantages they hold over traditional inorganic semiconductors. Unlike inorganic semiconductors, where electrons are often considered to be in delocalized, free, or quasi-free states (as described by Bloch's theory), electrons in conjugated polymers behave differently. They are strongly coupled within highly localized $\\sigma$ or $\\pi$-orbitals and interact significantly with the ionic cores. This means they are far from the idealized delocalized states presumed by Bloch's theory approaches. Consequently, after nearly a century of applying Bloch's theory to the electronic transport properties of inorganic materials, there is a clear need for a new theoretical framework to explain efficient charge transport in these organic solids. Our presented model addresses this need by incorporating crucial electron-electron interactions. Specifically, it accounts for both intra-site interactions and interactions between the $\\pi$-states located at alternating sites along the polymer chain. This framework provides a many-body charge conduction mechanism and explains the semiconducting properties of the undoped material. A significant outcome of our model is the prediction of two novel flat bands of excited bonding states. Intriguingly, these states obey Bose--Einstein statistics and facilitate charge transport. Furthermore, our model accurately reproduces experimental data, providing an excellent fit for measured UV-Vis absorption and electroluminescent spectra."
  },
  {
    "filename": "2509.18665v1.txt",
    "text": "Gravitational waves (GWs) from binary neutron star (BNS) merger remnants complement constraints from the inspiral phase, mass-radius measurements, and microscopic theory by providing information about the neutron-star equation of state (EOS) at extreme densities. We perform general-relativistic simulations of BNS mergers using EOS models that span the uncertain high-density regime. We find a robust correlation between the ratio of energy and angular momentum lost during the late-time post-merger GW signal - the long ringdown - and the EOS at the highest densities in neutron star cores. Applying this correlation to post-merger GW signals reduces EOS uncertainty at several times saturation density, where no direct constraints currently exist."
  },
  {
    "filename": "2509.18671v1.txt",
    "text": "In mobile manipulation, the manipulation policy has strong preferences for initial poses where it is executed. However, the navigation module focuses solely on reaching the task area, without considering which initial pose is preferable for downstream manipulation. To address this misalignment, we introduce N2M, a transition module that guides the robot to a preferable initial pose after reaching the task area, thereby substantially improving task success rates. N2M features five key advantages: (1) reliance solely on ego-centric observation without requiring global or historical information; (2) real-time adaptation to environmental changes; (3) reliable prediction with high viewpoint robustness; (4) broad applicability across diverse tasks, manipulation policies, and robot hardware; and (5) remarkable data efficiency and generalizability. We demonstrate the effectiveness of N2M through extensive simulation and real-world experiments. In the PnPCounterToCab task, N2M improves the averaged success rate from 3% with the reachability-based baseline to 54%. Furthermore, in the Toybox Handover task, N2M provides reliable predictions even in unseen environments with only 15 data samples, showing remarkable data efficiency and generalizability."
  },
  {
    "filename": "2509.18674v1.txt",
    "text": "A scalable Bayesian machine learning framework is introduced for estimating scalar properties of an unknown quantum state from measurement data, which bypasses full density matrix reconstruction. This work is the first to integrate the classical shadows protocol with a permutation-invariant set transformer architecture, enabling the approach to predict and correct bias in existing estimators to approximate the true Bayesian posterior mean. Measurement outcomes are encoded as fixed-dimensional feature vectors, and the network outputs a residual correction to a baseline estimator. Scalability to large quantum systems is ensured by the polynomial dependence of input size on system size and number of measurements. On Greenberger-Horne-Zeilinger state fidelity and second-order R\\'enyi entropy estimation tasks -- using random Pauli and random Clifford measurements -- this Bayesian estimator always achieves lower mean squared error than classical shadows alone, with more than a 99\\% reduction in the few copy regime."
  },
  {
    "filename": "2509.18676v1.txt",
    "text": "Learning robust visuomotor policies that generalize across diverse objects and interaction dynamics remains a central challenge in robotic manipulation. Most existing approaches rely on direct observation-to-action mappings or compress perceptual inputs into global or object-centric features, which often overlook localized motion cues critical for precise and contact-rich manipulation. We present 3D Flow Diffusion Policy (3D FDP), a novel framework that leverages scene-level 3D flow as a structured intermediate representation to capture fine-grained local motion cues. Our approach predicts the temporal trajectories of sampled query points and conditions action generation on these interaction-aware flows, implemented jointly within a unified diffusion architecture. This design grounds manipulation in localized dynamics while enabling the policy to reason about broader scene-level consequences of actions. Extensive experiments on the MetaWorld benchmark show that 3D FDP achieves state-of-the-art performance across 50 tasks, particularly excelling on medium and hard settings. Beyond simulation, we validate our method on eight real-robot tasks, where it consistently outperforms prior baselines in contact-rich and non-prehensile scenarios. These results highlight 3D flow as a powerful structural prior for learning generalizable visuomotor policies, supporting the development of more robust and versatile robotic manipulation. Robot demonstrations, additional results, and code can be found at https://sites.google.com/view/3dfdp/home."
  },
  {
    "filename": "2509.18681v1.txt",
    "text": "Machine Learning (ML) may offer new capabilities in airborne systems. However, as any piece of airborne systems, ML-based systems will be required to guarantee their safe operation. Thus, their development will have to be demonstrated to be compliant with the adequate guidance. So far, the European Union Aviation Safety Agency (EASA) has published a concept paper and an EUROCAE/SAE group is preparing ED-324. Both approaches delineate high-level objectives to confirm the ML model achieves its intended function and maintains training performance in the target environment. The paper aims to clarify the difference between an ML model and its corresponding unambiguous description, referred to as the Machine Learning Model Description (MLMD). It then refines the essential notion of semantics preservation to ensure the accurate replication of the model. We apply our contributions to several industrial use cases to build and compare several target models."
  },
  {
    "filename": "2509.18684v1.txt",
    "text": "Efficient memory access patterns play a crucial role in determining the overall performance of applications by exploiting temporal and spatial locality, thus maximizing cache locality. The Reuse Distance Histogram (RDH) is a widely used metric to quantify temporal locality, measuring the distance between consecutive accesses to the same memory location. Traditionally, calculating RDH requires program execution and memory trace collection to obtain dynamic memory access behavior. This trace collection is often time-consuming, resource-intensive, and unsuitable for early-stage optimization or large-scale applications. Static prediction, on the other hand, offers a significant speedup in estimating RDH and cache hit rates. However, these approaches lack accuracy, since the predictions come without running the program and knowing the complete memory access pattern, more specifically when arrays are used inside nested loops. This paper presents a novel static analysis framework for predicting the reuse profiles of array references in programs with nested loop structures, without requiring any runtime information. By analyzing loop bounds, access patterns in smaller problem sizes, and predictive equations, our method predicts access patterns of arrays and estimates reuse distances and cache hit rate at compile time. This paper extends our previous study by incorporating more analysis and improving prediction by addressing previously unhandled reuse patterns. We evaluate our technique against a widely accepted traditional trace-driven profiling tool, Parallel Reuse Distance Analysis (PARDA). The results demonstrate that our static predictor achieves comparable accuracy while offering orders-of-magnitude improvement in the analysis speed. This work offers a practical alternative to dynamic reuse profiling and paves the way for integration into compilers and static performance modeling tools."
  },
  {
    "filename": "2509.18691v1.txt",
    "text": "In recent years, self-supervised learning has amassed significant interest for training deep neural representations without labeled data. One such self-supervised learning approach is masked spectrogram modeling, where the objective is to learn semantically rich contextual representations by predicting removed or hidden portions of the input audio spectrogram. With the Transformer neural architecture at its core, masked spectrogram modeling has emerged as the prominent approach for learning general purpose audio representations, a.k.a. audio foundation models. Meanwhile, addressing the issues of the Transformer architecture, in particular the underlying Scaled Dot-product Attention operation, which scales quadratically with input sequence length, has led to renewed interest in recurrent sequence modeling approaches. Among them, Selective structured state space models (such as Mamba) and extended Long Short-Term Memory (xLSTM) are the two most promising approaches which have experienced widespread adoption. While the body of work on these two topics continues to grow, there is currently a lack of an adequate overview encompassing the intersection of these topics. In this paper, we present a comprehensive overview of the aforementioned research domains, covering masked spectrogram modeling and the previously mentioned neural sequence modeling architectures, Mamba and xLSTM. Further, we compare Transformers, Mamba and xLSTM based masked spectrogram models in a unified, reproducible framework on ten diverse downstream audio classification tasks, which will help interested readers to make informed decisions regarding suitability of the evaluated approaches to adjacent applications."
  },
  {
    "filename": "2509.18703v1.txt",
    "text": "This research focuses on rational pesticide design, using graph machine learning to accelerate the development of safer, eco-friendly agrochemicals, inspired by in silico methods in drug discovery. With an emphasis on ecotoxicology, the initial contributions include the creation of ApisTox, the largest curated dataset on pesticide toxicity to honey bees. We conducted a broad evaluation of machine learning (ML) models for molecular graph classification, including molecular fingerprints, graph kernels, GNNs, and pretrained transformers. The results show that methods successful in medicinal chemistry often fail to generalize to agrochemicals, underscoring the need for domain-specific models and benchmarks. Future work will focus on developing a comprehensive benchmarking suite and designing ML models tailored to the unique challenges of pesticide discovery."
  },
  {
    "filename": "2509.18720v1.txt",
    "text": "The discovery of VHE emission from the Crab pulsar and, more recently, multi-TeV emission from the Vela pulsar have challenged our current understanding of the emission mechanisms of these sources. Studying pulsar emission at TeV energies allows us to understand the engines that power some of the most extreme accelerators in the Galaxy. We present recent highlights from the VERITAS pulsar program using nearly two decades of VERITAS data and novel high energy analysis techniques optimized for emission up to 100 TeV. This work begins to characterize how the emerging population of multi-TeV pulsars can be predicted from existing multi-wavelength observations. In particular, we highlight a search for VHE emission above 1 TeV using over 17 years of Crab pulsar data, which extends the high energy end of the existing VERITAS spectrum. Additionally, we search for both optical and multi-TeV emission from bright Vela-like pulsars, including analysis of over 200 hours of data on PSR J2229+6114, which powers the Boomerang pulsar wind nebula and is putatively associated with the ultra-high-energy source 1LHAASO J2229+5927u. We discuss these results in the context of the broader pulsar population and their impacts on the prospects of new pulsar discoveries with next-generation VHE instruments."
  },
  {
    "filename": "2509.18735v1.txt",
    "text": "This work introduces 6G Twin, the first end-to-end artificial intelligence (AI)-native radio access network (RAN) design that unifies (i) neural Gaussian Radio Fields (GRF) for compressed channel state information (CSI) acquisition, (ii) continual channel prediction with handover persistence, and (iii) an energy-optimal nonlinear precoder (minPMAC). GRF replaces dense pilots with a sparse Gaussian field, cutting pilot overhead by about 100x while delivering 1.1 ms inference and less than 2 minutes on-site training, thus enabling millisecond-scale closed-loop operation. A replay-driven continual learner sustains accuracy under mobility and cell transitions, improving channel normalized mean square error (NMSE) by more than 10 dB over frozen predictors and an additional 2-5 dB over uniform replay, thereby stabilizing performance across UMi/UMa handovers. Finally, minPMAC solves a convex, order-free MAC precoder design that recovers the globally optimal order from Broadcast Channel (BC) duals and minimizes transmit energy subject to minimum-rate guarantees, achieving 4-10 times lower energy (scenario dependent) with monotonically increasing bits per joule as SNR grows. This translates to up to 5 times higher data rate at comparable power or the same rates at substantially lower power. Together, these components form a practical, GPU-ready framework that attains real-time CSI, robust tracking in dynamic networks with efficient handovers, and state-of-the-art throughput-energy tradeoffs under 3GPP-style settings."
  },
  {
    "filename": "2509.18739v1.txt",
    "text": "This paper studies how insurers can chose which claims to investigate for fraud. Given a prediction model, typically only claims with the highest predicted propability of being fraudulent are investigated. We argue that this can lead to inconsistent learning and propose a randomized alternative. More generally, we draw a parallel with the multi-arm bandit literature and argue that, in the presence of selection, the obtained observations are not iid. Hence, dependence on past observations should be accounted for when updating parameter estimates. We formalize selection in a binary regression framework and show that model updating and maximum-likelihood estimation can be implemented as if claims were investigated at random. Then, we define consistency of selection strategies and conjecture sufficient conditions for consistency. Our simulations suggest that the often-used selection strategy can be inconsistent while the proposed randomized alternative is consistent. Finally, we compare our randomized selection strategy with Thompson sampling, a standard multi-arm bandit heuristic. Our simulations suggest that the latter can be inefficient in learning low fraud probabilities."
  },
  {
    "filename": "2509.18741v1.txt",
    "text": "Machine learning (ML) has achieved remarkable success in climate and marine science. Given that greenhouse warming fundamentally reshapes ocean conditions such as stratification, circulation patterns and eddy activity, evaluating the climate adaptability of the ML model is crucial. While physical constraints have been shown to enhance the performance of ML models, kinetic energy (KE) cascade has not been used as a constraint despite its importance in regulating multi-scale ocean motions. Here we develop two sea surface height (SSH) prediction models (with and without KE cascade constraint) and quantify their climate adaptability at the Kuroshio Extension. Our results demonstrate that both models exhibit only slight performance degradation under greenhouse warming conditions. Incorporating the KE cascade as a physical constraint significantly improve the model performance, reducing eddy kinetic energy errors by 14.7% in the present climate and 15.9% under greenhouse warming. This work presents the first application of the kinetic energy (KE) cascade as a physical constraint for ML based ocean state prediction and demonstrates its robust adaptability across climates, offering guidance for the further development of global ML models for both present and future conditions."
  },
  {
    "filename": "2509.18760v1.txt",
    "text": "Robots must satisfy safety-critical state and input constraints despite disturbances and model mismatch. We introduce a robust model predictive control (RMPC) formulation that is fast, scalable, and compatible with real-time implementation. Our formulation guarantees robust constraint satisfaction, input-to-state stability (ISS) and recursive feasibility. The key idea is to decompose the uncertain nonlinear system into (i) a nominal nonlinear dynamic model, (ii) disturbance-feedback controllers, and (iii) bounds on the model error. These components are optimized jointly using sequential convex programming. The resulting convex subproblems are solved efficiently using a recent disturbance-feedback MPC solver. The approach is validated across multiple dynamics, including a rocket-landing problem with steerable thrust. An open-source implementation is available at https://github.com/antoineleeman/robust-nonlinear-mpc."
  },
  {
    "filename": "2509.18769v1.txt",
    "text": "This paper proposes the integration of Concentrated Solar Power Plant (CSP) in the Renewable-only virtual power plant (RVPP) for bidding in the electricity day-ahead and secondary reserve markets, as well as trading thermal energy through a heat purchase agreement. A reformulated two-stage robust optimization approach is introduced to account for multiple uncertainties, including electricity prices, non-dispatchable renewable energy sources electrical production, CSP thermal production, and uncertainties in electrical and thermal demand consumption. The provision of energy and reserve by the thermal storage of CSP is modeled using an adjustable approach, which allocates a share of energy for up and down reserves based on the profitability of the RVPP. Simulations are conducted for several case studies to demonstrate the effectiveness and computational efficiency of the proposed approach under different RVPP operator decisions against uncertain parameters and various trading strategies for electricity and thermal energy. The simulation results show that integrating CSP into RVPP enhances RVPP flexibility for both electrical and thermal trading. Furthermore, the results indicate that the profitability of the RVPP increases when all trading options are considered, across different levels of conservatism adopted by the RVPP operator in response to uncertain parameters."
  },
  {
    "filename": "2509.18782v1.txt",
    "text": "Power-law distributions are widely recognized in complex systems physics as indicative of underlying complexity in interaction networks and critical macroscopic behavior. Previous studies, notably those of Newman and others, have emphasized the importance of network structure and dynamics in understanding the emergence of such statistical patterns and predicting extreme events. In this study, we investigate the emergence of power-law behavior in delay distributions within a multi-level hierarchical network of agents governed by simple priority rules. Using railway systems as a case study, we model the dynamics of high-speed and local trains agents assigned distinct priority levels-operating within a simplified hierarchical network framework. By introducing Laplacian-distributed stochastic fluctuations into scheduled travel times, derived from empirical data, we observe that local trains exhibit a markedly higher incidence of higher delays than high-speed trains. To account for this phenomenon, we propose a queue-based dynamical model, calibrated using Italian railway data, and validate our findings through comparative analysis with both Italian and German datasets. The model accurately reproduces the empirically observed power-law exponent associated with the Italian local train delays. Furthermore, we analyze the influence of operational policies, such as priority assignment and delay compensation thresholds, revealing distinct cut-offs in delay distributions at 30 and 60 minutes for high-speed and local trains, respectively-corresponding to refund eligibility criteria in Italy. Such cut-offs are absent in the German case, where no comparable priority-change policies are in effect. These results underscore the capacity of simple hierarchical structures and rule-based dynamics to generate complex statistical behaviors without necessitating intricate interaction networks."
  },
  {
    "filename": "2509.18786v1.txt",
    "text": "In this paper, we address the point cloud registration problem, where well-known methods like ICP fail under uncertainty arising from sensor noise, pose-estimation errors, and partial overlap due to occlusion. We develop a novel approach, Gaussian Process Concept Attribution (GP-CA), which not only quantifies registration uncertainty but also explains it by attributing uncertainty to well-known sources of errors in registration problems. Our approach leverages active learning to discover new uncertainty sources in the wild by querying informative instances. We validate GP-CA on three publicly available datasets and in our real-world robot experiment. Extensive ablations substantiate our design choices. Our approach outperforms other state-of-the-art methods in terms of runtime, high sample-efficiency with active learning, and high accuracy. Our real-world experiment clearly demonstrates its applicability. Our video also demonstrates that GP-CA enables effective failure-recovery behaviors, yielding more robust robotic perception."
  },
  {
    "filename": "2509.18790v1.txt",
    "text": "Infrastructure as Code (IaC) automates the provisioning and management of IT infrastructure through scripts and tools, streamlining software deployment. Prior studies have shown that IaC scripts often contain recurring security misconfigurations, and several detection and mitigation approaches have been proposed. Most of these rely on static analysis, using statistical code representations or Machine Learning (ML) classifiers to distinguish insecure configurations from safe code.   In this work, we introduce a novel approach that enhances static analysis with semantic understanding by jointly leveraging natural language and code representations. Our method builds on two complementary ML models: CodeBERT, to capture semantics across code and text, and LongFormer, to represent long IaC scripts without losing contextual information. We evaluate our approach on misconfiguration datasets from two widely used IaC tools, Ansible and Puppet. To validate its effectiveness, we conduct two ablation studies (removing code text from the natural language input and truncating scripts to reduce context) and compare against four large language models (LLMs) and prior work. Results show that semantic enrichment substantially improves detection, raising precision and recall from 0.46 and 0.79 to 0.92 and 0.88 on Ansible, and from 0.55 and 0.97 to 0.87 and 0.75 on Puppet, respectively."
  },
  {
    "filename": "2509.18806v1.txt",
    "text": "Time-frequency (T-F) domain-based neural vocoders have shown promising results in synthesizing high-fidelity audio. Nevertheless, it remains unclear on the mechanism of effectively predicting magnitude and phase targets jointly. In this paper, we start from two representative T-F domain vocoders, namely Vocos and APNet2, which belong to the single-stream and dual-stream modes for magnitude and phase estimation, respectively. When evaluating their performance on a large-scale dataset, we accidentally observe severe performance collapse of APNet2. To stabilize its performance, in this paper, we introduce three simple yet effective strategies, each targeting the topological space, the source space, and the output space, respectively. Specifically, we modify the architectural topology for better information exchange in the topological space, introduce prior knowledge to facilitate the generation process in the source space, and optimize the backpropagation process for parameter updates with an improved output format in the output space. Experimental results demonstrate that our proposed method effectively facilitates the joint estimation of magnitude and phase in APNet2, thus bridging the performance disparities between the single-stream and dual-stream vocoders."
  },
  {
    "filename": "2509.18810v1.txt",
    "text": "Deep neural networks has been increasingly applied in fault diagnostics, where it uses historical data   to capture systems behavior, bypassing the need for high-fidelity physical models.   However, despite their competence in prediction tasks, these models often struggle with   the evaluation of their confidence. This matter is particularly   important in consistency-based diagnosis where decision logic is highly sensitive to false alarms.   To address this challenge, this work presents a diagnostic framework that uses   ensemble probabilistic machine learning to   improve diagnostic characteristics of data driven consistency based diagnosis   by quantifying and automating the prediction uncertainty.   The proposed method is evaluated across several case studies using both ablation   and comparative analyses, showing consistent improvements across a range of diagnostic metrics."
  },
  {
    "filename": "2509.18812v1.txt",
    "text": "We examine the central tenet of the current standard theory of neutrino oscillations, namely the assumption that neutrinos are emitted and detected as flavour neutrino states, which are coherent superpositions of massive neutrino states of different masses. We prove that all the quantum mechanical and quantum field theoretical arguments, including the invocation of the uncertainty principle and the wave packet description of massive neutrinos, entail the production of neutrinos as statistical ensembles of massive neutrino states. As the states in a statistical ensemble do not interfere, neutrino oscillations cannot be explained by the superposition of massive states. We point out that neutrino oscillations in vacuum can be consistently formulated in theories which include, among other assumptions, the premise that the asymptotic states are massless flavour neutrinos."
  },
  {
    "filename": "2509.18817v1.txt",
    "text": "The $pp \\to W^{\\pm} (\\to \\mu^{\\pm} \\nu_{\\mu}) X$ cross-sections are measured at a proton-proton centre-of-mass energy $\\sqrt{s} = 5.02$ TeV using a dataset corresponding to an integrated luminosity of 100 pb$^{-1}$ recorded by the LHCb experiment. Considering muons in the pseudorapidity range $2.2 < \\eta < 4.4$, the cross-sections are measured differentially in twelve intervals of muon transverse momentum between $28 < p_\\mathrm{T} < 52$ GeV. Integrated over $p_\\mathrm{T}$, the measured cross-sections are \\begin{align*} \\sigma_{W^+ \\to \\mu^+ \\nu_\\mu} &= 300.9 \\pm 2.4 \\pm 3.8 \\pm 6.0~\\text{pb}, \\\\ \\sigma_{W^- \\to \\mu^- \\bar{\\nu}_\\mu} &= 236.9 \\pm 2.1 \\pm 2.7 \\pm 4.7~\\text{pb}, \\end{align*} where the first uncertainties are statistical, the second are systematic, and the third are associated with the luminosity calibration. These integrated results are consistent with theoretical predictions.   This analysis introduces a new method to determine the $W$-boson mass using the measured differential cross-sections corrected for detector effects. The measurement is performed on this statistically limited dataset as a proof of principle and yields \\begin{align*} m_W = 80369 \\pm 130 \\pm 33~\\text{MeV}, \\end{align*} where the first uncertainty is experimental and the second is theoretical."
  },
  {
    "filename": "2509.18824v1.txt",
    "text": "Unified multimodal models have recently attracted considerable attention for their remarkable abilities in jointly understanding and generating diverse content. However, as contexts integrate increasingly numerous interleaved multimodal tokens, the iterative processes of diffusion denoising and autoregressive decoding impose significant computational overhead. To address this, we propose Hyper-Bagel, a unified acceleration framework designed to simultaneously speed up both multimodal understanding and generation tasks. Our approach uses a divide-and-conquer strategy, employing speculative decoding for next-token prediction and a multi-stage distillation process for diffusion denoising. The framework delivers substantial performance gains, achieving over a 2x speedup in multimodal understanding. For generative tasks, our resulting lossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a 22x speedup in image editing, all while preserving the high-quality output of the original model. We further develop a highly efficient 1-NFE model that enables near real-time interactive editing and generation. By combining advanced adversarial distillation with human feedback learning, this model achieves ultimate cost-effectiveness and responsiveness, making complex multimodal interactions seamless and instantaneous."
  },
  {
    "filename": "2509.18837v1.txt",
    "text": "Volatility is the canonical measure of financial risk, a role largely inherited from Modern Portfolio Theory. Yet, its universality rests on restrictive efficiency assumptions that render volatility, at best, an incomplete proxy for true risk. This paper identifies three fundamental inconsistencies: (i) volatility is path-independent and blind to temporal dependence and non-stationarity; (ii) its relevance collapses in derivative-intensive strategies, where volatility often represents opportunity rather than risk; and (iii) it lacks an absolute benchmark, providing no guidance on what level of volatility is economically ``fair'' in efficient markets. To address these limitations, we propose a new paradigm that reconceptualizes risk in terms of predictability rather than variability. Building on a general class of stochastic processes, we derive an analytical link between volatility and the Hurst-Holder exponent within the Multifractional Process with Random Exponent (MPRE) framework. This relationship yields a formal definition of ``fair volatility'', namely the volatility implied under market efficiency, where prices follow semi-martingale dynamics. Extensive empirical analysis on global equity indices supports this framework, showing that deviations from fair volatility provide a tractable measure of market inefficiency, distinguishing between momentum-driven and mean-reverting regimes. Our results advance both the theoretical foundations and empirical assessment of financial risk, offering a definition of volatility that is efficiency-consistent and economically interpretable."
  },
  {
    "filename": "2509.18841v1.txt",
    "text": "The hydrogen (H2) fueled direct-injection (DI) compression-ignition (CI) argon power cycle (APC) is an attractive technology to counteract the mismatch between energy demand and supply from renewable energy sources. The development of the APC, as well as air-breathing DI CI H2 engines, can be advanced by computational fluid dynamics (CFD) models. Reynolds-averaged Navier-Stokes (RANS) combustion models are an effective approach to predict global in-cylinder variables such pressure and heat release rate. However, validity of this method to simulate the complex phenomena associated with high-pressure, auto-igniting hydrogen jets is not ensured due to the underlying model assumptions. In this study, a RANS CFD environment using two commonly used eddy-viscosity models and the Reynolds stress model (RSM) is extensively validated. Combustion is modeled with a detailed chemistry model. First, hydrogen distributions in non-reacting jets are compared against literature data to assess the accuracy of the models on turbulent mixing at high pressure. Subsequently, the capability of the model to simulate auto-igniting jets is assessed by comparison with reported measurement data of closed-vessel experiments at high pressure. Ignition delay times, pressure rise profiles, and heat release rate profiles are compared for different ambient temperature and oxygen concentration. Adequate agreement is found for the diffusive combustion phase for all models, despite the lack of turbulence-chemistry interaction in the combustion model. Trends with ambient temperature and oxygen concentration were well predicted and the best agreement is found for the RSM."
  },
  {
    "filename": "2509.18846v1.txt",
    "text": "Accurate International Classification of Diseases (ICD) coding is critical for clinical documentation, billing, and healthcare analytics, yet it remains a labour-intensive and error-prone task. Although large language models (LLMs) show promise in automating ICD coding, their challenges in base model selection, input contextualization, and training data redundancy limit their effectiveness. We propose a modular framework for ICD-10 Clinical Modification (ICD-10-CM) code prediction that addresses these challenges through principled model selection, redundancy-aware data sampling, and structured input design. The framework integrates an LLM-as-judge evaluation protocol with Plackett-Luce aggregation to assess and rank open-source LLMs based on their intrinsic comprehension of ICD-10-CM code definitions. We introduced embedding-based similarity measures, a redundancy-aware sampling strategy to remove semantically duplicated discharge summaries. We leverage structured discharge summaries from Taiwanese hospitals to evaluate contextual effects and examine section-wise content inclusion under universal and section-specific modelling paradigms. Experiments across two institutional datasets demonstrate that the selected base model after fine-tuning consistently outperforms baseline LLMs in internal and external evaluations. Incorporating more clinical sections consistently improves prediction performance. This study uses open-source LLMs to establish a practical and principled approach to ICD-10-CM code prediction. The proposed framework provides a scalable, institution-ready solution for real-world deployment of automated medical coding systems by combining informed model selection, efficient data refinement, and context-aware prompting."
  },
  {
    "filename": "2509.18850v1.txt",
    "text": "Astrophysical jets from powerful active galactic nuclei (AGN) have recently been proposed as promising probes of dark matter (DM) in the sub-GeV mass range. AGN launch relativistic jets that accelerate cosmic rays (CRs) to very high energies, which can then interact with their surroundings and produce multiwavelength (MW) emission spanning from radio frequencies to TeV $\\gamma$ rays. If DM consists of light particles, their interactions with CRs could lead to an additional cooling mechanism that modifies the expected MW emission. In this work, we analyse the MW spectrum of Markarian 421, a well-studied AGN, using a multizone leptonic jet model that includes the interactions between CR electrons and DM particles. For the first time, we account for the uncertainties in the astrophysical jet dynamics, which have been previously neglected when constraining the CR-DM interactions. By fitting simultaneously jet parameters and DM-electrons interactions, we use the MW data from Markarian 421 to set constraints on the DM-induced CR cooling. We obtain 5$\\sigma$ upper limit $\\sigma_\\text{DM-e} \\lesssim 1 \\times 10^{-34}~\\text{cm}^2$ for a DM mass of $1~{\\rm MeV}$. We demonstrate that this is about a factor of five weaker than traditional approaches, implying that properly accounting for degeneracies between jet dynamics and DM interactions is key to derive robust constraints on DM interactions."
  },
  {
    "filename": "2509.18853v1.txt",
    "text": "This paper proposes an orthogonality-constrained modal search (OCMS) method for estimating modal wavenumbers and modal depth functions using a vertical linear array (VLA). Under the assumption of a known sound speed profile, OCMS leverages the orthogonality of distinct modal depth functions to extract both the modal depth functions and their corresponding wavenumbers, even when the VLA and a monochromatic sound source remain stationary.The performance of OCMS is evaluated through numerical simulations under varying signal-to-noise ratios (SNRs), different VLA apertures, varying numbers of VLA elements, VLA tilt and sound speed profile (SSP) uncertainty. The results demonstrate that OCMS is robust against noise, VLA aperture variations, and changes in the number of VLA elements, meanwhile, the algorithm maintains reliable performance when SSP uncertainty < 1 m/s and VLA tilt angle <5{\\deg}. Furthermore, the effectiveness of OCMS is validated using SwellEx96 experimental data. The relative error between the modal wavenumbers derived from experimental data and those computed via Kraken is on the order of $10^{-4}$."
  },
  {
    "filename": "2509.18868v1.txt",
    "text": "Under a unified operational definition, we define LLM memory as a persistent state written during pretraining, finetuning, or inference that can later be addressed and that stably influences outputs. We propose a four-part taxonomy (parametric, contextual, external, procedural/episodic) and a memory quadruple (location, persistence, write/access path, controllability). We link mechanism, evaluation, and governance via the chain write -> read -> inhibit/update. To avoid distorted comparisons across heterogeneous setups, we adopt a three-setting protocol (parametric only, offline retrieval, online retrieval) that decouples capability from information availability on the same data and timeline. On this basis we build a layered evaluation: parametric (closed-book recall, edit differential, memorization/privacy), contextual (position curves and the mid-sequence drop), external (answer correctness vs snippet attribution/faithfulness), and procedural/episodic (cross-session consistency and timeline replay, E MARS+). The framework integrates temporal governance and leakage auditing (freshness hits, outdated answers, refusal slices) and uncertainty reporting via inter-rater agreement plus paired tests with multiple-comparison correction. For updating and forgetting, we present DMM Gov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC), and RAG to form an auditable loop covering admission thresholds, rollout, monitoring, rollback, and change audits, with specs for timeliness, conflict handling, and long-horizon consistency. Finally, we give four testable propositions: minimum identifiability; a minimal evaluation card; causally constrained editing with verifiable forgetting; and when retrieval with small-window replay outperforms ultra-long-context reading. This yields a reproducible, comparable, and governable coordinate system for research and deployment."
  },
  {
    "filename": "2509.18869v1.txt",
    "text": "Retrieval-Augmented Generation (RAG) is increasingly employed in generative AI-driven scientific workflows to integrate rapidly evolving scientific knowledge bases, yet its reliability is frequently compromised by non-determinism in their retrieval components. This paper introduces ReproRAG, a comprehensive benchmarking framework designed to systematically measure and quantify the reproducibility of vector-based retrieval systems. ReproRAG investigates sources of uncertainty across the entire pipeline, including different embedding models, precision, retrieval algorithms, hardware configurations, and distributed execution environments. Utilizing a suite of metrics, such as Exact Match Rate, Jaccard Similarity, and Kendall's Tau, the proposed framework effectively characterizes the trade-offs between reproducibility and performance. Our large-scale empirical study reveals critical insights; for instance, we observe that different embedding models have remarkable impact on RAG reproducibility. The open-sourced ReproRAG framework provides researchers and engineers productive tools to validate deployments, benchmark reproducibility, and make informed design decisions, thereby fostering more trustworthy AI for science."
  },
  {
    "filename": "2509.18875v1.txt",
    "text": "Mixture cure models are widely used in survival analysis when a portion of patients is considered cured and is no longer at risk for the event of interest. In clinical settings, dynamic survival prediction is particularly important to refine prognosis by incorporating updated patient information over time. Landmarking methods have emerged as a flexible approach for this purpose, as they allow to summarize longitudinal covariates up to a given landmark time and to use these summaries in subsequent prediction. For mixture cure models, the only landmarking strategy available in the literature relies on the last observation carried forward (LOCF) method to summarize longitudinal dynamics up to the landmark time. However, LOCF discards most of the longitudinal information, does not correct for measurement error, and may rely on outdated values if observation times are far apart. To overcome these limitations, we propose a sequential approach that integrates model-based landmarking within a mixture cure model. Initially, longitudinal covariates are modeled using (generalized) linear mixed models, from which individual-specific random effects are predicted. The predicted random effects are then incorporated as covariates into a Cox proportional hazards cure model. We investigated the performance of the proposed approach under different cure fractions, sample sizes, and longitudinal data structures through an extensive simulation study. The results show that the model-based strategy provides more refined predictions compared to LOCF, even when the model is misspecified in favour of the LOCF approach. Finally, we illustrate our method using a real-world dataset on renal transplant patients."
  },
  {
    "filename": "2509.18885v1.txt",
    "text": "Speech enhancement (SE) performance is known to depend on noise characteristics and signal to noise ratio (SNR), yet intrinsic properties of the clean speech signal itself remain an underexplored factor. In this work, we systematically analyze how clean speech characteristics influence enhancement difficulty across multiple state of the art SE models, languages, and noise conditions. We extract a set of pitch, formant, loudness, and spectral flux features from clean speech and compute correlations with objective SE metrics, including frequency weighted segmental SNR and PESQ. Our results show that formant amplitudes are consistently predictive of SE performance, with higher and more stable formants leading to larger enhancement gains. We further demonstrate that performance varies substantially even within a single speaker's utterances, highlighting the importance of intraspeaker acoustic variability. These findings provide new insights into SE challenges, suggesting that intrinsic speech characteristics should be considered when designing datasets, evaluation protocols, and enhancement models."
  },
  {
    "filename": "2509.18890v1.txt",
    "text": "State of the art speech enhancement (SE) models achieve strong performance on neurotypical speech, but their effectiveness is substantially reduced for pathological speech. In this paper, we investigate strategies to address this gap for both predictive and generative SE models, including i) training models from scratch using pathological data, ii) finetuning models pretrained on neurotypical speech with additional data from pathological speakers, and iii) speaker specific personalization using only data from the individual pathological test speaker. Our results show that, despite the limited size of pathological speech datasets, SE models can be successfully trained or finetuned on such data. Finetuning models with data from several pathological speakers yields the largest performance improvements, while speaker specific personalization is less effective, likely due to the small amount of data available per speaker. These findings highlight the challenges and potential strategies for improving SE performance for pathological speakers."
  },
  {
    "filename": "2509.18893v1.txt",
    "text": "While heterophily has been widely studied in node-level tasks, its impact on graph-level tasks remains unclear. We present the first analysis of heterophily in graph-level learning, combining theoretical insights with empirical validation. We first introduce a taxonomy of graph-level labeling schemes, and focus on motif-based tasks within local structure labeling, which is a popular labeling scheme. Using energy-based gradient flow analysis, we reveal a key insight: unlike frequency-dominated regimes in node-level tasks, motif detection requires mixed-frequency dynamics to remain flexible across multiple spectral components. Our theory shows that motif objectives are inherently misaligned with global frequency dominance, demanding distinct architectural considerations. Experiments on synthetic datasets with controlled heterophily and real-world molecular property prediction support our findings, showing that frequency-adaptive model outperform frequency-dominated models. This work establishes a new theoretical understanding of heterophily in graph-level learning and offers guidance for designing effective GNN architectures."
  },
  {
    "filename": "2509.18907v1.txt",
    "text": "Depleted carbonate hydrocarbon reservoirs are promising sites for geological CO2 storage, yet the presence of residual hydrocarbons introduces complex pore-scale interactions that influence the dynamics of solid dissolution. This study reveals how residual oil affects dissolution patterns and effective reaction rates during CO2-acidified brine injection into Ketton limestone under reservoir conditions. We combine time-resolved X-ray microtomography (micro-CT), core-flooding experiments, and direct numerical simulations to assess the impact of pore space heterogeneity, oil distribution and injection rate. We find that the coupling between pore structure, residual oil saturation and oil displacement control flow heterogeneity, reactive surface accessibility, dissolution patterns and the reaction rates. At low injection rate, dissolution by channel widening is enhanced by oil displacement. This mechanism is especially important when dissolution is suppressed by heterogeneity in the pore space and the residual oil. At high injection rates, a more uniform dissolution occurs and can be enhanced by re-mobilisation of oil blocking brine flow. Effective reaction rates in two-phase flow are found to be lower than in the the equivalent single-phase case and up to two orders of magnitude lower than the batch rates due to persistent transport limitations. These findings offer mechanistic insights into multiphase reactive transport in carbonates and highlight the need for accurate understanding of the impact of the hydrocarbon phase on reaction to improve predictions of CO2 storage efficiency."
  },
  {
    "filename": "2509.18923v1.txt",
    "text": "Recently, symmetry-broken ground states, such as correlated insulating states, magnetic order and superconductivity, have been discovered in twisted bilayer graphene (tBLG) and twisted trilayer graphene (tTLG) near the so-called magic-angle. Understanding the magnetic order in these systems is challenging, as atomistic methods become extremely expensive near the magic angle and continuum approaches fail to capture important atomistic details. In this work, we develop a self-consistent approach based on a continuum model that incorporates both short-ranged Hubbard interactions and long-ranged Coulomb interactions, therefore allowing efficient exploration of magnetic order in moir\\'e graphene multilayers. With this approach, we perform a systematic analysis of the magnetic phase diagram of tBLG as a function of doping level and twist angle, near the magic angle. We find that the results are consistent with previous perturbative atomistic Hartree+U calculations. Furthermore, we predict stable magnetic orders for the tTLG. We found that the magnetic orders are similar to those in tBLG for small values of on-site repulsion. In the future, the developed method can be utilized to investigate magnetic ordering tendencies from short-range exchange interactions in other moir\\'e graphene multilayers as a function of doping, twist angle, screening environment, among other variables."
  },
  {
    "filename": "2509.18928v1.txt",
    "text": "Autoregressive diffusion models (ARDMs) have recently been applied to speech generation, achieving state-of-the-art (SOTA) performance in zero-shot text-to-speech. By autoregressively generating continuous speech tokens with next-token diffusion, these models offer a promising alternative to next-token prediction, avoiding the technical complexities associated with discrete speech tokenization. As a relatively new paradigm, research on reinforcement learning (RL)-based fine-tuning of speech ARDMs remains limited. In this paper, we propose Autoregressive Diffusion-Direct Preference Optimization (ARDM-DPO) to advance this research. By fine-tuning the recently proposed zero-shot text-to-speech model DiTAR with DPO, we achieve significant improvements in terms of speech expressiveness and robustness for long texts."
  },
  {
    "filename": "2509.18933v1.txt",
    "text": "Wireless communications are characterized by their unpredictability, posing challenges for maintaining consistent communication quality. This paper presents a comprehensive analysis of various prediction models, with a focus on achieving accurate and efficient Wi-Fi link quality forecasts using machine learning techniques. Specifically, the paper evaluates the performance of data-driven models based on the linear combination of exponential moving averages, which are designed for low-complexity implementations and are then suitable for hardware platforms with limited processing resources. Accuracy of the proposed approaches was assessed using experimental data from a real-world Wi-Fi testbed, considering both channel-dependent and channel-independent training data. Remarkably, channel-independent models, which allow for generalized training by equipment manufacturers, demonstrated competitive performance. Overall, this study provides insights into the practical deployment of machine learning-based prediction models for enhancing Wi-Fi dependability in industrial environments."
  },
  {
    "filename": "2509.18934v1.txt",
    "text": "Adversarial smart contracts, mostly on EVM-compatible chains like Ethereum and BSC, are deployed as EVM bytecode to exploit vulnerable smart contracts typically for financial gains. Detecting such malicious contracts at the time of deployment is an important proactive strategy preventing loss from victim contracts. It offers a better cost-benefit than detecting vulnerabilities on diverse potential victims. However, existing works are not generic with limited detection types and effectiveness due to imbalanced samples, while the emerging LLM technologies, which show its potentials in generalization, have two key problems impeding its application in this task: hard digestion of compiled-code inputs, especially those with task-specific logic, and hard assessment of LLMs' certainty in their binary answers, i.e., yes-or-no answers. Therefore, we propose a generic adversarial smart contracts detection framework FinDet, which leverages LLMs with two enhancements addressing above two problems. FinDet takes as input only the EVM-bytecode contracts and identifies adversarial ones among them with high balanced accuracy. The first enhancement extracts concise semantic intentions and high-level behavioral logic from the low-level bytecode inputs, unleashing the LLM reasoning capability restricted by the task input. The second enhancement probes and measures the LLM uncertainty to its multi-round answering to the same query, improving the LLM answering robustness for binary classifications required by the task output. Our comprehensive evaluation shows that FinDet achieves a BAC of 0.9223 and a TPR of 0.8950, significantly outperforming existing baselines. It remains robust under challenging conditions including unseen attack patterns, low-data settings, and feature obfuscation. FinDet detects all 5 public and 20+ unreported adversarial contracts in a 10-day real-world test, confirmed manually."
  },
  {
    "filename": "2509.18943v1.txt",
    "text": "Studying critical states in quasiperiodic systems is of great importance in localization physics. Previously identified critical states share a common characteristic: they exhibit persistent critical features in the thermodynamic limit. In this Letter, we predict an exotic type of critical state, termed size-dependent critical states, which exhibit a fundamentally distinct behavior. Specifically, they display critical localization signatures only at finite sizes, but transition to Anderson localization in the thermodynamic limit. We establish that the physical origin of size-dependent critical states lies in the synergistic interplay between local non-reciprocal domain wall and NHSE. By revealing a critical phase that challenges the established paradigm of critical localization, our work opens new avenues for exploring localization phenomena in quasiperiodic systems."
  },
  {
    "filename": "2509.18951v1.txt",
    "text": "Quantitative structure-activity relationship (QSAR) modelling is widely employed in materials sci- ence to predict properties of interest and extract useful descriptors for measured properties. In thermal barrier coatings (TBC), QSAR can significantly shorten the experimental discovery cycle, which can take years. Although machine learning methods are commonly employed for QSAR, their performance depends on the data quality and how instances are represented. Traditional, hand-crafted descriptors based on known material properties are limited to represent materials that share the same basic crystal structure, limited the size of the dataset. By contrast, graph neural networks offer a more expressive representation, encoding atomic positions and bonds in the crystal lattice. In this study, we compare Random Forest (RF) and Gaussian Process (GP) models trained on hand-crafted descriptors from the literature with graph-based representations for high-entropy, rare-earth pyrochlore oxides using the Crystal Graph Convolutional Neural Network (CGCNN). Two different types of augmentation methods are also explored to account for the limited data size, one of which is only applicable to graph-based representations. Our findings show that the CGCNN model substantially outperforms the RF and GP models, underscoring the potential of graph-based representations for enhanced QSAR modelling in TBC research."
  },
  {
    "filename": "2509.18954v1.txt",
    "text": "LiDAR-based localization and SLAM often rely on iterative matching algorithms, particularly the Iterative Closest Point (ICP) algorithm, to align sensor data with pre-existing maps or previous scans. However, ICP is prone to errors in featureless environments and dynamic scenes, leading to inaccurate pose estimation. Accurately predicting the uncertainty associated with ICP is crucial for robust state estimation but remains challenging, as existing approaches often rely on handcrafted models or simplified assumptions. Moreover, a few deep learning-based methods for localizability estimation either depend on a pre-built map, which may not always be available, or provide a binary classification of localizable versus non-localizable, which fails to properly model uncertainty. In this work, we propose a data-driven framework that leverages deep learning to estimate the registration error covariance of ICP before matching, even in the absence of a reference map. By associating each LiDAR scan with a reliable 6-DoF error covariance estimate, our method enables seamless integration of ICP within Kalman filtering, enhancing localization accuracy and robustness. Extensive experiments on the KITTI dataset demonstrate the effectiveness of our approach, showing that it accurately predicts covariance and, when applied to localization using a pre-built map or SLAM, reduces localization errors and improves robustness."
  },
  {
    "filename": "2509.18962v1.txt",
    "text": "Ensemble methods for stream mining necessitate managing multiple models and updating them as data distributions evolve. Considering the calls for more sustainability, established methods are however not sufficiently considerate of ensemble members' computational expenses and instead overly focus on predictive capabilities. To address these challenges and enable green online learning, we propose heterogeneous online ensembles (HEROS). For every training step, HEROS chooses a subset of models from a pool of models initialized with diverse hyperparameter choices under resource constraints to train. We introduce a Markov decision process to theoretically capture the trade-offs between predictive performance and sustainability constraints. Based on this framework, we present different policies for choosing which models to train on incoming data. Most notably, we propose the novel $\\zeta$-policy, which focuses on training near-optimal models at reduced costs. Using a stochastic model, we theoretically prove that our $\\zeta$-policy achieves near optimal performance while using fewer resources compared to the best performing policy. In our experiments across 11 benchmark datasets, we find empiric evidence that our $\\zeta$-policy is a strong contribution to the state-of-the-art, demonstrating highly accurate performance, in some cases even outperforming competitors, and simultaneously being much more resource-friendly."
  },
  {
    "filename": "2509.18966v1.txt",
    "text": "I study the renormalization of D-dimensional level-k Wess-Zumino-Witten theory with Stiefel-manifold target space $\\mathrm{St}_{N,N-D-1} \\cong \\mathrm{SO}(N)/\\mathrm{SO}(D+1)$, with a particular focus on $D = 3$. I investigate in particular whether such a theory admits IR-stable fixed points of the renormalization group flow. Such fixed points have been suggested to describe conformal phases of matter that do not have a known dual (super-)renormalizable Lagrangian for $N \\geq 7$ in $D = 3$. They are hence of interest both from the point of view of quantum phases of matter as well as pure field theory. The $D$-dimensional expressions enable the computation, by analytic computation, of beta functions in $D = 2 + \\epsilon$, at least to first non-trivial order. In $D = 2$, a stable fixed point is found, serving a generalization of the famed $\\mathrm{SU}(2)_k$ Wess-Zumino-Witten conformal field theory; it annihilates in $D = 2 + \\epsilon$ with an unstable fixed point which splits off from the Gaussian one for $\\epsilon > 0$. Although the story is thus qualitatively similar to that of SO(5) deconfined (pseudo-)criticality, for $N \\geqslant 6$, the annihilation appears to occur only for $\\epsilon > 1$, suggesting the existence of a stable phase in $D = 3$. Comparisons of the scaling dimension of the lowest singlet operator are made with known results for $N = 6$, which is dual to QED$_3$ with $N_\\mathrm{f} = 4$ fermion flavors. The predictions for the $N = 7$ Stiefel liquid represent to my knowledge the first computation of this kind for a Wess-Zumino-Witten theory without a known gauge theory dual."
  },
  {
    "filename": "2509.18971v1.txt",
    "text": "The gravitational collapse of collisionless matter leads to shell-crossing singularities that challenge the applicability of standard perturbation theory. Here, we present the first fully perturbative approach in three dimensions by using Lagrangian coordinates that asymptotically captures the highly nonlinear nature of matter evolution after the first shell-crossing. This is made possible essentially thanks to two basic ingredients: (1) We employ high-order standard Lagrangian perturbation theory to evolve the system until shell-crossing, and (2) we exploit the fact that the density caustic structure near the first shell-crossing begins generically with pancake formation. The latter property allows us to exploit largely known one-dimensional results to determine perturbatively the gravitational backreaction after collapse, yielding accurate solutions within our post-collapse perturbation theory (PCPT) formalism. We validate the PCPT predictions against high-resolution Vlasov-Poisson simulations and demonstrate that PCPT provides a robust framework for describing the early stages of post-collapse dynamics."
  },
  {
    "filename": "2509.18980v1.txt",
    "text": "We investigate whether large language models (LLMs) can generate effective, user-facing explanations from a mathematically interpretable recommendation model. The model is based on constrained matrix factorization, where user types are explicitly represented and predicted item scores share the same scale as observed ratings, making the model's internal representations and predicted scores directly interpretable. This structure is translated into natural language explanations using carefully designed LLM prompts. Many works in explainable AI rely on automatic evaluation metrics, which often fail to capture users' actual needs and perceptions. In contrast, we adopt a user-centered approach: we conduct a study with 326 participants who assessed the quality of the explanations across five key dimensions-transparency, effectiveness, persuasion, trust, and satisfaction-as well as the recommendations themselves.To evaluate how different explanation strategies are perceived, we generate multiple explanation types from the same underlying model, varying the input information provided to the LLM. Our analysis reveals that all explanation types are generally well received, with moderate statistical differences between strategies. User comments further underscore how participants react to each type of explanation, offering complementary insights beyond the quantitative results."
  },
  {
    "filename": "2509.18986v1.txt",
    "text": "Predictive process monitoring is a sub-domain of process mining which aims to forecast the future of ongoing process executions. One common prediction target is the remaining time, meaning the time that will elapse until a process execution is completed. In this paper, we compare four different remaining time prediction approaches in a real-life outbound warehouse process of a logistics company in the aviation business. For this process, the company provided us with a novel and original event log with 169,523 traces, which we can make publicly available. Unsurprisingly, we find that deep learning models achieve the highest accuracy, but shallow methods like conventional boosting techniques achieve competitive accuracy and require significantly fewer computational resources."
  },
  {
    "filename": "2509.18988v1.txt",
    "text": "This paper considers the problem of adaptively overriding unsafe actions of a nominal controller in the presence of high-relative-degree nonovershooting constraints and parametric uncertainties. To prevent the design from being coupled with high-order derivatives of the parameter estimation error, we adopt a modular design approach in which the controller and the parameter identifier are designed separately. The controller module ensures that any safety violations caused by parametric uncertainties remain bounded, provided that the parameter estimation error and its first-order derivative are either bounded or square-integrable. The identifier module, in turn, guarantees that these requirements on the parameter estimation error are satisfied. Both theoretical analysis and simulation results demonstrate that the closed-loop safety violation is bounded by a tunable function of the initial estimation error. Moreover, as time increases, the parameter estimate converges to the true value, and the amount of safety violation decreases accordingly."
  },
  {
    "filename": "2509.18990v1.txt",
    "text": "Simulation-Grounded Neural Networks (SGNNs) are predictive models trained entirely on synthetic data from mechanistic simulations. They have achieved state-of-the-art performance in domains where real-world labels are limited or unobserved, but lack a formal underpinning.   We present the foundational theory of simulation-grounded learning. We show that SGNNs implement amortized Bayesian inference under a simulation prior and converge to the Bayes-optimal predictor. We derive generalization bounds under model misspecification and prove that SGNNs can learn unobservable scientific quantities that empirical methods provably cannot. We also formalize a novel form of mechanistic interpretability uniquely enabled by SGNNs: by attributing predictions to the simulated mechanisms that generated them, SGNNs yield posterior-consistent, scientifically grounded explanations.   We provide numerical experiments to validate all theoretical predictions. SGNNs recover latent parameters, remain robust under mismatch, and outperform classical tools: in a model selection task, SGNNs achieve half the error of AIC in distinguishing mechanistic dynamics. These results establish SGNNs as a principled and practical framework for scientific prediction in data-limited regimes."
  },
  {
    "filename": "2509.18997v1.txt",
    "text": "Representation learning from unlabeled data has been extensively studied in statistics, data science and signal processing with a rich literature on techniques for dimension reduction, compression, multi-dimensional scaling among others. However, current deep learning models use new principles for unsupervised representation learning that cannot be easily analyzed using classical theories. For example, visual foundation models have found tremendous success using self-supervision or denoising/masked autoencoders, which effectively learn representations from massive amounts of unlabeled data. However, it remains difficult to characterize the representations learned by these models and to explain why they perform well for diverse prediction tasks or show emergent behavior. To answer these questions, one needs to combine mathematical tools from statistics and optimization. This paper provides an overview of recent theoretical advances in representation learning from unlabeled data and mentions our contributions in this direction."
  },
  {
    "filename": "2509.18998v1.txt",
    "text": "Computational models provide crucial insights into complex biological processes such as cancer evolution, but their mechanistic nature often makes them nonlinear and parameter-rich, complicating calibration. We systematically evaluate parameter probability distributions in cell migration models using Bayesian calibration across four complementary strategies: parametric and surrogate models, each with and without explicit model discrepancy. This approach enables joint analysis of parameter uncertainty, predictive performance, and interpretability. Applied to a real data experiment of glioblastoma progression in microfluidic devices, surrogate models achieve higher computational efficiency and predictive accuracy, whereas parametric models yield more reliable parameter estimates due to their mechanistic grounding. Incorporating model discrepancy exposes structural limitations, clarifying where model refinement is necessary. Together, these comparisons offer practical guidance for calibrating and improving computational models of complex biological systems."
  },
  {
    "filename": "2509.18999v1.txt",
    "text": "Background: Mechanical ventilation is life-saving for preterm infants with respiratory distress syndrome but can also contribute to lung injury and long-term morbidity. Protective ventilation strategies are recommended, yet implementation in neonatal intensive care units remains inconsistent, and infants continue to be exposed to injurious ventilator settings. Objective: To develop and validate a cohort of neonatal digital twins, based on mechanistic models of cardiopulmonary physiology calibrated to individual patient data, as a tool for simulating and optimising protective ventilation strategies. Methods: A high-fidelity computational simulator of human cardiopulmonary physiology was adapted to neonatal-specific parameters, including lung compliance, dead space, pulmonary vascular resistance, oxygen consumption, and fetal haemoglobin oxygen affinity. Digital twins were generated using data at 65 time points from 11 preterm neonates receiving volume-controlled ventilation. Model parameters were calibrated to minimise the error between simulated and observed PaO2, PaCO2, and peak inspiratory pressure (PIP). Results: Digital twins reproduced measured data with mean absolute percentage errors of 3.9% (PaO2), 3.0% (PaCO2), and 5.8% (PIP) across the cohort. Predictions for uncalibrated variables (pHa, SaO2, mean and minimum airway pressure) also showed high accuracy, with errors <5%. Strong correlations and narrow limits of agreement were observed across all patients and time points. Conclusions: This study demonstrates, for the first time, the feasibility of creating fully mechanistic digital twins of mechanically ventilated neonates with RDS. The twins accurately captured patient-specific gas exchange and respiratory mechanics, supporting their potential as a platform for conducting virtual clinical trials and for the design of individualized, lung-protective ventilation strategies."
  },
  {
    "filename": "2509.19005v1.txt",
    "text": "The Minimum Bisection Problem is a well-known NP-hard problem in combinatorial optimization, with practical applications in areas such as parallel computing, network design, and machine learning. In this paper, we examine the potential of using D-Wave Systems' quantum annealing solvers to solve the Minimum Bisection Problem, which we formulate as a Quadratic Unconstrained Binary Optimization model. A key challenge in this formulation lies in choosing an appropriate penalty parameter, as it plays a crucial role in ensuring both the quality of the solution and the satisfaction of the problem's constraints. To address this, we introduce a novel machine learning-based approach for adaptive tuning of the penalty parameter. Specifically, we use a Gradient Boosting Regressor model trained to predict suitable penalty parameter values based on structural properties of the input graph, the number of nodes and the graph's density. This method enables the penalty parameter to be adjusted dynamically for each specific problem instance, improving the solver's ability to balance the competing goals of minimizing the cut size and maintaining equally sized partitions. We test our approach on a large dataset of randomly generated Erd\\H{o}s-R\\'enyi graphs with up to 4,000 nodes, and we compare the results with classical partitioning algorithms, Metis and Kernighan-Lin. Experimental findings demonstrate that our adaptive tuning strategy significantly improves the performance of the quantum annealing hybrid solver and consistently outperforms the classical methods used, indicating its potential as an alternative for the graph partitioning problem."
  },
  {
    "filename": "2509.19027v1.txt",
    "text": "We formalize glass-box analysis for computer systems and introduce three principled tools. First, the Glass-Box Transparency Index (GTI) quantifies the fraction of performance variance explainable by internal features and comes equipped with bounds, invariances, cross-validated estimation, and bootstrap confidence intervals. Second, Explainable Throughput Decomposition (ETD) uses Shapley values to provide an efficiency-preserving attribution of throughput, together with non-asymptotic Monte Carlo error guarantees and convexity (Jensen) gap bounds. Third, we develop an exact Markov analytic framework for branch predictors, including a closed-form misprediction rate for a two-bit saturating counter under a two-state Markov branch process and its i.i.d. corollary. Additionally, we establish an identifiability theorem for recovering event rates from aggregated hardware counters and provide stability bounds under noise."
  },
  {
    "filename": "2509.19042v1.txt",
    "text": "We build a 167-indicator comprehensive credit risk indicator set, integrating macro, corporate financial, bond-specific indicators, and for the first time, 30 large-scale corporate non-financial indicators. We use seven machine learning models to construct a bond credit spread prediction model, test their spread predictive power and economic mechanisms, and verify their credit rating prediction effectiveness. Results show these models outperform Chinese credit rating agencies in explaining credit spreads. Specially, adding non-financial indicators more than doubles their out-of-sample performance vs. traditional feature-driven models. Mechanism analysis finds non-financial indicators far more important than traditional ones (macro-level, financial, bond features)-seven of the top 10 are non-financial (e.g., corporate governance, property rights nature, information disclosure evaluation), the most stable predictors. Models identify high-risk traits (deteriorating operations, short-term debt, higher financing constraints) via these indicators for spread prediction and risk identification. Finally, we pioneer a credit rating model using predicted spreads (predicted implied rating model), with full/sub-industry models achieving over 75% accuracy, recall, F1. This paper provides valuable guidance for bond default early warning, credit rating, and financial stability."
  },
  {
    "filename": "2509.19049v1.txt",
    "text": "The model minority myth obscures the educational disparities among Asian student groups in physics education. This study estimated the variation in conceptual physics knowledge across 19 Asian racial/ethnic groups at the start and end of introductory physics courses. Utilizing data from the LASSO platform, we analyzed responses from 16,810 students enrolled in 493 introductory calculus-based physics courses across 64 U.S. institutions. We applied Multilevel Analysis of Individual Heterogeneity and Discriminatory Accuracy (MAIHDA) to analyze the student outcomes with the Force Concept Inventory and Force and Motion Conceptual Evaluation. We found that the predicted posttest score of the lowest performing group is the same as the predicted pretest score of the highest performing group. Disaggregated data reveal performance differences among Asian groups that aggregated reporting conceals. To avoid the challenges that can arise when disaggregating data, instructors and researchers must consider many factors, such as research questions and/or methodological constraints. By leveraging the expanded identity options within the LASSO platform and the MAIHDA model, our approach offers a powerful framework for exposing hidden disparities and advancing equity in STEM education."
  },
  {
    "filename": "2509.19051v1.txt",
    "text": "In this article, a failure mode dependent and thermodynamically consistent continuum damage model with polynomial-based damage hardening functions is proposed for continuum damage modeling of laminated composite panels. The damage model parameters are characterized based on all uniaxial/shear experimental stress-strain curves. Steepest descent optimization algorithm is used to minimize the difference between model predicted and experimental stress-strain curves to get the optimzed model parameters. The fully characterized damage evolution equations are used for damage prediction of a moderately thick laminated composite curved beam modeled using first-order shear deformation theory. Finite element method with load control is used to get the non-linear algebraic equations which are solved using Newton Raphson method. The developed model is compared with the existing failure mode dependent and failure mode independent damage models. The results depict the efficacy of the proposed model to capture non-linearity in the load vs deflection curve due to stiffness degradation and different damage in tension andcompression consistent with uniaxial/shear stress-strain response and strength properties of the material, respectively."
  },
  {
    "filename": "2509.19054v1.txt",
    "text": "Future energy projections and their inherent uncertainty play a key role in the design of photovoltaic-battery energy storage systems (PV-BESS) for household use. In this study, both stochastic and robust optimization techniques are simultaneously integrated into a Hybrid Adaptive Robust-Stochastic Optimization (HARSO) model. Uncertainty in future PV generation is addressed using a stochastic approach, while uncertainty in power demand is handled through robust optimization. To solve the tri-level structure emerging from the hybrid approach, a Column-and-Constraint Generation (CCG) algorithm is implemented. The model also accounts for battery degradation by considering multiple commercially available battery chemistries, enabling a more realistic evaluation of long-term system costs and performance. To demonstrate its applicability, the model is applied to a case study involving the optimal design of a PV-BESS system for a household in Spain. The empirical analysis includes both first-life (FL) and second-life (SL) batteries with different chemistries, providing a comprehensive evaluation of design alternatives under uncertainty. Results indicate that the optimal solution is highly dependent on the level of robustness considered, leading to a shift in design strategy. Under less conservative settings, robustness is achieved by increasing battery capacity, while higher levels of conservatism favor expanding PV capacity to meet demand."
  },
  {
    "filename": "2509.19059v1.txt",
    "text": "We numerically study excitation transfer in an artificial LH1--RC complex -- an $N$-site donor ring coupled to a central acceptor -- driven by a narrowband optical mode and evolved under a Lindblad master equation with loss and dephasing. In the absence of disorder, the light-driven system exhibits a tall, narrow on-resonance efficiency peak (near unity for our parameters); dephasing lowers and narrows this peak without shifting its position. Off resonance, the efficiency shows environmentally assisted transport with a clear non-monotonic dependence on dephasing and a finite optimum. Under static disorder, two regimes emerge: photon--ring coupling and diagonal energetic disorder mix the drive into dark ring modes, activate dissipative channels, and depress efficiency over a detuning window, whereas intra-ring coupling disorder has a much smaller impact in the tested range; increasing the intra-ring coupling $g$ moves dark-mode crossings away from the operating detuning and restores near-peak performance. In the ordered, symmetric, single-excitation, narrowband limit we \\emph{analytically derive} closed-form transfer efficiencies by projecting onto the $k{=}0$ bright mode and solving the photon--bright mode--acceptor trimer via a Laplace/linear-algebra (determinant) formula; these expressions include a probability-conservation identity $\\eta + \\sum_k L_k = 1$ that benchmarks the simulations and quantitatively predicts the resonant line shape and its dephasing-induced narrowing. A minimal ring toy model further reproduces coherent trapping and its relief by moderate dephasing (ENAQT). These analytics are exact in the ordered limit and serve as mechanistic guides outside this limit, yielding practical design rules for robust, bio-inspired light-harvesting devices."
  },
  {
    "filename": "2509.19078v1.txt",
    "text": "Deep Gaussian processes (DGPs) enable expressive hierarchical Bayesian modeling but pose substantial challenges for posterior inference, especially over inducing variables. Denoising diffusion variational inference (DDVI) addresses this by modeling the posterior as a time-reversed diffusion from a simple Gaussian prior. However, DDVI's fixed unconditional starting distribution remains far from the complex true posterior, resulting in inefficient inference trajectories and slow convergence. In this work, we propose Diffusion Bridge Variational Inference (DBVI), a principled extension of DDVI that initiates the reverse diffusion from a learnable, data-dependent initial distribution. This initialization is parameterized via an amortized neural network and progressively adapted using gradients from the ELBO objective, reducing the posterior gap and improving sample efficiency. To enable scalable amortization, we design the network to operate on the inducing inputs, which serve as structured, low-dimensional summaries of the dataset and naturally align with the inducing variables' shape. DBVI retains the mathematical elegance of DDVI, including Girsanov-based ELBOs and reverse-time SDEs,while reinterpreting the prior via a Doob-bridged diffusion process. We derive a tractable training objective under this formulation and implement DBVI for scalable inference in large-scale DGPs. Across regression, classification, and image reconstruction tasks, DBVI consistently outperforms DDVI and other variational baselines in predictive accuracy, convergence speed, and posterior quality."
  },
  {
    "filename": "2509.19088v1.txt",
    "text": "Do \"digital twins\" capture individual responses in surveys and experiments? We run 19 pre-registered studies on a national U.S. panel and their LLM-powered digital twins (constructed based on previously-collected extensive individual-level data) and compare twin and human answers across 164 outcomes. The correlation between twin and human answers is modest (approximately 0.2 on average) and twin responses are less variable than human responses. While constructing digital twins based on rich individual-level data improves our ability to capture heterogeneity across participants and predict relative differences between them, it does not substantially improve our ability to predict the exact answers given by specific participants or enhance predictions of population means. Twin performance varies by domain and is higher among more educated, higher-income, and ideologically moderate participants. These results suggest current digital twins can capture some degree of relative differences but are unreliable for individual-level predictions and sample mean and variance estimation, underscoring the need for careful validation before use. Our data and code are publicly available for researchers and practitioners interested in optimizing digital twin pipelines."
  },
  {
    "filename": "2509.19092v1.txt",
    "text": "Multimodal sensing reduces beam training overhead but is constrained by machine learning complexity and dataset demands. To address this, we propose a data-free (DF) knowledge distillation (KD) framework for efficient LiDAR-aided mmWave beam tracking, i.e., predicting the best current and future beams. Specifically, we propose a knowledge inversion framework, where a generator synthesizes LiDAR input data from random noise, guided by a loss function defined on the features and outputs of a pre-trained teacher model. The student model is then trained using the synthetic data and knowledge distilled from the teacher. The generator loss combines three terms, called metadata loss, activation loss, and entropy loss. For student training, in addition to the standard Kullback-Leibler divergence loss, we also consider a mean-squared error (MSE) loss between the teacher and student logits. Simulation results show that the proposed DF-KD (slightly) outperforms the teacher in Top-1 and Top-5 accuracies. Moreover, we observe that the metadata loss contributes significantly to the generator performance, and that the MSE loss for the student can effectively replace the standard KD loss while requiring fewer fine-tuned hyperparameters."
  },
  {
    "filename": "2509.19099v1.txt",
    "text": "We establish a coordinate-invariant Heisenberg-type lower bound for quantum states strictly localized in geodesic balls of radius $r_g$ on horizon-regular spacelike slices of static, spherically symmetric, asymptotically flat (AF) black-holes. Via a variance-eigenvalue equivalence the momentum uncertainty reduces to the first Dirichlet eigenvalue of the Laplace-Beltrami operator, yielding a slice-uniform Hardy baseline $\\sigma_p r_g \\ge \\hbar/2$ under mild convexity assumptions on the balls; the bound is never attained and admits a positive gap both on compact interior regions and uniformly far out. For the Schwarzschild Painlev\\'e-Gullstrand (PG) slice, whose induced 3-geometry is Euclidean, one recovers the exact Euclidean scale $\\sigma_p r_g \\ge \\pi\\hbar$, which is optimal among all admissible slices. The entire construction extends across the black-hole horizon, and it transfers to the static axisymmetric Weyl class, where the Hardy floor, strict gap, and AF $\\pi$-scale persist (a global PG-like optimum need not exist)."
  },
  {
    "filename": "2509.19105v1.txt",
    "text": "Successful navigation in outdoor environments requires accurate prediction of the physical interactions between the robot and the terrain. To this end, several methods rely on geometric or semantic labels to classify traversable surfaces. However, such labels cannot distinguish visually similar surfaces that differ in material properties. Spectral sensors enable inference of material composition from surface reflectance measured across multiple wavelength bands. Although spectral sensing is gaining traction in robotics, widespread deployment remains constrained by the need for custom hardware integration, high sensor costs, and compute-intensive processing pipelines. In this paper, we present RGB Image to Spectral Signature Neural Network (RS-Net), a deep neural network designed to bridge the gap between the accessibility of RGB sensing and the rich material information provided by spectral data. RS-Net predicts spectral signatures from RGB patches, which we map to terrain labels and friction coefficients. The resulting terrain classifications are integrated into a sampling-based motion planner for a wheeled robot operating in outdoor environments. Likewise, the friction estimates are incorporated into a contact-force-based MPC for a quadruped robot navigating slippery surfaces. Thus, we introduce a framework that learns the task-relevant physical property once during training and thereafter relies solely on RGB sensing at test time. The code is available at https://github.com/prajapatisarvesh/RS-Net."
  },
  {
    "filename": "2509.19112v1.txt",
    "text": "Understanding causality in event sequences where outcome labels such as diseases or system failures arise from preceding events like symptoms or error codes is critical. Yet remains an unsolved challenge across domains like healthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label causal discovery method for sparse, high-dimensional event sequences comprising of thousands of unique event types. Using two pretrained causal Transformers as domain-specific foundation models for event sequences. CARGO infers in parallel, per sequence one-shot causal graphs and aggregates them using an adaptive frequency fusion to reconstruct the global Markov boundaries of labels. This two-stage approach enables efficient probabilistic reasoning at scale while bypassing the intractable cost of full-dataset conditional independence testing. Our results on a challenging real-world automotive fault prediction dataset with over 29,100 unique event types and 474 imbalanced labels demonstrate CARGO's ability to perform structured reasoning."
  },
  {
    "filename": "2509.19116v1.txt",
    "text": "Photoproduction processes have gained a renewed interest following the approval of the EIC, making their implementation in Monte Carlo event generators highly desirable. We present recent efforts to develop a POWHEG BOX extension simulating dijet production from direct and resolved photons at next-to-leading order in QCD merged to parton showers, employing the Weizs\\\"acker-Williams Approximation. It will facilitate event generation for collisions involving leptons, protons and heavy ions. Thus, it will be particularly useful for the study of ultra-peripheral collisions at CERN's LHC and for making predictions relevant to BNL's EIC."
  },
  {
    "filename": "2509.19117v1.txt",
    "text": "Large language models (LLMs) excel in many tasks of software engineering, yet progress in leveraging them for vulnerability discovery has stalled in recent years. To understand this phenomenon, we investigate LLMs through the lens of classic code metrics. Surprisingly, we find that a classifier trained solely on these metrics performs on par with state-of-the-art LLMs for vulnerability discovery. A root-cause analysis reveals a strong correlation and a causal effect between LLMs and code metrics: When the value of a metric is changed, LLM predictions tend to shift by a corresponding magnitude. This dependency suggests that LLMs operate at a similarly shallow level as code metrics, limiting their ability to grasp complex patterns and fully realize their potential in vulnerability discovery. Based on these findings, we derive recommendations on how research should more effectively address this challenge."
  },
  {
    "filename": "2509.19127v1.txt",
    "text": "In Titan's atmosphere, the chemistry of small hydrocarbons and nitriles represent an important link from molecular species to the ubiquitous organic haze that gives Titan its characteristic yellow color. Here we present a new search for two previously undetected molecules, triacetylene (C$_{6}$H$_{2}$) and the gas phase dicyanoacetylene (C$_{4}$N$_{2}$), using the Echelon-Cross-Echelle Spectrograph (EXES) instrument aboard the SOFIA (Stratospheric Observatory For Infrared Astronomy) aircraft. We do not detect these two molecules but determine upper limits for their mixing ratios and column abundances. We find the $3\\sigma$ upper limits on the uniform volume mixing ratio (VMR) above 100 km for C$_{6}$H$_{2}$ to be $4.3\\times10^{-11}$ which is lower than the photochemical model predictions. This new upper limit suggests that the growth of linear molecules is inhibited. We also put a strict upper limit on the uniform VMR for gas phase C$_{4}$N$_{2}$ above 125 km to be $1.0\\times10^{-10}$. This upper limit is well below the saturation mixing ratio at this altitude for C$_{4}$N$_{2}$ and greatly limits the feasibility of C$_{4}$N$_{2}$ forming ice from condensation."
  },
  {
    "filename": "2509.19130v1.txt",
    "text": "Environmental sensing can significantly enhance mmWave communications by assisting beam training, yet its benefits must be balanced against the associated sensing costs. To this end, we propose a unified machine learning framework that dynamically determines when to sense and leverages sensory data for beam prediction. Specifically, we formulate a joint sensing and beamforming problem that maximizes the av- erage signal-to-noise ratio under an average sensing budget. Lyapunov optimization is employed to enforce the sensing constraint, while a deep Q-Network determines the sensing slots. A pretrained deep neural network then maps the sens- ing data to optimal beams in the codebook. Simulations based on the real-world DeepSense dataset demonstrate that the pro- posed approach substantially reduces sensing overhead while maintaining satisfactory communications performance."
  },
  {
    "filename": "2509.19131v1.txt",
    "text": "Our understanding of the origin of heavy elements beyond iron relies on the rapid neutron capture process (r-process), which accounts for roughly half of their cosmic abundance. However, the extreme neutron-rich conditions required for the r-process involve many nuclei that remain experimentally inaccessible, making theoretical predictions essential. We explore the impact of nuclear masses calculated with the ab initio valence-space in-medium similarity renormalization group, focusing on the region around the N = 82 shell closure. We show that such ab initio mass calculations can refine the r-process predictions compared to global, but more phenomenological mass models. With the ab initio masses, the waiting point of the second r-process peak is strengthened, which leads to an overall slower nucleosynthesis flow, lower abundances of nuclei beyond the peak, and a stronger shift of the third r-process peak."
  },
  {
    "filename": "2509.19132v1.txt",
    "text": "We investigate the local atomic and electronic structure, thermodynamic stability, and defect chemistry of $A_{6}B_{2}$O$_{17}$ ($A$ = Zr/Hf, $B$ = Nb/Ta) oxides using first-principles density functional theory (DFT) calculations. We examine both ordered unit cells as well as fully disordered special quasirandom structures to clearly discern the effects of cation disorder. Structural predictions align closely with previous experimental results and follow established ionic radii trends. The electronic structure is strongly dependent on $B$-cation species: $A_{6}$Ta$_{2}$O$_{17}$ compositions have ~30% larger band gaps than their $A_{6}$Nb$_{2}$O$_{17}$ counterparts. Defect chemistry is similar for all compositions, with anion vacancies being more energetically favorable than corresponding cation defects. All explored $A_{6}B_{2}$O$_{17}$ compositions are enthalpically unstable with respect to their $A$O$_{2}$ and $B_{2}$O$_{5}$ competing oxides and are therefore classified as entropy-stabilized materials, supporting prior experimental results. The pronounced agreement between our disordered supercell predictions with experimental measurements indicates all explored $A_{6}B_{2}$O$_{17}$ compositions contain substantial cation disorder across all 6-, 7-, and 8-coordinated sites. Our findings collectively provide a fundamental understanding of the $A_{6}B_{2}$O$_{17}$ material family through DFT calculations, establishing a framework for future compositional tuning to engineer targeted material properties."
  },
  {
    "filename": "2509.19133v1.txt",
    "text": "The Fe xxii doublet has been previously used to determine the density of collisionally ionized emission from magnetic cataclysmic variable stars. We test how this diagnostic doublet behaves for a photoionized plasma with an active galactic nucleus (AGN) spectral energy distribution (SED). We use the photoionized plasma code pion and ~440 ks of archival Chandra HETG for the well-known Seyfert 2 galaxy NGC 1068 to test the behaviour of the Fe xxii doublet in the context of an AGN. This marks the first time these data have been examined with pion. We find that in a photoionized plasma, the Fe xxii doublet is dependent on the density, ionization state, and SED used. Thus, this density diagnostic remains model-dependent. In the context of NGC 1068 the doublet predicts an emission region ~100 rg from the central black hole. This would require a direct line of sight to the central engine, which is at odds with the Seyfert 2 nature of this source. In practice, these results highlight the complexities and challenges of applying photoionized models. With these data, we cannot exclude the possibility of a direct line of sight to the central engine of NGC 1068, but we cannot confirm it. Future observations with instruments such as Athena are needed to explore the Fe xxii doublet further."
  },
  {
    "filename": "2509.19135v1.txt",
    "text": "Human mobility traces, often recorded as sequences of check-ins, provide a unique window into both short-term visiting patterns and persistent lifestyle regularities. In this work we introduce GSTM-HMU, a generative spatio-temporal framework designed to advance mobility analysis by explicitly modeling the semantic and temporal complexity of human movement. The framework consists of four key innovations. First, a Spatio-Temporal Concept Encoder (STCE) integrates geographic location, POI category semantics, and periodic temporal rhythms into unified vector representations. Second, a Cognitive Trajectory Memory (CTM) adaptively filters historical visits, emphasizing recent and behaviorally salient events in order to capture user intent more effectively. Third, a Lifestyle Concept Bank (LCB) contributes structured human preference cues, such as activity types and lifestyle patterns, to enhance interpretability and personalization. Finally, task-oriented generative heads transform the learned representations into predictions for multiple downstream tasks. We conduct extensive experiments on four widely used real-world datasets, including Gowalla, WeePlace, Brightkite, and FourSquare, and evaluate performance on three benchmark tasks: next-location prediction, trajectory-user identification, and time estimation. The results demonstrate consistent and substantial improvements over strong baselines, confirming the effectiveness of GSTM-HMU in extracting semantic regularities from complex mobility data. Beyond raw performance gains, our findings also suggest that generative modeling provides a promising foundation for building more robust, interpretable, and generalizable systems for human mobility intelligence."
  },
  {
    "filename": "2509.19142v1.txt",
    "text": "Bimanual grasping is essential for robots to handle large and complex objects. However, existing methods either focus solely on single-arm grasping or employ separate grasp generation and bimanual evaluation stages, leading to coordination problems including collision risks and unbalanced force distribution. To address these limitations, we propose BiGraspFormer, a unified end-to-end transformer framework that directly generates coordinated bimanual grasps from object point clouds. Our key idea is the Single-Guided Bimanual (SGB) strategy, which first generates diverse single grasp candidates using a transformer decoder, then leverages their learned features through specialized attention mechanisms to jointly predict bimanual poses and quality scores. This conditioning strategy reduces the complexity of the 12-DoF search space while ensuring coordinated bimanual manipulation. Comprehensive simulation experiments and real-world validation demonstrate that BiGraspFormer consistently outperforms existing methods while maintaining efficient inference speed (<0.05s), confirming the effectiveness of our framework. Code and supplementary materials are available at https://sites.google.com/bigraspformer"
  },
  {
    "filename": "2509.19154v1.txt",
    "text": "Conventional phase segregation is controlled by a positive interfacial tension, which implies that the system relaxes towards a state in which the interfacial area (or length) is minimized, typically manifesting as a single droplet that grows with the system size. Intriguingly, the extension of the underlying Model B paradigm by two non-potential terms (Active Model B+) is able to describe the stable coexistence of many finite droplets. Here we numerical study Active Model B+ in the vicinity of the transition between a single droplet (macrophase segregation) and multiple droplets (microphase segregation). Our results show that, although noise shifts transitions, the overall agreement with the mean-field theoretical predictions is very good. We find a strong correlation of droplet properties with a single parameter that determines the number, density, and the fractal dimension of droplets. Deeper inside the droplet phase we observe another transition to a hexagonal lattice of regular droplets."
  },
  {
    "filename": "2509.19157v1.txt",
    "text": "Quantum machine learning methods often rely on fixed, hand-crafted quantum encodings that may not capture optimal features for downstream tasks. In this work, we study the power of quantum autoencoders in learning data-driven quantum representations. We first theoretically demonstrate that the quantum autoencoder method is efficient in terms of sample complexity throughout the entire training process. Then we numerically train the quantum autoencoder on 3 million peptide sequences, and evaluate their effectiveness across multiple peptide classification problems including antihypertensive peptide prediction, blood-brain barrier-penetration, and cytotoxic activity detection. The learned representations were compared against Hamiltonian-evolved baselines using a quantum kernel with support vector machines. Results show that quantum autoencoder learned representations achieve accuracy improvements ranging from 0.4\\% to 8.1\\% over Hamiltonian baselines across seven datasets, demonstrating effective generalization to diverse downstream datasets with pre-training enabling effective transfer learning without task-specific fine-tuning. This work establishes that quantum autoencoder architectures can effectively learn from large-scale datasets (3 million samples) with compact parameterizations ($\\sim$900 parameters), demonstrating their viability for practical quantum applications."
  },
  {
    "filename": "2509.19170v1.txt",
    "text": "The use of continuous instead of discrete tokens during the Chain-of-Thought (CoT) phase of reasoning LLMs has garnered attention recently, based on the intuition that a continuous mixture of discrete tokens could simulate a superposition of several reasoning paths simultaneously. Theoretical results have formally proven that continuous tokens have much greater expressivity and can solve specific problems more efficiently. However, practical use of continuous tokens has been limited by strong training difficulties: previous works either just use continuous tokens at inference time on a pre-trained discrete-token model, or must distill the continuous CoT from ground-truth discrete CoTs and face computational costs that limit the CoT to very few tokens.   This is the first work introducing a scalable method to learn continuous CoTs via reinforcement learning (RL), without distilling from reference discrete CoTs. We use \"soft\" tokens: mixtures of tokens together with noise on the input embedding to provide RL exploration. Computational overhead is minimal, enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs match discrete-token CoTs for pass@1 and surpass them for pass@32, showing greater CoT diversity. In systematic comparisons, the best-performing scenario is to train with continuous CoT tokens then use discrete tokens for inference, meaning the \"soft\" models can be deployed in a standard way. Finally, we show continuous CoT RL training better preserves the predictions of the base model on out-of-domain tasks, thus providing a softer touch to the base model."
  },
  {
    "filename": "2509.19171v1.txt",
    "text": "We develop a spectral element lattice Boltzmann method (SELBM) with the flux bounce-back (FBB) scheme, to enable accurate simulations of single-phase fluid dynamics in unstructured mesh. We adopt an Eulerian description of the streaming process in place of the perfect shift in the regular LBM. The spectral element method is used to spatially discretize the convective term, while the strong stability-preserving Runge-Kutta (SSPRK) method is used for time integration. To increase stability, we investigate the use of an explicit filter, particularly in the context of the sensitive double shear layer problem. The results indicate that by using the high-order polynomial, we can effectively eliminate the small vortices around the neck region. We introduce the flux bounce-back scheme to enable the current scheme to handle complex boundaries. The proposed scheme and flux boundary method are validated through benchmark simulations, including the unsteady Couette flow and the planar Poiseuille flow. Further validation is provided through the Taylor-Green vortex problem, demonstrating the accuracy and convergence of the scheme for isotropic turbulence. Finally, we consider a fully developed turbulent flow within a cylindrical pipe and correctly predict the turbulent boundary layer profile."
  },
  {
    "filename": "2509.19176v1.txt",
    "text": "We study the lattice Yang-Mills-Higgs model with inverse gauge coupling $\\beta>0$ and Higgs length $\\alpha>0$, in the ``complete breakdown of symmetry\" regime. For lattice dimension $d\\ge 2$ and (abelian) gauge group U(1), we prove that for any $m>0$, if $\\alpha= m\\beta$ and $\\beta$ is large enough, the model exhibits exponential decay of correlations. This extends the classical result of Osterwalder and Seiler (1978), who required in addition that $m$ be sufficiently large. Our result also verifies a phase diagram from the physics literature, predicted by Fradkin and Shenker (1979). The proof is based on the Glimm-Jaffe-Spencer cluster expansion around a massive Gaussian field, following the approach of Balaban et al. (1984)."
  },
  {
    "filename": "2509.19178v1.txt",
    "text": "One of the main codes to analyze and optimize stellarator configurations is the EMC3 code, which implements a state-of-the-art 3D Monte Carlo plasma edge transport code. However, so far, a self-consistent treatment of the E x B drift is absent. This plasma drift is known to significantly impact the particle and heat distribution in the plasma edge. It is desirable to incorporate this drift into EMC3 to improve the predictive capabilities of the code. The calculation of the E x B drift requires the approximation of the electric field E, which is proportional to the gradient of the electric potential $ \\varphi $. In previous work, the gradient was calculated with a least squares method based on a finite difference approximation of the electric potential. However, due to the stochastic nature of EMC3, the output plasma fields computed by the code are inherently noisy. The finite difference method further amplifies the noise, with the amplification growing as the grid size decreases. We continue from, which introduced a new noise-robust method for 1D derivatives. We extend the noise-robust method to 2D and apply it to the electric potential. We show that a PDE can be derived that describes the evolution of the electric field in case of a uniform diffusion coefficient. This PDE allows us to approximate the electric field directly with a Monte Carlo simulation, thus avoiding the need for a finite difference approximation. We illustrate the accuracy of the method and the noise robustness with a test case."
  },
  {
    "filename": "2509.19180v1.txt",
    "text": "Neural-network-based machine learning interatomic potentials have emerged as powerful tools for predicting atomic energies and forces, enabling accurate and efficient simulations in atomistic modeling. A key limitation of traditional deep learning approaches, however, is their inability to provide reliable estimates of predictive uncertainty. Such uncertainty quantification is critical for assessing model reliability, especially in materials science, where often the model is applied on out-of-distribution data. Different strategies have been proposed to address this challenge, with deep ensembles and Bayesian neural networks being among the most widely used. In this work, we introduce an implementation of Bayesian neural networks with variational inference in the aenet-PyTorch framework. To evaluate their applicability to machine learning interatomic potentials, we systematically compare the performance of variational BNNs and deep ensembles on a dataset of 7,815 TiO$_{2}$ structures. The models are trained on both the full dataset and a subset to assess how variations in data representation influence predictive accuracy and uncertainty estimation. This analysis provides insights into the strengths and limitations of each approach, offering practical guidance for the development of uncertainty-aware machine learning interatomic potentials."
  },
  {
    "filename": "2509.19184v1.txt",
    "text": "The escape of Lyman-$\\alpha$ (Ly$\\alpha$) radiation encodes valuable information on the neutral interstellar medium and is often used as a proxy for the escape of ionizing photons. Yet, the theory of Ly$\\alpha$ transfer through anisotropic gas distributions remains underdeveloped. We present Monte Carlo radiative transfer simulations of Ly$\\alpha$ propagation through porous, inhomogeneous neutral gas, systematically exploring the effects of channel geometry, outflows, dust, and lognormally distributed column densities. We find that Ly$\\alpha$ photons do not preferentially escape through the lowest-column-density pathways, but instead traverse channels of substantial optical depth, leading to suppressed central flux and the absence of strongly beamed escape. Subdividing channels has little impact, indicating that geometry and covering fraction are more important than porosity. Channels containing moderate amounts of neutral hydrogen alter escape in characteristic ways, including the appearance of quadruple-peaked spectra, which can be captured by a simple flux-channel relation. Outflows reshape the spectra by facilitating escape through dense media, redshifting photons and blending central features, while dust modulates the visibility of small channels by suppressing flux at line center; in both cases, we develop an analytical model that predicts the resulting central fluxes. Extending to lognormal column density fields, we show that Ly$\\alpha$ photons probe a broad range of optical depths, producing skewed spectra that can be approximated by weighted sums of homogeneous models. Our results have direct implications for using Ly$\\alpha$ as a tracer of gas properties and ionizing photon escape; for instance, spectra suggestive of high column densities may nonetheless allow LyC leakage through narrow channels."
  },
  {
    "filename": "2509.19185v1.txt",
    "text": "Foundation model (FM)-based AI agents are rapidly gaining adoption across diverse domains, but their inherent non-determinism and non-reproducibility pose testing and quality assurance challenges. While recent benchmarks provide task-level evaluations, there is limited understanding of how developers verify the internal correctness of these agents during development.   To address this gap, we conduct the first large-scale empirical study of testing practices in the AI agent ecosystem, analyzing 39 open-source agent frameworks and 439 agentic applications. We identify ten distinct testing patterns and find that novel, agent-specific methods like DeepEval are seldom used (around 1%), while traditional patterns like negative and membership testing are widely adapted to manage FM uncertainty. By mapping these patterns to canonical architectural components of agent frameworks and agentic applications, we uncover a fundamental inversion of testing effort: deterministic components like Resource Artifacts (tools) and Coordination Artifacts (workflows) consume over 70% of testing effort, while the FM-based Plan Body receives less than 5%. Crucially, this reveals a critical blind spot, as the Trigger component (prompts) remains neglected, appearing in around 1% of all tests.   Our findings offer the first empirical testing baseline in FM-based agent frameworks and agentic applications, revealing a rational but incomplete adaptation to non-determinism. To address it, framework developers should improve support for novel testing methods, application developers must adopt prompt regression testing, and researchers should explore barriers to adoption. Strengthening these practices is vital for building more robust and dependable AI agents."
  },
  {
    "filename": "2509.19189v1.txt",
    "text": "Scaling laws have played a cornerstone role in guiding the training of large language models (LLMs). However, most existing works on scaling laws primarily focus on the final-step loss, overlooking the loss dynamics during the training process and, crucially, the impact of learning rate schedule (LRS). In this paper, we aim to bridge this gap by studying a teacher-student kernel regression setup trained via online stochastic gradient descent (SGD). Leveraging a novel intrinsic time viewpoint and stochastic differential equation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL), which characterizes the evolution of population risk during the training process for general LRSs. Remarkably, the impact of the LRSs is captured through an explicit convolution-type functional term, making their effects fully tractable. To illustrate the utility of FSL, we analyze three widely used LRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under both data-limited and compute-limited regimes. We provide theoretical justification for widely adopted empirical practices in LLMs pre-training such as (i) higher-capacity models are more data- and compute-efficient; (ii) learning rate decay can improve training efficiency; (iii) WSD-like schedules can outperform direct-decay schedules. Lastly, we explore the practical relevance of FSL as a surrogate model for fitting, predicting and optimizing the loss curves in LLM pre-training, with experiments conducted across model sizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen the understanding of LLM pre-training dynamics and provide insights for improving large-scale model training."
  },
  {
    "filename": "2509.19200v1.txt",
    "text": "A thermodynamic integration (TI) protocol incorporating dummy atoms is introduced to calculate free energy differences for disulfide bond formation in proteins. This method successfully reproduces experimental redox potentials for multiple proteins, providing improved insights into the redox regulation of various proteins."
  },
  {
    "filename": "2509.19206v1.txt",
    "text": "Continuous and reliable access to curated biological data repositories is indispensable for accelerating rigorous scientific inquiry and fostering reproducible research. Centralized repositories, though widely used, are vulnerable to single points of failure arising from cyberattacks, technical faults, natural disasters, or funding and political uncertainties. This can lead to widespread data unavailability, data loss, integrity compromises, and substantial delays in critical research, ultimately impeding scientific progress. Centralizing essential scientific resources in a single geopolitical or institutional hub is inherently dangerous, as any disruption can paralyze diverse ongoing research. The rapid acceleration of data generation, combined with an increasingly volatile global landscape, necessitates a critical re-evaluation of the sustainability of centralized models. Implementing federated and decentralized architectures presents a compelling and future-oriented pathway to substantially strengthen the resilience of scientific data infrastructures, thereby mitigating vulnerabilities and ensuring the long-term integrity of data. Here, we examine the structural limitations of centralized repositories, evaluate federated and decentralized models, and propose a hybrid framework for resilient, FAIR, and sustainable scientific data stewardship. Such an approach offers a significant reduction in exposure to governance instability, infrastructural fragility, and funding volatility, and also fosters fairness and global accessibility. The future of open science depends on integrating these complementary approaches to establish a globally distributed, economically sustainable, and institutionally robust infrastructure that safeguards scientific data as a public good, further ensuring continued accessibility, interoperability, and preservation for generations to come."
  },
  {
    "filename": "2509.19212v1.txt",
    "text": "Multimodal Large Language Models (MLLMs) are increasingly deployed in real-world applications, yet their ability to make context-aware safety decisions remains limited. Existing methods often fail to balance oversensitivity (unjustified refusals of benign queries) and undersensitivity (missed detection of visually grounded risks), leaving a persistent gap in safety alignment. To address this issue, we introduce Safety-aware Contrastive Decoding (SafeCoDe), a lightweight and model-agnostic decoding framework that dynamically adjusts token generation based on multimodal context. SafeCoDe operates in two stages: (1) a contrastive decoding mechanism that highlights tokens sensitive to visual context by contrasting real and Gaussian-noised images, and (2) a global-aware token modulation strategy that integrates scene-level reasoning with token-level adjustment to adapt refusals according to the predicted safety verdict. Extensive experiments across diverse MLLM architectures and safety benchmarks, covering undersensitivity, oversensitivity, and general safety evaluations, show that SafeCoDe consistently improves context-sensitive refusal behaviors while preserving model helpfulness."
  },
  {
    "filename": "2509.19217v1.txt",
    "text": "An analysis of the tilt angles of the active regions in 15-24 activity cycles was performed. We used data from measurements of magnetic fields in the sunspot umbra in the period 1918 -2019 at the Mount Wilson Observatory, as well as the tilt angles of active regions in 'white' light at the Kodaikanal and Mount Wilson observatories in activity cycles 15-21. The mean tilt angles of active regions $\\overline{\\gamma}$ and the slope $\\mu$ from latitude $\\theta$ in the activity cycles are considered. Low-latitude bipoles are the most important in predicting the strength of solar cycles. In this work, we selected the cutoff latitude $\\theta_{cut}$ at which the highest correlation is observed with the strength of the next activity cycle for active regions with latitude $\\theta<\\theta_{cut}$. It was found that for magnetic field measurement data, the highest correlation of the parameters $\\overline{\\gamma}$ and $\\mu$ with the strength of the next solar activity cycle is characteristic of bipoles in the equatorial zone with $\\theta<\\theta_{cut}\\approx 14.2^o$. For white light observation data, $\\theta_{cut}\\approx 8.5^o$ for Mount Wilson observatory and $\\theta_{cut}\\approx 9.4^o$ for Kodaikanal observatory."
  },
  {
    "filename": "2509.19218v1.txt",
    "text": "Evaluation of hydrocephalus in children is challenging, and the related research is limited by a lack of publicly available, expert-annotated datasets, particularly those with segmentation of the choroid plexus. To address this, we present HyKid, an open-source dataset from 48 pediatric patients with hydrocephalus. 3D MRIs were provided with 1mm isotropic resolution, which was reconstructed from routine low-resolution images using a slice-to-volume algorithm. Manually corrected segmentations of brain tissues, including white matter, grey matter, lateral ventricle, external CSF, and the choroid plexus, were provided by an experienced neurologist. Additionally, structured data was extracted from clinical radiology reports using a Retrieval-Augmented Generation framework. The strong correlation between choroid plexus volume and total CSF volume provided a potential biomarker for hydrocephalus evaluation, achieving excellent performance in a predictive model (AUC = 0.87). The proposed HyKid dataset provided a high-quality benchmark for neuroimaging algorithms development, and it revealed the choroid plexus-related features in hydrocephalus assessments. Our datasets are publicly available at https://www.synapse.org/Synapse:syn68544889."
  },
  {
    "filename": "2509.19222v1.txt",
    "text": "Recent advances in text-to-video (T2V) generation have enabled the creation of high-fidelity, temporally coherent clips from natural language prompts. Yet these systems come with significant computational costs, and their energy demands remain poorly understood. In this paper, we present a systematic study of the latency and energy consumption of state-of-the-art open-source T2V models. We first develop a compute-bound analytical model that predicts scaling laws with respect to spatial resolution, temporal length, and denoising steps. We then validate these predictions through fine-grained experiments on WAN2.1-T2V, showing quadratic growth with spatial and temporal dimensions, and linear scaling with the number of denoising steps. Finally, we extend our analysis to six diverse T2V models, comparing their runtime and energy profiles under default settings. Our results provide both a benchmark reference and practical insights for designing and deploying more sustainable generative video systems."
  },
  {
    "filename": "2509.19227v1.txt",
    "text": "With the widespread deployment of dashcams and advancements in computer vision, developing accident prediction models from the dashcam perspective has become critical for proactive safety interventions. However, two key challenges persist: modeling feature-level interactions among traffic participants (often occluded in dashcam views) and capturing complex, asynchronous multi-temporal behavioral cues preceding accidents. To deal with these two challenges, a Multi-scale Feature Interaction Network (MsFIN) is proposed for early-stage accident anticipation from dashcam videos. MsFIN has three layers for multi-scale feature aggregation, temporal feature processing and multi-scale feature post fusion, respectively. For multi-scale feature aggregation, a Multi-scale Module is designed to extract scene representations at short-term, mid-term and long-term temporal scales. Meanwhile, the Transformer architecture is leveraged to facilitate comprehensive feature interactions. Temporal feature processing captures the sequential evolution of scene and object features under causal constraints. In the multi-scale feature post fusion stage, the network fuses scene and object features across multiple temporal scales to generate a comprehensive risk representation. Experiments on DAD and DADA datasets show that MsFIN significantly outperforms state-of-the-art models with single-scale feature extraction in both prediction correctness and earliness. Ablation studies validate the effectiveness of each module in MsFIN, highlighting how the network achieves superior performance through multi-scale feature fusion and contextual interaction modeling."
  },
  {
    "filename": "2509.19231v1.txt",
    "text": "We present ChiReSSD, a speech reconstruction framework that preserves children speaker's identity while suppressing mispronunciations. Unlike prior approaches trained on healthy adult speech, ChiReSSD adapts to the voices of children with speech sound disorders (SSD), with particular emphasis on pitch and prosody. We evaluate our method on the STAR dataset and report substantial improvements in lexical accuracy and speaker identity preservation. Furthermore, we automatically predict the phonetic content in the original and reconstructed pairs, where the proportion of corrected consonants is comparable to the percentage of correct consonants (PCC), a clinical speech assessment metric. Our experiments show Pearson correlation of 0.63 between automatic and human expert annotations, highlighting the potential to reduce the manual transcription burden. In addition, experiments on the TORGO dataset demonstrate effective generalization for reconstructing adult dysarthric speech. Our results indicate that disentangled, style-based TTS reconstruction can provide identity-preserving speech across diverse clinical populations."
  },
  {
    "filename": "2509.19233v1.txt",
    "text": "In the context of the energy transition, with increasing integration of renewable sources and cross-border electricity exchanges, power grids are encountering greater uncertainty and operational risk. Maintaining grid stability under varying conditions is a complex task, and power flow simulators are commonly used to support operators by evaluating potential actions before implementation. However, traditional physical solvers, while accurate, are often too slow for near real-time use. Machine learning models have emerged as fast surrogates, and to improve their adherence to physical laws (e.g., Kirchhoff's laws), they are often trained with embedded constraints which are also known as physics-informed or hybrid models. This paper presents an ablation study to demystify hybridization strategies, ranging from incorporating physical constraints as regularization terms or unsupervised losses, and exploring model architectures from simple multilayer perceptrons to advanced graph-based networks enabling the direct optimization of physics equations. Using our custom benchmarking pipeline for hybrid models called LIPS, we evaluate these models across four dimensions: accuracy, physical compliance, industrial readiness, and out-of-distribution generalization. The results highlight how integrating physical knowledge impacts performance across these criteria. All the implementations are reproducible and provided in the corresponding Github page."
  },
  {
    "filename": "2509.19234v1.txt",
    "text": "Algorithmic stability is an established tool for analyzing generalization. While adversarial training enhances model robustness, it often suffers from robust overfitting and an enlarged generalization gap. Although recent work has established the convergence of adversarial training in decentralized networks, its generalization properties remain unexplored. This work presents a stability-based generalization analysis of adversarial training under the diffusion strategy for convex losses. We derive a bound showing that the generalization error grows with both the adversarial perturbation strength and the number of training steps, a finding consistent with single-agent case but novel for decentralized settings. Numerical experiments on logistic regression validate these theoretical predictions."
  },
  {
    "filename": "2509.19239v1.txt",
    "text": "The spatial distribution of chemical elements in the Galactic disk provides key constraints on models of galaxy evolution. However, studies using planetary nebulae (PNe) as tracers have been historically limited by large uncertainties in their distances. To overcome the long-standing distance uncertainties, we recalibrated the H$\\alpha$ surface brightness-radius relation (Frew et al. 2016) with Gaia DR3 parallaxes, deriving statistical distances for 1,200 PNe and Bayesian distances for 419 objects with reliable parallaxes. Adopting Bayesian values preferentially, we determined the O/H radial gradient for 230 disk PNe. We tested three models: a single linear gradient, a segmented fit with one break, and a segmented fit with two breaks. Although model selection is statistically inconclusive, segmented fits indicate a change in slope near the solar radius ($R \\sim 8$ kpc), with a flatter or slightly positive gradient inward and a steeper negative gradient outward. This feature may reflect changes in star formation efficiency driven by the Galactic bar or the corotation resonance of the spiral arms. Comparison with other tracers - Cepheids, red giants, and open clusters - shows qualitative consistency. The two-dimensional O/H distribution in the Galactic plane supports the adopted distances and reveals modest azimuthal asymmetry, with enhanced abundances near the bar at positive longitudes, and a bimodal abundance structure between the inner and outer solar regions. Our results provide new constraints on the chemical evolution of the Milky Way, the impact of non-axisymmetric structures, and the possible existence of distinct radial abundance regimes across the Galactic disk."
  },
  {
    "filename": "2509.19240v1.txt",
    "text": "The light odd-Z elements P, Cl, K, and Sc are underproduced in galactic chemical evolution models compared to spectroscopic observations of stars in the Milky Way. The most promising solution to this puzzle is that some massive stars experience O-C shell mergers boosting their yields through dynamic, convective-reactive nucleosynthesis. We report how convective macro physics based on 3D $4\\pi$ hydrodynamic simulations impacts production in the O shell by post-processing the $\\mathrm{M_{ZAMS}}=15~\\mathrm{M_\\odot}$ $Z=0.02$ model from the NuGrid dataset. We explore a mixing downturn, boosted velocities, reduced ingestion rate, and convective quenching. Across 24 mixing cases, the pre-explosive yields for [P/Fe], [Cl/Fe], [K/Fe], and [Sc/Fe] are modified by $[-0.33,0.23]~\\mathrm{dex}$, $[-0.84,0.64]~\\mathrm{dex}$, $[-0.78,1.48]~\\mathrm{dex}$, and $[-0.36,1.29]~\\mathrm{dex}$, respectively. Cases with a convective downturn with the fastest ingestion rate have the largest enhancement, and production is non-monotonic with boosted velocities. Which reactions are most important for the convective-reactive element production pathways depends on the mixing. We parameterize production of $^{40}\\mathrm{K}$ ($t_{1/2} = 1.248~\\mathrm{Gyr}$), an important radiogenic heat source for younger ($2{-}3~\\mathrm{Gyr}$) rocky planets and find a yield variation exceeding three orders of magnitude. This range of initial abundances for $^{40}\\mathrm{K}$ implies the early geodynamic behaviour of silicate mantles in rocky planets can differ greatly from that of Earth. These results underscore the importance of investigating the 3D macro physics of shell merger convection through hydrodynamic simulations to develop a predictive understanding of the origin and variability of the light odd-Z elements and the $^{40}\\mathrm{K}/\\mathrm{K}$ ratio in planet host stars."
  },
  {
    "filename": "2509.19248v1.txt",
    "text": "Given an Abelian groups $G$, denote $\\mu(G)$ the size of its largest sum-free subset and $f_{\\max}(G)$ the number of maximal sum-free sets in $G$. Confirming a prediction by Liu and Sharifzadeh, we prove that all even-order $G\\ne \\mathbb{Z}_2^k$ have exponentially fewer maximal sum-free sets than $\\mathbb{Z}_2^k$, i.e. $f_{\\max}(G) \\leq 2^{(1/2-c)\\mu(G)}$, where $c > 10^{-64}$.   We construct an infinite family of Abelian groups $G$ with intermediate growth in the number of maximal sum-free sets, i.e., with $   2^{(\\frac{1}{2}+c)\\mu(G)}\\leq f_{\\max}(G) \\leq 3^{(\\frac{1}{3}-c)\\mu(G)}   $, where $c=10^{-4}$. This disproves a conjecture of Liu and Sharifzadeh and also answers a question of Hassler and Treglown in the negative.   Furthermore, we determine for every even-order group $G$, the number of maximal distinct sum-free sets (where a distinct sum is $a+b= c$ with distinct $a,b,c$): it is $ 2^{(1/2+o(1))\\mu(G)}$   with the only exception being $G=\\mathbb{Z}_2^k \\oplus \\mathbb{Z}_3$, when this function is $3^{(1/3+o(1))\\mu(G)}$, refuting a conjecture of Hassler and Treglown.   Our proofs rely on a container theorem due to Green and Ruzsa. Other key ingredient is a sharp upper bound we establish on the number of maximal independent sets in graphs with given matching number, which interpolates between the classical results of Moon and Moser, and Hujter and Tuza. A special case of our bound implies that every $n$-vertex graph with a perfect matching has at most $2^{n/2}$ maximal independent sets, resolving another conjecture of Hassler and Treglown."
  },
  {
    "filename": "2509.19249v1.txt",
    "text": "The growing disparity between the exponential scaling of computational resources and the finite growth of high-quality text data now constrains conventional scaling approaches for large language models (LLMs). To address this challenge, we introduce Reinforcement Learning on Pre-Training data (RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast to prior approaches that scale training primarily through supervised learning, RLPT enables the policy to autonomously explore meaningful trajectories to learn from pre-training data and improve its capability through reinforcement learning (RL). While existing RL strategies such as reinforcement learning from human feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR) rely on human annotation for reward construction, RLPT eliminates this dependency by deriving reward signals directly from pre-training data. Specifically, it adopts a next-segment reasoning objective, rewarding the policy for accurately predicting subsequent text segments conditioned on the preceding context. This formulation allows RL to be scaled on pre-training data, encouraging the exploration of richer trajectories across broader contexts and thereby fostering more generalizable reasoning skills. Extensive experiments on both general-domain and mathematical reasoning benchmarks across multiple models validate the effectiveness of RLPT. For example, when applied to Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$, $6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and AIME25, respectively. The results further demonstrate favorable scaling behavior, suggesting strong potential for continued gains with more compute. In addition, RLPT provides a solid foundation, extending the reasoning boundaries of LLMs and enhancing RLVR performance."
  },
  {
    "filename": "2509.19254v1.txt",
    "text": "Non-ordinary states of consciousness (NOC) provide an opportunity to experience highly intense, unique, and perceptually rich subjective states. The neural mechanisms supporting these experiences remain poorly understood. This study examined brain activity associated with a self-induced, substance-free NOC known as Auto-Induced Cognitive Trance (AICT). Twenty-seven trained participants underwent high-density electroencephalography (EEG) recordings during rest and AICT. We analyzed the aperiodic component of the power spectrum (1/f), Lempel-Ziv complexity, and sample entropy from five-minute signal segments. A machine learning approach was used to classify rest and AICT, identify discriminative features, and localize their sources. We also compared EEG metrics across conditions and assessed whether baseline activity predicted the magnitude of change during AICT. Classification analyses revealed condition-specific differences in spectral exponents, complexity, and entropy. The aperiodic component showed the strongest discriminative power, followed by entropy and complexity. Source localization highlighted frontal regions, the posterior cingulate cortex, and the left parietal cortex as key contributors to the AICT state. Baseline neural activity in frontal and parietal regions predicted individual variability in the transition from rest to AICT. These findings indicate that AICT engages brain regions implicated in rich subjective experiences and provide mechanistic insights into how self-induced trance states influence neural functioning."
  },
  {
    "filename": "2509.19255v1.txt",
    "text": "We have fabricated three-dimensional (3D) networks of ultrathin carbon nanotubes (CNTs) within the ~5-Angstrom diameter pores of zeolite ZSM-5 crystals using the chemical vapour deposition (CVD) process. The 1D electronic characteristics of ultrathin CNTs are characterized by van Hove singularities in the density of states. Boron doping was strategically employed to tune the Fermi energy near a van Hove singularity, which is supported by extensive ab-initio calculations, while the 3D network structure ensures the formation of a phase-coherent bulk superconducting state under a 1D to 3D crossover. We report characteristic signatures of superconductivity using four complementary experimental methods: magnetization, specific heat, resistivity, and point-contact spectroscopy, all consistently support a critical temperature Tc at ambient conditions ranging from 220 to 250 K. In particular, point-contact spectroscopy revealed a multigap nature of superconductivity with a large ~30 meV leading gap, in rough agreement with the prediction of the Bardeen-Cooper-Schrieffer (BCS) theory of superconductivity. The differential conductance response displays a particle-hole symmetry and is tuneable between the tunnelling and Andreev limits via the transparency of the contact, as uniquely expected for a superconductor. Preliminary experiments also reveal a giant pressure effect which increases the Tc above the ambient temperature."
  },
  {
    "filename": "2509.19263v1.txt",
    "text": "Tropical storms cause extensive property damage and loss of life, making them one of the most destructive types of natural hazards. The development of predictive models that identify interventions effective at mitigating storm impacts has considerable potential to reduce these adverse outcomes. In this study, we use an artificial intelligence (AI)-driven approach for optimizing intervention schemes that improve resilience to coastal flooding. We combine three different AI models to optimize the selection of intervention types, sites, and scales in order to minimize the expected cost of flooding damage in a given region, including the cost of installing and maintaining interventions. Our approach combines data-driven generation of storm surge fields, surrogate modeling of intervention impacts, and the solving of a continuous-armed bandit problem. We applied this methodology to optimize the selection of sea wall and oyster reef interventions near Tyndall Air Force Base (AFB) in Florida, an area that was catastrophically impacted by Hurricane Michael. Our analysis predicts that intervention optimization could be used to potentially save billions of dollars in storm damage, far outpacing greedy or non-optimal solutions."
  },
  {
    "filename": "2509.19283v1.txt",
    "text": "The geological storage of hydrogen (H_2) requires reliable long-term caprock sealing, yet the nanoscale interactions between H_2 and clay minerals remain critically underexplored despite their importance for storage security. This lack of understanding has limited the ability to predict mechanical stability and leakage risks in H_2 storage formations. Using molecular simulations, this study investigates the swelling behavior and mechanical properties of sodium montmorillonite (Mt), a common smectite clay, under varying hydration states and interlayer H_2 contents. Results show that H_2 accelerates hydration-state transitions, narrows the stability window of crystalline swelling, and promotes asymmetric plume formation in confined interlayers. H_2 alters cation and water coordination, thereby weakening Na^+--Mt electrostatic interactions and modulating H-bond networks at the interface and in the bulk. Mechanical analysis reveals pronounced anisotropy in Mt. In-plane stiffness is mainly governed by basal spacing expansion, whereas out-of-plane stiffness is highly sensitive to the initial presence of water or H_2, which weaken interlayer cohesion. Tensile and compressive strengths in the in-plane directions follow in-plane stiffness trends, while the out-of-plane tensile strength is governed by Mt--water H-bonds. The presence of H_2 further promotes Mt sheets separation by disrupting nanoscale liquid bridges. Collectively, these results provide the first atomistic-scale evidence that intercalated H_2 reshapes swelling energetics, elastic anisotropy, and failure pathways in Mt, highlighting critical nanoscale mechanisms that may compromise caprock integrity during underground H_2 storage."
  },
  {
    "filename": "2509.19295v1.txt",
    "text": "Audio-based pedestrian detection is a challenging task and has, thus far, only been explored in noise-limited environments. We present a new dataset, results, and a detailed analysis of the state-of-the-art in audio-based pedestrian detection in the presence of vehicular noise. In our study, we conduct three analyses: (i) cross-dataset evaluation between noisy and noise-limited environments, (ii) an assessment of the impact of noisy data on model performance, highlighting the influence of acoustic context, and (iii) an evaluation of the model's predictive robustness on out-of-domain sounds. The new dataset is a comprehensive 1321-hour roadside dataset. It incorporates traffic-rich soundscapes. Each recording includes 16kHz audio synchronized with frame-level pedestrian annotations and 1fps video thumbnails."
  },
  {
    "filename": "2509.19297v1.txt",
    "text": "Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective solution for novel view synthesis. Existing methods predominantly rely on a pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a 3D Gaussian. We rethink this widely adopted formulation and identify several inherent limitations: it renders the reconstructed 3D models heavily dependent on the number of input views, leads to view-biased density distributions, and introduces alignment errors, particularly when source views contain occlusions or low texture. To address these challenges, we introduce VolSplat, a new multi-view feed-forward paradigm that replaces pixel alignment with voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature matching, ensuring robust multi-view consistency. Furthermore, it enables adaptive control over Gaussian density based on 3D scene complexity, yielding more faithful Gaussian point clouds, improved geometric consistency, and enhanced novel-view rendering quality. Experiments on widely used benchmarks including RealEstate10K and ScanNet demonstrate that VolSplat achieves state-of-the-art performance while producing more plausible and view-consistent Gaussian reconstructions. In addition to superior results, our approach establishes a more scalable framework for feed-forward 3D reconstruction with denser and more robust representations, paving the way for further research in wider communities. The video results, code and trained models are available on our project page: https://lhmd.top/volsplat."
  },
  {
    "filename": "representatives.txt",
    "text": "Astrophysical jets from powerful active galactic nuclei (AGN) have recently been proposed as promising probes of dark matter (DM) in the sub-GeV mass range. AGN launch relativistic jets that accelerate cosmic rays (CRs) to very high energies, which can then interact with their surroundings and produce multiwavelength (MW) emission spanning from radio frequencies to TeV $\\gamma$ rays. If DM consists of light particles, their interactions with CRs could lead to an additional cooling mechanism that modifies the expected MW emission. In this work, we analyse the MW spectrum of Markarian 421, a well-studied AGN, using a multizone leptonic jet model that includes the interactions between CR electrons and DM particles. For the first time, we account for the uncertainties in the astrophysical jet dynamics, which have been previously neglected when constraining the CR-DM interactions. By fitting simultaneously jet parameters and DM-electrons interactions, we use the MW data from Markarian 421 to set constraints on the DM-induced CR cooling. We obtain 5$\\sigma$ upper limit $\\sigma_\\text{DM-e} \\lesssim 1 \\times 10^{-34}~\\text{cm}^2$ for a DM mass of $1~{\\rm MeV}$. We demonstrate that this is about a factor of five weaker than traditional approaches, implying that properly accounting for degeneracies between jet dynamics and DM interactions is key to derive robust constraints on DM interactions.\n\n================================================================================\nCluster 1 Representative File: 2509.18282v1.txt"
  }
]